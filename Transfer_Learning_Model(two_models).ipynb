{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer_Learning_Model(two models).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YixinFan11/Machine-Learning-in-Science-II/blob/master/Transfer_Learning_Model(two_models).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading Data"
      ],
      "metadata": {
        "id": "Dh8sdl7GjldE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle --upgrade\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c machine-learning-in-science-2022\n",
        "!mkdir content/data\n",
        "!unzip machine-learning-in-science-2022.zip -d data"
      ],
      "metadata": {
        "id": "CoyIIArtjSmy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2f02923-b3ba-49ee-aac4-0d5759781d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "machine-learning-in-science-2022.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "mkdir: cannot create directory ‘content/data’: No such file or directory\n",
            "Archive:  machine-learning-in-science-2022.zip\n",
            "replace data/sampleSubmission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Header Files"
      ],
      "metadata": {
        "id": "_icuQveXkjs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# python standard libraries\n",
        "import os\n",
        "import random\n",
        "import fnmatch\n",
        "import datetime\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "\n",
        "# data processing\n",
        "import numpy as np\n",
        "np.set_printoptions(formatter={'float_kind':lambda x: \"%.4f\" % x})\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.width', 300)\n",
        "pd.set_option('display.float_format', '{:,.4f}'.format)\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "# tensorflow\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import *\n",
        "from keras.models import Sequential  # V2 is tensorflow.keras.xxxx, V1 is keras.xxx\n",
        "from keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "from keras.models import load_model\n",
        "\n",
        "print( f'tf.__version__: {tf.__version__}' )\n",
        "print( f'keras.__version__: {keras.__version__}' )\n",
        "\n",
        "# sklearn\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# imaging\n",
        "import cv2\n",
        "from imgaug import augmenters as img_aug\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFTU39iBjXwi",
        "outputId": "5fe52701-5fbc-4138-a480-9826e3913beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.__version__: 2.8.0\n",
            "keras.__version__: 2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizing the Data"
      ],
      "metadata": {
        "id": "bARERba9ksXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the Data\n",
        "Data = pd.read_csv('/content/data/training_norm.csv')\n",
        "print('Normalized Angle \\n',Data.head())\n",
        "\n",
        "# De normalizing the angle\n",
        "# Data['angle'] = Data['angle'].apply(lambda ang: float(ang * 80) + 50)\n",
        "Data[:10]\n",
        "Data.sort_values(by=['image_id'])\n",
        "print('Denormalized Angle \\n',Data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBE2A1T2koCR",
        "outputId": "bc3eb773-b2f9-4718-ae0c-4f6a18fcf073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized Angle \n",
            "    image_id  angle  speed\n",
            "0         1 0.4375 0.0000\n",
            "1         2 0.8125 1.0000\n",
            "2         3 0.4375 1.0000\n",
            "3         4 0.6250 1.0000\n",
            "4         5 0.5000 0.0000\n",
            "Denormalized Angle \n",
            "    image_id  angle  speed\n",
            "0         1 0.4375 0.0000\n",
            "1         2 0.8125 1.0000\n",
            "2         3 0.4375 1.0000\n",
            "3         4 0.6250 1.0000\n",
            "4         5 0.5000 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Missing Images"
      ],
      "metadata": {
        "id": "nIaY8xmVk0SS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I took the opprotunity to resize the images anyways\n",
        "path = '/content/data/training_data/training_data'\n",
        "\n",
        "## Takes around 2 minutes remember\n",
        "corrupted_images = [] \n",
        "\n",
        "# Resizing the images and the getting the corrupt images\n",
        "# It takes 6min 10sec\n",
        "from PIL import Image\n",
        "import os, sys\n",
        "import glob\n",
        "for filename in tqdm(glob.iglob(path + '**/*.png', recursive=True)):\n",
        "    #print(filename)\n",
        "    try:\n",
        "        im = Image.open(filename)\n",
        "        im = im.resize((224,224), Image.ANTIALIAS)\n",
        "        im.save(filename , 'png', quality=90)\n",
        "    except:\n",
        "        corrupted_images.append(filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWtUFRVdkusH",
        "outputId": "4e4f3f4b-c779-488e-8c64-f1a18d7e8b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "13798it [05:47, 39.73it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(corrupted_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDkgKBo1k4E0",
        "outputId": "33e938f0-8849-46d4-bbd2-d9b49c0f0251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/data/training_data/training_data/8285.png', '/content/data/training_data/training_data/3141.png', '/content/data/training_data/training_data/4895.png', '/content/data/training_data/training_data/10171.png', '/content/data/training_data/training_data/3999.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Split"
      ],
      "metadata": {
        "id": "zeWhvHK4tsQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label(file_path):\n",
        "    file_id = int(str(file_path).split('/')[-1].split('.')[0])\n",
        "    img_id, angle, speed = Data[Data['image_id'] == int(file_id)].to_numpy().squeeze()\n",
        "    return angle,speed"
      ],
      "metadata": {
        "id": "uBXPWjyGuFYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = []\n",
        "\n",
        "import os, sys\n",
        "import glob\n",
        "for filename in glob.iglob(path + '**/*.png', recursive=True):\n",
        "    if filename not in corrupted_images:\n",
        "        img_path.append(filename)\n",
        "print(len(img_path))\n",
        "\n",
        "Y_angle = []\n",
        "Y_speed = []\n",
        "# for i in img_path:\n",
        "#     vals = []\n",
        "#     a,s = get_label(i)\n",
        "#     vals.append(a) ; vals.append(s);\n",
        "#     Y.append(vals)\n",
        "for i in img_path:\n",
        "    vals = []\n",
        "    a,s = get_label(i)\n",
        "    Y_angle.append(a)\n",
        "    Y_speed.append(s)\n",
        "\n",
        "\n",
        "x_train_angle, x_valid_angle, y_train_angle, y_valid_angle = train_test_split(img_path , Y_angle , test_size = 0.25)\n",
        "x_train_speed, x_valid_speed, y_train_speed, y_valid_speed = train_test_split(img_path , Y_speed , test_size = 0.25)\n",
        "print(len(x_train_angle),len(x_valid_angle),len(x_train_speed),len(x_valid_speed))\n",
        "print(len(y_train_angle),len(y_valid_angle),len(y_train_speed),len(y_valid_speed))"
      ],
      "metadata": {
        "id": "GOcolXKftwa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cc941ba-267b-4efa-a547-d71288561afd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13793\n",
            "10344 3449 10344 3449\n",
            "10344 3449 10344 3449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y_angle)\n",
        "print(Y_speed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sG51hixYameX",
        "outputId": "d674123b-3678-46c3-87d9-c28483079faf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8125, 0.25, 0.625, 0.6875, 0.5625, 0.75, 0.4375, 0.4375, 0.6875, 0.5, 0.625, 0.5, 0.75, 0.6875, 0.8125, 0.4375, 0.6875, 0.75, 0.5, 0.375, 0.5625, 0.6875, 0.5, 0.5625, 0.5625, 0.4375, 0.5, 0.75, 0.6875, 0.4375, 0.4375, 0.75, 0.8125, 0.75, 0.75, 0.4375, 0.6875, 0.5625, 0.75, 0.6875, 0.5, 0.6875, 0.5, 0.5, 0.5625, 0.4375, 0.6875, 0.4375, 0.5, 0.625, 0.75, 0.5625, 0.4375, 0.75, 0.4375, 0.4375, 0.75, 0.5625, 0.75, 0.8125, 0.75, 0.75, 0.6875, 0.6875, 0.625, 0.5625, 0.625, 0.6875, 0.625, 0.625, 0.75, 0.75, 0.75, 0.75, 0.625, 0.4375, 0.625, 0.5625, 0.625, 0.1875, 0.5625, 0.625, 0.5, 0.5, 0.8125, 0.75, 0.4375, 0.75, 0.4375, 0.75, 0.6875, 0.4375, 0.5, 0.8125, 0.75, 0.125, 0.375, 0.5, 0.5, 0.8125, 0.6875, 0.4375, 0.6875, 0.25, 0.8125, 0.4375, 0.625, 0.5, 0.75, 0.625, 0.75, 0.5, 0.6875, 0.625, 0.625, 0.4375, 0.6875, 0.5625, 0.6875, 0.75, 0.625, 0.5, 0.4375, 0.5625, 0.5, 0.5, 0.6875, 0.5, 0.4375, 0.875, 0.6875, 0.6875, 0.75, 0.625, 0.6875, 0.6875, 0.5, 0.4375, 0.5, 0.6875, 0.75, 0.5, 0.5, 0.5625, 0.625, 0.6875, 0.5, 0.5625, 0.5, 0.625, 0.4375, 0.75, 0.4375, 0.4375, 0.4375, 0.3125, 0.75, 0.6875, 0.75, 0.6875, 0.625, 0.625, 0.6875, 0.4375, 0.5, 0.6875, 0.625, 0.25, 0.5, 0.625, 0.4375, 0.5, 0.875, 0.5625, 0.875, 0.8125, 0.625, 0.4375, 0.5625, 0.875, 0.625, 0.6875, 0.625, 0.625, 0.5625, 0.4375, 0.625, 0.4375, 0.25, 0.625, 0.25, 0.625, 0.625, 0.5, 0.625, 0.5, 0.625, 0.4375, 0.5625, 0.5625, 0.6875, 0.5, 0.5625, 0.6875, 0.9375, 0.4375, 0.8125, 0.5, 0.4375, 0.5625, 0.4375, 0.5, 0.375, 0.5625, 0.6875, 0.6875, 0.4375, 0.625, 0.6875, 0.3125, 0.125, 0.375, 0.375, 0.5, 0.1875, 0.5, 0.75, 0.5, 0.375, 0.625, 0.6875, 0.4375, 0.5625, 0.8125, 0.625, 0.8125, 0.3125, 0.5, 0.5625, 0.625, 0.5625, 0.6875, 0.625, 0.6875, 0.8125, 0.5625, 0.6875, 0.5625, 0.6875, 0.5, 0.6875, 0.75, 0.5625, 0.5, 0.625, 0.6875, 0.5625, 0.6875, 0.8125, 0.75, 0.1875, 0.6875, 0.375, 0.5625, 0.625, 0.6875, 0.6875, 0.875, 0.5625, 0.5625, 0.5, 0.875, 0.6875, 0.625, 0.75, 0.75, 0.5625, 0.625, 0.5625, 0.4375, 0.5, 0.625, 0.75, 0.5625, 0.8125, 0.6875, 0.5, 0.625, 0.6875, 0.6875, 0.5, 0.75, 0.5, 0.75, 0.4375, 0.75, 0.5625, 0.3125, 0.8125, 0.625, 0.8125, 0.75, 0.6875, 0.5625, 0.5, 0.625, 0.6875, 0.625, 0.8125, 0.4375, 0.4375, 0.5, 0.6875, 0.5, 0.5, 0.4375, 0.6875, 0.8125, 0.3125, 0.4375, 0.75, 0.5625, 0.4375, 0.5625, 0.75, 0.8125, 0.5625, 0.625, 0.5625, 0.75, 0.125, 0.5625, 0.4375, 0.5625, 0.75, 0.625, 0.4375, 0.375, 0.6875, 0.5625, 0.75, 0.5625, 0.4375, 0.8125, 0.5, 0.6875, 0.625, 0.5625, 0.5, 0.5625, 0.4375, 0.375, 0.4375, 0.75, 0.5625, 0.6875, 0.6875, 0.75, 0.6875, 0.625, 0.6875, 0.375, 0.75, 0.625, 0.6875, 0.5, 0.5, 0.8125, 0.75, 0.6875, 0.75, 0.75, 0.4375, 0.5625, 0.8125, 0.75, 0.75, 0.875, 0.5, 0.6875, 0.6875, 0.4375, 0.4375, 0.5, 0.4375, 0.8125, 0.75, 0.5625, 0.4375, 0.875, 0.5, 0.625, 0.5625, 0.6875, 0.875, 0.625, 0.75, 0.625, 0.75, 0.625, 0.6875, 0.4375, 0.625, 0.5625, 0.8125, 0.6875, 0.75, 0.4375, 0.5625, 0.75, 0.4375, 0.625, 0.6875, 0.6875, 0.5, 0.75, 0.625, 0.6875, 0.5, 0.75, 0.5625, 0.625, 0.625, 0.6875, 0.4375, 0.5, 0.125, 0.375, 0.5625, 0.75, 0.4375, 0.8125, 0.0, 0.6875, 0.5625, 0.8125, 0.8125, 0.4375, 0.5625, 0.4375, 0.625, 0.6875, 0.8125, 0.0625, 0.5625, 0.8125, 0.6875, 0.6875, 0.6875, 0.75, 0.6875, 0.8125, 0.625, 0.5625, 0.4375, 0.6875, 0.75, 0.75, 0.625, 0.5, 0.6875, 0.5, 0.75, 0.625, 0.75, 0.375, 0.8125, 0.4375, 0.6875, 0.625, 0.6875, 0.625, 0.75, 0.5, 0.75, 0.5625, 0.5625, 0.5625, 0.8125, 0.8125, 0.5, 0.5, 0.5, 0.4375, 0.75, 0.5625, 0.4375, 0.75, 0.875, 0.4375, 0.75, 0.75, 0.8125, 0.75, 0.625, 0.4375, 0.75, 0.625, 0.3125, 0.5, 0.75, 0.75, 0.625, 0.625, 0.75, 0.6875, 0.4375, 0.6875, 0.4375, 0.5, 0.75, 0.5, 0.5625, 0.6875, 0.5, 0.625, 0.8125, 0.4375, 0.625, 0.6875, 0.5, 0.5625, 0.5, 0.5, 0.6875, 0.6875, 0.5, 0.375, 0.625, 0.6875, 0.5, 0.3125, 0.625, 0.6875, 0.75, 0.8125, 0.5, 0.5625, 0.4375, 0.875, 0.75, 0.5625, 0.4375, 0.125, 0.1875, 0.5, 0.6875, 0.625, 0.1875, 0.5, 0.6875, 0.5, 0.625, 0.6875, 0.5, 0.625, 0.6875, 0.5625, 0.3125, 0.5, 0.75, 0.5, 0.5, 0.8125, 0.4375, 0.5, 0.8125, 0.75, 0.5, 0.4375, 0.4375, 0.6875, 0.4375, 0.6875, 0.6875, 0.8125, 0.4375, 0.75, 0.625, 0.625, 0.625, 0.75, 0.6875, 0.5, 0.6875, 0.5, 0.75, 0.6875, 0.5, 0.5, 0.625, 0.75, 0.625, 0.3125, 0.5, 0.6875, 0.8125, 0.8125, 0.5, 0.875, 0.5625, 0.6875, 0.8125, 0.625, 0.625, 0.8125, 0.875, 0.625, 0.4375, 0.75, 0.8125, 0.75, 0.625, 0.625, 0.8125, 0.75, 0.6875, 0.75, 0.5, 0.8125, 0.375, 0.8125, 0.5625, 0.625, 0.4375, 0.4375, 0.75, 0.625, 0.625, 1.0, 0.8125, 0.5625, 0.8125, 0.5, 0.75, 0.6875, 0.4375, 0.4375, 0.4375, 0.5, 0.5, 0.25, 0.625, 0.875, 0.8125, 0.625, 0.1875, 0.625, 0.5625, 0.8125, 0.5, 0.5625, 0.75, 0.75, 0.625, 0.6875, 0.5625, 0.4375, 0.5625, 0.1875, 0.5625, 0.625, 0.6875, 0.5625, 0.5, 0.5625, 0.75, 0.625, 0.625, 0.5, 0.6875, 0.75, 0.625, 0.6875, 0.6875, 0.75, 0.5, 0.5625, 0.625, 0.5, 0.75, 0.75, 0.75, 0.6875, 0.625, 0.4375, 0.5625, 0.625, 0.6875, 0.8125, 0.5625, 0.5, 0.8125, 0.5625, 0.5, 0.625, 0.4375, 0.5625, 0.8125, 0.5625, 0.75, 0.6875, 0.8125, 0.5, 0.75, 0.6875, 0.4375, 0.625, 0.625, 0.625, 0.75, 0.75, 0.75, 0.5625, 0.625, 0.5625, 0.75, 0.75, 0.4375, 0.6875, 0.4375, 0.8125, 0.1875, 0.5, 0.4375, 0.75, 0.75, 0.5, 0.4375, 0.75, 0.625, 0.75, 0.5, 0.0, 0.625, 0.375, 0.5625, 0.4375, 0.5, 0.5625, 0.375, 0.625, 0.5625, 0.5, 0.75, 0.625, 0.4375, 0.5, 0.5, 0.5, 0.625, 0.625, 0.75, 0.6875, 0.875, 0.75, 0.5625, 0.5, 0.5625, 0.625, 0.8125, 0.6875, 0.75, 0.5, 0.625, 0.75, 0.4375, 0.5, 0.625, 0.8125, 0.8125, 0.5625, 0.625, 0.625, 0.75, 0.5, 0.8125, 0.75, 0.5, 0.5625, 0.5, 0.625, 0.75, 0.6875, 0.5, 0.625, 0.625, 0.4375, 0.625, 0.4375, 0.3125, 0.625, 0.4375, 0.625, 0.75, 0.6875, 0.6875, 0.375, 0.5, 0.5625, 0.4375, 0.625, 0.5, 0.0, 0.625, 0.6875, 0.5, 0.6875, 0.375, 0.5, 0.6875, 0.75, 0.5625, 0.625, 0.5, 0.75, 0.625, 0.625, 0.75, 0.8125, 0.5, 0.5625, 0.625, 0.75, 0.8125, 0.6875, 0.4375, 0.625, 0.75, 0.5625, 0.5, 0.5, 0.8125, 0.5625, 0.8125, 0.625, 0.5, 0.75, 0.75, 0.625, 0.4375, 0.625, 0.5625, 0.5625, 0.5, 0.4375, 0.625, 0.5625, 0.6875, 0.6875, 0.625, 0.5, 0.6875, 0.1875, 0.625, 0.75, 0.5, 0.4375, 0.5625, 0.75, 0.4375, 0.625, 0.625, 0.75, 0.75, 0.8125, 0.0, 0.5625, 0.625, 0.8125, 0.5625, 0.8125, 0.6875, 0.625, 0.5, 0.5625, 0.625, 0.4375, 0.5, 0.5, 0.75, 0.8125, 0.75, 0.5, 0.5, 0.8125, 0.4375, 0.4375, 0.8125, 0.75, 0.5, 0.8125, 0.3125, 0.625, 0.5, 0.75, 0.5, 0.5625, 0.8125, 0.5, 0.4375, 0.5, 0.5, 0.6875, 0.5, 0.4375, 0.5625, 0.6875, 0.6875, 0.8125, 0.5, 0.6875, 0.5625, 0.125, 0.8125, 0.6875, 0.8125, 0.75, 0.625, 0.5, 0.4375, 0.75, 0.5, 0.875, 0.6875, 0.5, 0.4375, 0.5625, 0.5625, 0.75, 0.5, 0.5, 0.4375, 0.625, 0.6875, 0.4375, 0.3125, 0.375, 0.75, 0.625, 0.625, 0.6875, 0.625, 0.625, 0.5625, 0.5, 0.5625, 0.375, 0.5625, 0.5, 0.625, 0.875, 0.5, 0.75, 0.4375, 0.6875, 0.625, 0.6875, 0.4375, 0.5625, 0.75, 0.625, 0.75, 0.8125, 0.4375, 0.5, 0.75, 0.5, 0.75, 0.5, 0.6875, 0.625, 0.75, 0.75, 0.5, 0.8125, 0.6875, 0.1875, 0.5625, 0.5625, 0.75, 0.75, 0.6875, 0.5, 0.625, 0.625, 0.6875, 0.375, 0.75, 0.6875, 0.4375, 0.75, 0.5625, 0.5625, 0.5, 0.4375, 0.5, 0.5625, 0.5625, 0.6875, 0.625, 0.625, 0.5, 0.5625, 0.3125, 0.5, 0.5, 0.75, 0.5, 0.6875, 0.5625, 0.4375, 0.6875, 0.5, 0.625, 0.0625, 0.75, 0.5, 0.375, 0.4375, 0.6875, 0.25, 0.4375, 0.5, 0.4375, 0.75, 0.375, 0.6875, 0.6875, 0.625, 0.6875, 0.0, 0.75, 0.4375, 0.5, 0.5, 0.5625, 0.5, 0.75, 0.625, 0.4375, 0.1875, 0.5, 0.6875, 0.75, 0.625, 0.8125, 0.75, 0.625, 0.4375, 0.625, 0.8125, 0.5, 0.6875, 0.5625, 0.5625, 0.5, 0.75, 0.8125, 0.4375, 0.875, 0.3125, 0.6875, 0.5, 0.625, 0.375, 0.75, 0.5625, 0.6875, 0.5, 0.5625, 0.8125, 0.75, 0.625, 0.375, 0.5, 0.5, 0.5625, 0.5625, 0.5, 0.5, 0.4375, 0.5, 0.8125, 0.625, 0.5, 0.75, 0.6875, 0.75, 0.6875, 1.0, 0.5625, 0.5, 0.5, 0.625, 0.75, 0.5, 0.5, 0.625, 0.875, 0.5, 0.8125, 0.5625, 0.875, 0.5, 0.4375, 0.75, 0.4375, 0.625, 0.75, 0.4375, 0.375, 0.4375, 0.375, 0.75, 0.8125, 0.5625, 0.75, 0.75, 0.625, 0.75, 0.25, 0.375, 0.5, 0.625, 0.5625, 0.625, 0.8125, 0.75, 0.5625, 0.75, 0.625, 0.5625, 0.625, 0.75, 0.8125, 0.8125, 0.6875, 0.6875, 0.6875, 0.75, 0.75, 0.8125, 0.6875, 0.5, 0.4375, 0.75, 0.8125, 0.6875, 0.6875, 0.75, 0.75, 0.375, 0.625, 0.4375, 0.625, 0.4375, 0.3125, 0.75, 0.4375, 0.6875, 0.5625, 0.625, 0.6875, 0.625, 0.6875, 0.3125, 0.5625, 0.75, 0.625, 0.625, 0.625, 0.75, 0.4375, 0.4375, 0.5, 0.625, 0.625, 0.5, 0.5625, 0.4375, 0.75, 0.6875, 0.5, 0.6875, 0.75, 0.8125, 0.5, 0.6875, 0.625, 0.6875, 0.75, 0.75, 0.875, 0.1875, 0.625, 0.625, 0.8125, 0.8125, 0.6875, 0.4375, 0.6875, 0.5625, 0.3125, 0.625, 0.75, 0.75, 0.5, 0.5, 0.375, 0.8125, 0.625, 0.6875, 0.75, 0.5, 0.5625, 0.5, 0.6875, 0.5, 0.5625, 0.625, 0.75, 0.5625, 0.5625, 0.75, 0.75, 0.6875, 0.375, 0.5, 0.6875, 0.75, 0.625, 0.5, 0.375, 0.5625, 0.4375, 0.75, 0.75, 0.4375, 0.8125, 0.75, 0.5, 0.5625, 0.6875, 0.4375, 0.5625, 0.25, 0.5625, 0.625, 0.625, 0.5625, 0.5625, 0.8125, 0.625, 0.75, 0.75, 0.75, 0.5625, 0.5625, 0.75, 0.625, 0.4375, 0.8125, 0.5, 0.875, 0.5, 0.875, 0.5625, 0.625, 0.75, 0.75, 0.75, 0.375, 0.5625, 0.625, 0.6875, 0.375, 0.625, 0.6875, 0.5, 0.75, 0.125, 0.4375, 0.6875, 0.75, 0.8125, 0.4375, 0.5, 0.4375, 0.625, 0.6875, 0.625, 0.875, 0.5, 0.5, 0.6875, 0.4375, 0.6875, 0.5, 0.4375, 0.6875, 0.6875, 0.5625, 0.4375, 0.625, 0.4375, 0.625, 0.8125, 0.6875, 0.625, 0.625, 0.6875, 0.8125, 0.4375, 0.625, 0.4375, 0.25, 0.8125, 0.6875, 0.75, 0.5625, 0.625, 0.5625, 0.625, 0.5, 0.8125, 0.6875, 0.5625, 0.5, 0.8125, 0.6875, 0.625, 0.625, 0.8125, 0.625, 0.6875, 0.5, 0.5625, 0.5, 0.625, 0.5, 0.3125, 0.75, 0.5, 0.5, 0.4375, 0.8125, 0.5625, 0.3125, 0.6875, 0.75, 0.5, 0.6875, 0.625, 0.625, 0.6875, 0.5625, 0.75, 0.8125, 0.5625, 0.125, 0.6875, 0.75, 0.5, 0.5, 0.6875, 0.5625, 0.5625, 0.5, 0.5, 0.625, 0.75, 0.6875, 0.75, 0.75, 0.75, 0.5, 0.375, 0.8125, 0.5, 0.4375, 0.625, 0.5, 0.6875, 0.5625, 0.875, 0.5625, 0.9375, 0.75, 0.6875, 0.5625, 0.75, 0.625, 0.6875, 0.6875, 0.5, 0.625, 0.4375, 0.6875, 0.6875, 0.5, 0.625, 0.4375, 0.4375, 0.5625, 0.75, 0.375, 0.5625, 0.625, 0.375, 0.8125, 0.3125, 0.5, 0.75, 0.625, 0.25, 0.6875, 0.375, 0.8125, 0.8125, 0.75, 0.75, 0.8125, 0.75, 0.5, 0.75, 0.5, 0.8125, 0.8125, 0.75, 0.5, 0.5, 0.5625, 0.5625, 0.875, 0.625, 0.75, 0.3125, 0.5, 0.75, 0.625, 0.6875, 0.625, 0.6875, 0.6875, 0.4375, 0.6875, 0.8125, 0.6875, 0.875, 0.6875, 0.8125, 0.625, 0.5625, 0.0, 0.4375, 0.5625, 0.4375, 0.375, 0.5, 0.4375, 0.625, 0.25, 0.5, 0.75, 0.6875, 0.75, 0.5, 0.625, 0.625, 0.6875, 0.75, 0.8125, 0.5, 0.5625, 0.5625, 0.5, 0.6875, 0.5625, 0.4375, 0.5, 0.75, 0.5625, 0.6875, 0.625, 0.6875, 0.75, 0.6875, 0.4375, 0.5625, 0.6875, 0.625, 0.4375, 0.8125, 0.6875, 0.8125, 0.625, 0.75, 0.75, 0.625, 0.5, 0.8125, 0.8125, 0.75, 0.8125, 0.5, 0.5, 0.4375, 0.375, 0.5625, 0.6875, 0.5, 0.6875, 0.5, 0.375, 0.75, 0.1875, 0.75, 0.8125, 0.75, 0.75, 0.6875, 0.9375, 0.5, 0.5, 0.6875, 0.5625, 0.8125, 0.625, 0.75, 0.5625, 0.5, 0.625, 0.6875, 0.6875, 0.625, 0.5, 0.5625, 0.6875, 0.625, 0.8125, 0.125, 0.625, 0.5, 0.5, 0.625, 0.375, 0.6875, 0.75, 0.75, 0.875, 0.8125, 0.625, 0.625, 0.5625, 0.5625, 0.5, 0.5625, 0.5625, 0.8125, 0.625, 0.5, 0.625, 0.6875, 0.6875, 0.625, 0.5625, 0.8125, 0.5625, 0.75, 0.8125, 0.5, 0.375, 0.625, 0.9375, 0.6875, 0.5625, 0.875, 0.3125, 0.8125, 0.8125, 0.75, 0.5, 0.75, 0.625, 0.75, 0.5, 0.4375, 0.75, 0.3125, 0.5625, 0.375, 0.4375, 0.5, 0.8125, 0.8125, 0.6875, 0.8125, 0.375, 0.1875, 0.5625, 0.625, 0.625, 0.75, 0.6875, 0.5625, 0.5625, 0.75, 0.5, 0.9375, 0.75, 0.6875, 0.4375, 0.5625, 0.5625, 0.625, 0.6875, 0.75, 0.75, 0.625, 0.5, 0.6875, 0.8125, 0.6875, 0.6875, 0.6875, 0.875, 0.5, 0.6875, 0.5, 0.375, 0.4375, 0.6875, 0.3125, 0.3125, 0.6875, 0.75, 0.6875, 0.3125, 0.625, 0.625, 0.5625, 0.6875, 0.4375, 0.375, 0.5, 0.75, 0.75, 0.75, 0.5, 0.875, 0.8125, 0.5625, 0.6875, 0.5, 0.625, 0.5625, 0.6875, 0.5, 0.75, 0.75, 0.8125, 0.5625, 0.25, 0.75, 0.3125, 0.875, 0.5, 0.375, 0.5625, 1.0, 0.625, 0.625, 0.4375, 0.5, 0.6875, 0.0, 0.4375, 0.5, 0.625, 0.75, 0.625, 0.625, 0.5, 0.6875, 0.625, 0.125, 0.5, 0.75, 0.625, 0.5, 0.75, 0.6875, 0.6875, 0.625, 0.5, 0.6875, 0.5, 0.6875, 0.5625, 0.5625, 0.625, 0.3125, 0.625, 0.5, 0.75, 0.5625, 0.5625, 0.4375, 0.8125, 0.625, 0.75, 0.6875, 0.8125, 0.5, 0.4375, 0.5, 0.5625, 0.6875, 0.6875, 0.3125, 0.5625, 0.6875, 0.625, 0.5625, 0.5, 0.625, 0.75, 0.4375, 0.6875, 0.75, 0.5, 0.625, 0.625, 0.375, 0.75, 0.75, 0.8125, 0.8125, 0.75, 0.5625, 0.75, 0.8125, 0.625, 0.5625, 0.4375, 0.4375, 0.625, 0.8125, 0.4375, 0.375, 0.625, 0.5625, 0.5625, 0.75, 0.3125, 0.5625, 0.5, 0.375, 0.6875, 0.625, 0.6875, 0.6875, 0.625, 0.75, 0.8125, 0.5625, 0.5, 0.6875, 0.4375, 0.5625, 0.6875, 0.75, 0.6875, 0.8125, 0.8125, 0.375, 0.8125, 0.8125, 0.4375, 0.375, 0.5625, 0.6875, 0.6875, 0.75, 0.4375, 0.625, 0.3125, 0.625, 0.5625, 0.4375, 0.375, 0.8125, 0.8125, 0.6875, 0.75, 0.3125, 0.5625, 0.4375, 0.5, 0.75, 0.8125, 0.375, 0.5625, 0.625, 0.4375, 0.4375, 0.625, 0.625, 0.6875, 0.5625, 0.8125, 0.625, 0.75, 0.8125, 0.4375, 0.4375, 0.75, 0.5, 0.6875, 0.5, 0.5, 0.75, 0.75, 0.6875, 0.5625, 0.8125, 0.75, 0.6875, 0.5, 0.75, 0.75, 0.4375, 0.75, 0.4375, 0.5, 0.6875, 0.5, 0.5, 0.6875, 0.5, 0.625, 0.375, 0.75, 0.5, 0.5625, 0.75, 0.375, 0.6875, 0.5, 0.4375, 0.5, 0.8125, 0.625, 0.6875, 0.4375, 0.375, 0.5, 0.4375, 0.75, 0.5, 0.25, 0.6875, 0.8125, 0.8125, 0.4375, 0.5, 0.8125, 0.5, 0.5, 0.75, 0.5, 0.3125, 0.75, 0.5, 0.6875, 0.5625, 0.75, 0.875, 0.75, 0.125, 0.6875, 0.5, 0.5625, 0.5625, 0.5, 0.375, 0.4375, 0.6875, 0.375, 0.75, 0.75, 0.5, 0.5625, 0.625, 0.5, 0.625, 0.0, 0.5625, 0.375, 0.5625, 0.4375, 0.25, 0.6875, 0.625, 0.5, 0.625, 0.5, 0.75, 0.8125, 0.5625, 0.5, 0.625, 0.625, 0.5, 0.4375, 0.4375, 0.875, 0.6875, 0.75, 0.5, 0.5625, 0.8125, 0.6875, 0.5, 0.625, 0.4375, 0.5, 0.5, 0.6875, 0.4375, 0.5, 0.6875, 0.625, 0.8125, 0.75, 0.6875, 0.5625, 0.8125, 0.75, 0.75, 0.5, 0.3125, 0.625, 0.5, 0.625, 0.5625, 0.625, 0.8125, 0.4375, 0.4375, 0.75, 0.75, 0.8125, 0.4375, 0.625, 0.6875, 0.5, 0.3125, 0.75, 0.6875, 0.5625, 0.4375, 0.875, 0.75, 0.5, 0.5625, 0.75, 0.6875, 0.3125, 0.75, 0.5, 0.5, 0.375, 0.4375, 0.5, 0.6875, 0.4375, 0.625, 0.3125, 0.625, 0.5625, 0.75, 0.8125, 0.4375, 0.6875, 0.5625, 0.5625, 0.5, 0.6875, 0.75, 0.75, 0.4375, 0.6875, 0.3125, 0.625, 0.5625, 0.5625, 0.625, 0.8125, 0.8125, 0.6875, 0.5, 0.6875, 0.75, 0.5, 0.625, 0.4375, 0.5625, 0.4375, 0.625, 0.625, 0.75, 0.6875, 0.5, 0.6875, 0.8125, 0.4375, 0.4375, 0.3125, 0.5625, 0.5, 0.75, 0.5625, 0.125, 0.6875, 0.8125, 0.8125, 0.5625, 0.4375, 0.375, 0.5625, 0.875, 0.5, 0.5, 0.6875, 0.9375, 0.4375, 0.625, 0.8125, 0.4375, 0.4375, 0.6875, 0.625, 0.4375, 0.75, 0.5, 0.375, 0.75, 0.625, 0.875, 0.4375, 0.6875, 0.6875, 0.8125, 0.4375, 0.75, 0.8125, 0.5, 0.3125, 0.625, 0.5625, 0.4375, 0.625, 0.4375, 0.5, 0.1875, 0.625, 0.4375, 0.5, 0.8125, 0.625, 0.5, 0.6875, 0.5, 0.5, 0.5625, 0.8125, 0.625, 0.8125, 0.75, 0.5, 0.4375, 0.5625, 0.6875, 0.6875, 0.8125, 0.5625, 0.625, 0.4375, 0.75, 0.4375, 0.75, 0.5625, 0.4375, 0.4375, 0.75, 0.25, 0.8125, 0.6875, 0.5625, 0.4375, 0.75, 0.75, 0.4375, 0.8125, 0.5625, 0.75, 0.5, 0.75, 0.625, 0.4375, 0.5625, 0.5625, 0.75, 0.5, 0.5625, 0.5, 0.75, 0.625, 0.5, 0.6875, 0.625, 0.5, 0.4375, 0.6875, 0.625, 0.4375, 0.625, 0.625, 0.6875, 0.5, 0.5, 0.5625, 0.375, 0.5, 0.5, 0.8125, 0.5625, 0.625, 0.5625, 0.5, 0.6875, 0.5625, 0.5625, 0.625, 0.4375, 0.75, 0.5, 0.75, 0.6875, 0.75, 0.5, 0.875, 0.8125, 0.3125, 0.625, 0.8125, 0.6875, 0.8125, 0.6875, 0.75, 0.875, 0.75, 0.8125, 0.6875, 0.5625, 0.5, 0.625, 0.875, 0.75, 0.5625, 0.625, 0.4375, 0.5, 0.375, 0.625, 0.8125, 0.75, 0.5, 0.75, 0.75, 0.6875, 0.8125, 0.4375, 0.4375, 0.625, 0.625, 0.8125, 0.625, 0.75, 0.8125, 0.5625, 0.8125, 0.8125, 0.4375, 0.875, 0.3125, 0.625, 0.75, 0.5625, 0.6875, 0.4375, 0.75, 0.6875, 0.6875, 0.8125, 0.375, 0.5625, 0.4375, 0.5, 0.6875, 0.375, 0.5625, 0.5625, 0.5625, 0.625, 0.4375, 0.6875, 0.625, 0.625, 0.75, 0.6875, 0.5, 0.8125, 0.625, 0.75, 0.5, 0.75, 0.5, 0.6875, 0.5, 0.625, 0.5, 0.625, 0.625, 0.5, 0.375, 0.4375, 0.625, 0.75, 0.625, 0.8125, 0.75, 0.5, 0.8125, 0.6875, 0.6875, 0.875, 0.5625, 0.375, 0.8125, 0.5, 0.4375, 0.6875, 0.6875, 0.6875, 0.75, 0.5, 0.4375, 0.6875, 0.75, 0.75, 0.375, 0.8125, 0.375, 0.625, 0.5625, 0.6875, 0.75, 0.5625, 0.6875, 0.6875, 0.625, 0.6875, 0.8125, 0.75, 0.5, 0.75, 0.75, 0.75, 0.25, 0.6875, 0.75, 0.5, 0.5625, 0.5625, 0.5, 0.5, 0.5, 0.6875, 0.625, 0.6875, 0.625, 0.5, 0.5, 0.4375, 0.5, 0.3125, 0.5, 0.375, 0.4375, 0.5, 0.5625, 0.875, 0.6875, 0.5, 0.625, 0.75, 0.4375, 0.6875, 0.6875, 0.5625, 0.4375, 0.5, 0.5, 0.5625, 0.375, 0.4375, 0.5, 0.375, 0.5625, 0.6875, 0.625, 0.3125, 0.5, 0.3125, 0.6875, 0.5625, 0.6875, 0.625, 0.8125, 0.5, 0.6875, 0.8125, 0.75, 0.875, 0.8125, 0.875, 0.875, 0.875, 0.6875, 0.5625, 0.5, 0.6875, 0.5, 0.6875, 0.5, 0.75, 0.6875, 0.5625, 0.5, 0.5625, 0.5, 0.5, 0.625, 0.375, 0.625, 0.1875, 0.9375, 0.6875, 0.75, 0.8125, 0.6875, 0.5625, 0.6875, 0.625, 0.6875, 0.5, 0.8125, 0.625, 0.75, 0.6875, 0.75, 0.625, 0.75, 0.75, 0.75, 0.0, 0.625, 0.75, 0.5625, 0.75, 0.625, 0.8125, 0.4375, 0.8125, 0.6875, 0.5, 0.5625, 0.5625, 0.6875, 0.5, 0.875, 0.6875, 0.5625, 0.625, 0.75, 0.4375, 0.6875, 0.75, 0.5, 0.1875, 0.75, 0.5, 0.75, 0.6875, 0.625, 0.875, 0.625, 0.5, 0.625, 0.625, 0.5, 0.375, 0.625, 0.8125, 0.375, 0.375, 0.6875, 0.25, 0.4375, 0.8125, 0.6875, 0.75, 0.375, 0.75, 0.75, 0.625, 0.6875, 0.75, 0.6875, 0.4375, 0.6875, 0.625, 0.5625, 0.75, 0.6875, 0.8125, 0.75, 0.5, 0.375, 0.3125, 0.4375, 0.5625, 0.625, 0.625, 0.375, 0.5625, 0.8125, 0.6875, 0.4375, 0.375, 0.75, 0.5, 0.5, 0.5625, 0.8125, 0.4375, 0.5, 0.75, 0.5, 0.4375, 0.375, 0.75, 0.5625, 0.625, 0.625, 0.6875, 0.875, 0.6875, 0.5, 0.6875, 0.75, 0.3125, 0.625, 0.4375, 0.4375, 0.75, 0.5, 0.5, 0.5625, 0.5, 0.6875, 0.75, 0.5, 0.75, 0.4375, 0.4375, 0.625, 0.6875, 0.4375, 0.8125, 0.5, 0.5625, 0.625, 0.5, 0.625, 0.625, 0.4375, 0.6875, 0.5625, 0.5, 0.8125, 0.5625, 0.375, 0.625, 0.3125, 0.6875, 0.5625, 0.6875, 0.6875, 0.5, 0.75, 0.625, 0.625, 0.875, 0.75, 0.5, 0.8125, 0.8125, 0.6875, 0.6875, 0.4375, 0.75, 0.4375, 0.8125, 0.375, 0.75, 0.5, 0.4375, 0.6875, 0.75, 0.375, 0.4375, 0.6875, 0.4375, 0.5625, 0.875, 0.6875, 0.5, 0.5625, 0.8125, 0.4375, 0.6875, 0.75, 0.75, 0.6875, 0.6875, 0.5625, 0.8125, 0.625, 0.625, 0.6875, 0.625, 0.6875, 0.4375, 0.75, 0.625, 0.5, 0.6875, 0.375, 0.5, 0.75, 0.4375, 0.6875, 0.8125, 0.375, 0.5625, 0.6875, 0.75, 0.6875, 0.5, 0.6875, 0.0, 0.625, 0.5625, 0.625, 0.6875, 0.75, 0.75, 0.875, 0.5, 0.5625, 0.5, 1.0, 0.375, 0.6875, 0.5625, 0.4375, 0.625, 0.875, 0.75, 0.75, 0.6875, 0.5, 0.6875, 0.6875, 0.625, 0.5625, 0.6875, 0.4375, 0.4375, 0.8125, 0.6875, 0.4375, 0.25, 0.625, 0.8125, 0.625, 0.6875, 0.5, 0.5, 0.5625, 0.5, 0.4375, 0.75, 0.875, 0.625, 0.5625, 0.5, 0.5625, 0.375, 0.6875, 0.75, 0.625, 0.5, 0.5625, 0.4375, 0.375, 0.5, 0.625, 0.4375, 0.375, 0.4375, 0.75, 0.5625, 0.625, 0.75, 0.6875, 0.8125, 0.5625, 0.6875, 0.5, 0.5625, 0.75, 0.5, 0.5, 0.6875, 0.4375, 0.5, 0.75, 0.4375, 0.625, 0.5, 0.625, 0.75, 0.75, 0.4375, 0.4375, 0.625, 0.625, 0.8125, 0.5, 0.8125, 0.6875, 0.5625, 0.75, 0.6875, 0.75, 0.5625, 0.75, 0.4375, 0.75, 0.625, 0.4375, 0.8125, 0.375, 0.625, 0.5625, 0.625, 0.5, 0.625, 0.625, 0.6875, 0.25, 0.6875, 0.5, 0.4375, 0.5, 0.625, 0.625, 0.75, 0.8125, 0.625, 0.4375, 0.6875, 0.6875, 0.8125, 0.6875, 0.5625, 0.625, 0.5, 0.5625, 0.5, 0.75, 0.5625, 0.5, 0.625, 0.5, 0.5, 0.375, 0.625, 0.5, 0.875, 0.625, 0.8125, 0.875, 0.5625, 0.75, 0.6875, 0.5625, 0.625, 0.5625, 0.75, 0.5, 0.9375, 0.6875, 0.8125, 0.75, 0.4375, 0.5, 0.5625, 0.625, 0.6875, 0.5, 0.5625, 0.625, 0.4375, 0.5, 0.625, 0.75, 0.4375, 0.75, 0.5, 0.75, 0.625, 0.625, 0.5625, 0.6875, 0.5, 0.4375, 0.5, 0.6875, 0.75, 0.3125, 0.625, 0.5, 0.8125, 0.6875, 0.4375, 0.5625, 0.625, 0.5, 0.8125, 0.625, 0.875, 0.625, 0.625, 0.6875, 0.8125, 0.625, 0.8125, 0.75, 0.625, 0.875, 0.625, 0.8125, 0.6875, 0.625, 0.5625, 0.875, 0.3125, 0.375, 0.8125, 0.3125, 0.5, 0.5, 0.125, 0.5, 0.6875, 0.6875, 0.25, 0.1875, 0.5, 0.4375, 0.75, 0.4375, 0.625, 0.75, 0.6875, 0.6875, 0.75, 0.8125, 0.6875, 0.5, 0.6875, 0.625, 0.75, 0.8125, 0.5625, 0.5, 0.5, 0.5625, 0.8125, 0.625, 0.375, 0.625, 0.3125, 0.625, 0.5625, 0.6875, 0.875, 0.6875, 0.75, 0.8125, 0.5, 0.75, 0.6875, 0.5625, 0.4375, 0.4375, 0.75, 0.75, 0.5625, 0.25, 1.0, 0.5625, 0.5625, 0.75, 0.75, 0.4375, 0.75, 0.8125, 0.6875, 0.75, 0.375, 0.625, 0.5625, 0.5, 0.3125, 0.75, 0.5, 0.75, 0.5, 0.4375, 0.5625, 0.8125, 0.75, 0.375, 0.8125, 0.375, 0.1875, 0.4375, 0.875, 0.625, 0.6875, 0.625, 0.625, 0.75, 0.6875, 0.5, 0.5625, 0.75, 0.75, 0.625, 0.5, 0.625, 0.5625, 0.625, 0.625, 0.625, 0.5, 0.5, 0.5, 0.75, 0.6875, 0.4375, 0.6875, 0.8125, 0.8125, 0.5625, 0.8125, 0.5625, 0.5625, 0.8125, 0.5625, 1.0, 1.0, 0.625, 0.75, 0.6875, 0.6875, 0.75, 0.625, 0.6875, 0.6875, 0.5625, 0.4375, 0.6875, 0.4375, 0.75, 0.5625, 0.75, 0.75, 0.5625, 0.6875, 0.8125, 0.4375, 0.5, 0.5, 0.75, 0.75, 0.625, 0.8125, 0.625, 0.75, 0.5, 0.625, 0.75, 0.625, 0.5, 0.3125, 0.625, 0.75, 0.5, 0.4375, 0.625, 0.5625, 0.625, 0.75, 0.5, 0.5625, 0.4375, 0.5625, 0.75, 0.625, 0.5625, 0.75, 0.5625, 0.4375, 0.75, 0.4375, 0.875, 0.6875, 0.4375, 0.8125, 0.6875, 0.625, 0.5, 0.6875, 0.8125, 0.6875, 0.5, 0.8125, 0.625, 0.5, 0.5, 0.5625, 0.8125, 0.625, 0.625, 0.5625, 0.5625, 0.6875, 0.5, 0.4375, 0.5, 0.5, 0.6875, 0.5, 0.6875, 0.4375, 0.5625, 0.625, 0.8125, 0.5, 0.5625, 0.75, 0.4375, 0.8125, 0.5, 0.8125, 0.5625, 0.625, 0.6875, 0.75, 0.625, 0.6875, 0.5625, 0.6875, 0.4375, 0.3125, 0.5, 0.625, 0.4375, 0.8125, 0.0625, 0.4375, 0.875, 0.8125, 0.75, 0.25, 0.75, 0.6875, 0.5, 0.625, 0.5625, 0.5625, 0.6875, 0.625, 0.75, 0.6875, 0.5625, 0.75, 0.8125, 0.75, 0.75, 0.5625, 0.75, 0.5, 0.4375, 0.8125, 0.5, 0.5, 0.75, 0.8125, 0.6875, 0.5625, 0.5, 0.6875, 0.6875, 0.625, 0.4375, 0.6875, 0.4375, 0.6875, 0.625, 0.5, 0.8125, 0.4375, 0.5, 0.625, 0.625, 0.6875, 0.5625, 0.6875, 0.6875, 0.75, 0.4375, 0.75, 0.625, 0.75, 0.5, 0.6875, 0.5625, 0.6875, 0.4375, 0.625, 0.6875, 0.6875, 0.5625, 0.75, 0.4375, 0.5, 0.0, 0.5, 0.5625, 0.4375, 0.8125, 0.5625, 0.375, 0.75, 0.6875, 0.5, 0.5, 0.625, 0.6875, 0.1875, 0.6875, 0.5, 0.5625, 0.4375, 0.625, 0.75, 0.25, 0.5, 0.6875, 0.5, 0.625, 0.75, 0.625, 0.625, 0.4375, 0.625, 0.5, 0.5625, 0.5625, 0.5, 0.75, 0.8125, 0.6875, 0.6875, 0.8125, 0.6875, 0.5, 0.8125, 0.375, 0.6875, 0.6875, 0.75, 0.5625, 0.5625, 0.875, 0.3125, 0.5625, 0.4375, 0.0, 0.75, 0.625, 0.5625, 0.75, 0.5, 0.5625, 0.6875, 0.5625, 0.625, 0.75, 0.8125, 0.75, 0.6875, 0.5, 0.4375, 0.75, 0.6875, 0.8125, 0.4375, 0.6875, 0.5, 0.8125, 0.6875, 0.75, 0.6875, 0.5, 0.6875, 0.5625, 0.5, 0.75, 0.6875, 0.8125, 0.4375, 0.5625, 0.625, 0.6875, 0.3125, 0.625, 0.6875, 0.75, 0.6875, 0.625, 0.4375, 0.625, 0.75, 0.75, 0.4375, 0.5625, 0.75, 0.5, 0.125, 0.75, 0.5625, 0.75, 0.5625, 0.625, 0.75, 0.625, 0.375, 0.5625, 0.5, 0.625, 0.6875, 0.4375, 0.375, 0.625, 0.6875, 0.5, 0.75, 0.5625, 0.4375, 0.75, 0.6875, 0.6875, 0.5, 0.5625, 0.75, 0.625, 0.5, 0.625, 0.75, 0.625, 0.5, 0.6875, 0.5625, 0.8125, 0.8125, 0.6875, 0.4375, 0.5, 0.4375, 0.4375, 0.5, 0.5625, 0.625, 0.625, 0.6875, 0.5, 0.4375, 0.4375, 0.625, 0.5, 0.75, 0.5625, 0.5, 0.4375, 0.5, 0.6875, 0.75, 0.625, 0.625, 0.4375, 0.6875, 0.5625, 0.8125, 0.625, 0.8125, 0.625, 0.6875, 0.5625, 0.5625, 0.8125, 0.75, 0.4375, 0.75, 0.75, 0.75, 0.8125, 0.75, 0.625, 0.875, 0.5625, 0.4375, 0.75, 0.6875, 0.5625, 0.125, 0.5625, 0.625, 0.4375, 0.4375, 0.6875, 0.8125, 0.5625, 0.6875, 0.8125, 0.75, 0.6875, 0.4375, 0.6875, 0.375, 0.625, 0.5625, 0.625, 0.5, 0.4375, 0.5625, 0.6875, 0.75, 0.5, 0.75, 0.4375, 0.5, 0.75, 0.8125, 0.6875, 0.625, 0.5625, 0.4375, 0.5625, 0.625, 0.5625, 0.125, 0.75, 0.4375, 0.5625, 0.75, 0.8125, 0.6875, 0.75, 0.5, 0.5, 0.75, 0.6875, 0.5, 0.75, 0.6875, 0.75, 0.75, 0.6875, 0.8125, 0.5625, 0.6875, 0.625, 0.5, 0.75, 0.375, 0.5, 0.625, 0.8125, 0.75, 0.75, 0.8125, 0.3125, 0.4375, 0.8125, 0.8125, 0.5, 0.625, 0.5, 0.5, 0.75, 0.4375, 0.75, 0.75, 0.6875, 0.4375, 0.875, 0.0, 0.4375, 0.4375, 0.5625, 0.75, 0.625, 0.6875, 0.625, 0.25, 0.75, 0.625, 0.5, 0.6875, 0.625, 0.625, 0.5, 0.5625, 0.375, 0.5625, 0.5625, 0.8125, 0.5, 0.6875, 0.625, 0.5, 0.4375, 0.625, 0.6875, 0.4375, 0.625, 0.6875, 0.5, 0.625, 0.4375, 0.4375, 0.75, 0.75, 0.625, 0.6875, 0.5625, 0.5, 0.4375, 0.4375, 0.625, 0.625, 0.5, 0.125, 0.5625, 0.6875, 0.4375, 0.8125, 0.8125, 0.5, 0.625, 0.625, 0.625, 0.875, 0.5, 0.625, 0.5, 0.3125, 0.75, 0.5, 0.5625, 0.75, 0.625, 0.5, 0.8125, 0.75, 0.9375, 0.8125, 0.375, 0.875, 0.6875, 0.5, 0.5, 0.75, 0.75, 0.125, 0.75, 0.5, 0.75, 0.6875, 0.3125, 0.0625, 0.6875, 0.625, 0.6875, 0.5, 0.4375, 0.5, 0.6875, 0.5625, 0.375, 0.375, 0.75, 0.4375, 0.75, 0.75, 0.5, 0.5, 0.75, 0.375, 0.6875, 0.625, 0.6875, 0.6875, 0.75, 0.375, 0.875, 0.75, 0.6875, 0.5, 0.5625, 0.5, 0.75, 0.625, 0.625, 0.625, 0.5, 0.875, 0.75, 0.5, 0.75, 0.6875, 0.625, 0.5, 0.4375, 0.8125, 0.875, 0.4375, 0.625, 0.875, 0.4375, 0.6875, 0.75, 0.6875, 0.625, 0.5625, 0.5625, 0.8125, 0.75, 0.75, 0.3125, 0.75, 0.4375, 0.6875, 0.6875, 0.4375, 0.1875, 0.625, 1.0, 0.6875, 0.6875, 0.6875, 0.6875, 0.75, 0.75, 0.5625, 0.75, 0.6875, 0.4375, 0.625, 0.5625, 0.6875, 0.5625, 0.6875, 0.625, 0.75, 0.5625, 0.5625, 0.5, 0.5, 0.75, 0.5625, 0.5, 0.5, 0.6875, 0.75, 0.5625, 0.5, 0.6875, 0.625, 0.4375, 0.5625, 0.6875, 0.6875, 0.625, 0.6875, 0.625, 0.8125, 0.4375, 0.8125, 0.5, 0.625, 0.5, 0.4375, 0.375, 0.8125, 0.5625, 0.75, 0.75, 0.6875, 0.8125, 0.4375, 0.5, 0.625, 0.5625, 0.8125, 0.4375, 0.6875, 0.5625, 0.5, 0.75, 0.5625, 0.625, 0.8125, 0.4375, 0.4375, 0.5, 0.4375, 0.375, 0.3125, 0.5, 0.5625, 0.5, 0.75, 0.6875, 0.75, 0.5625, 0.75, 0.75, 0.75, 0.75, 0.6875, 0.75, 0.5625, 0.5, 0.75, 0.375, 0.5625, 0.4375, 0.625, 0.5, 0.5, 0.5, 0.1875, 0.6875, 0.625, 0.625, 0.625, 0.625, 0.5, 0.8125, 0.8125, 0.8125, 0.6875, 0.5625, 0.75, 0.75, 0.75, 0.6875, 0.625, 0.625, 0.5, 0.75, 0.5, 0.6875, 0.625, 0.6875, 0.6875, 0.5625, 0.75, 0.6875, 0.375, 0.5625, 0.75, 0.6875, 0.6875, 0.5625, 0.625, 0.5625, 0.6875, 0.75, 0.75, 0.8125, 0.8125, 0.8125, 0.5, 0.75, 0.6875, 0.5, 0.4375, 0.4375, 0.6875, 0.75, 0.625, 0.6875, 0.625, 0.6875, 0.8125, 0.375, 0.75, 0.625, 0.5, 0.75, 0.75, 0.6875, 0.375, 0.5, 0.5625, 0.8125, 0.75, 0.125, 0.5625, 0.5, 0.625, 0.625, 0.6875, 0.75, 0.625, 0.75, 0.5, 0.375, 0.4375, 0.6875, 0.875, 0.5, 0.625, 0.3125, 0.5, 0.625, 0.8125, 0.6875, 0.875, 0.8125, 0.625, 0.5625, 0.8125, 0.8125, 0.8125, 0.5625, 0.4375, 0.8125, 0.3125, 0.75, 0.4375, 0.375, 0.5, 0.5, 0.5625, 0.75, 0.625, 0.375, 0.6875, 0.6875, 0.5, 0.8125, 0.6875, 0.5, 0.8125, 0.5, 0.5, 0.5, 0.8125, 0.5625, 0.6875, 0.5625, 0.5, 0.4375, 0.625, 0.6875, 0.375, 0.75, 0.5625, 0.75, 0.625, 0.4375, 0.4375, 0.4375, 0.375, 0.625, 0.6875, 0.625, 0.4375, 0.8125, 0.625, 0.5, 0.5625, 0.8125, 0.4375, 0.5625, 0.625, 0.375, 0.6875, 0.4375, 0.75, 0.625, 0.4375, 0.75, 0.5625, 0.75, 0.5625, 0.5, 0.6875, 0.6875, 0.6875, 0.8125, 0.75, 0.875, 0.5625, 0.875, 0.6875, 0.75, 0.5625, 0.875, 0.4375, 0.5, 0.625, 0.6875, 0.375, 0.625, 0.75, 0.5625, 0.375, 0.375, 0.8125, 0.625, 0.4375, 0.375, 0.75, 0.5625, 0.5625, 0.5625, 0.9375, 0.375, 0.5625, 0.75, 0.8125, 0.5625, 0.6875, 0.75, 0.5, 0.4375, 0.75, 0.625, 0.625, 0.5, 0.4375, 0.5, 0.5625, 0.5, 0.6875, 0.0, 0.8125, 0.6875, 0.75, 0.0, 0.75, 0.5625, 0.5, 0.375, 0.5, 0.4375, 0.5, 0.5, 0.1875, 0.75, 0.4375, 0.5, 0.625, 0.625, 0.6875, 0.6875, 0.625, 0.75, 0.8125, 0.75, 0.75, 0.8125, 0.6875, 0.375, 0.4375, 0.5625, 0.5, 0.625, 0.5, 0.4375, 0.625, 0.5625, 0.375, 0.625, 0.5, 0.5625, 0.625, 0.5, 0.75, 0.5, 0.625, 0.6875, 0.5, 0.625, 0.875, 0.875, 0.6875, 0.875, 0.75, 0.5, 0.75, 0.5625, 0.625, 0.75, 0.625, 0.5625, 0.75, 0.4375, 0.6875, 0.4375, 0.625, 0.5625, 0.5, 0.6875, 0.8125, 0.5625, 0.5625, 0.625, 0.4375, 0.6875, 0.5, 0.4375, 0.625, 0.625, 0.75, 0.6875, 0.625, 0.75, 0.8125, 0.5, 0.6875, 0.4375, 0.6875, 0.8125, 0.8125, 0.625, 0.6875, 0.625, 0.375, 0.625, 0.6875, 0.3125, 0.25, 0.625, 0.4375, 0.5, 0.75, 0.625, 0.75, 0.6875, 0.625, 0.5, 0.75, 0.6875, 0.6875, 1.0, 0.9375, 0.4375, 0.6875, 0.6875, 0.8125, 0.4375, 0.375, 0.4375, 0.6875, 0.5, 0.6875, 0.4375, 0.4375, 0.6875, 0.4375, 0.8125, 0.5625, 0.875, 0.625, 0.0, 0.6875, 0.8125, 0.5, 0.75, 0.5, 0.5, 0.5, 0.8125, 0.5, 1.0, 0.8125, 0.875, 0.5625, 0.5625, 0.5, 0.6875, 0.4375, 0.9375, 0.75, 0.5, 0.4375, 0.3125, 0.8125, 0.8125, 0.75, 0.6875, 0.3125, 0.5625, 0.375, 0.75, 0.4375, 0.375, 0.75, 0.8125, 0.6875, 0.5625, 0.6875, 0.625, 0.625, 0.6875, 0.75, 0.5625, 0.4375, 0.8125, 0.8125, 0.8125, 0.8125, 0.4375, 0.5, 0.3125, 0.8125, 0.5, 0.5, 0.6875, 0.6875, 0.75, 0.5, 0.5625, 0.625, 0.4375, 0.5, 0.5, 0.5625, 0.4375, 0.6875, 1.0, 0.625, 0.75, 0.5, 0.5, 0.4375, 0.8125, 0.75, 0.75, 0.6875, 0.5, 0.375, 0.4375, 0.4375, 0.75, 0.875, 0.5625, 0.6875, 0.9375, 0.4375, 0.5, 0.625, 0.5, 0.625, 0.75, 0.625, 0.8125, 0.4375, 0.625, 0.6875, 0.75, 0.8125, 0.375, 0.75, 0.5, 0.5, 0.625, 0.8125, 0.75, 0.4375, 0.6875, 0.75, 0.75, 0.625, 0.5, 0.125, 0.4375, 0.6875, 0.75, 0.75, 0.75, 0.5, 0.625, 0.4375, 0.5625, 0.625, 0.4375, 0.625, 0.5, 0.6875, 0.6875, 0.4375, 0.6875, 0.6875, 0.4375, 0.5625, 0.5, 0.8125, 0.6875, 0.6875, 0.625, 0.6875, 0.6875, 0.625, 0.625, 0.5625, 0.5, 0.6875, 0.625, 0.6875, 0.75, 0.4375, 0.5625, 0.8125, 0.6875, 0.4375, 0.625, 0.4375, 0.5625, 0.75, 0.6875, 0.4375, 0.6875, 0.4375, 0.5, 0.6875, 0.625, 0.625, 0.5625, 0.625, 0.8125, 0.75, 0.4375, 0.4375, 0.375, 0.875, 0.4375, 0.75, 0.5, 0.625, 0.5625, 0.5625, 0.75, 0.625, 0.5625, 0.75, 0.5, 0.4375, 0.75, 0.6875, 0.625, 0.4375, 0.5625, 0.4375, 0.375, 0.5625, 0.625, 0.75, 0.1875, 0.625, 0.625, 0.75, 0.3125, 0.6875, 0.6875, 0.625, 0.4375, 0.6875, 0.6875, 0.6875, 0.6875, 0.5, 0.875, 0.3125, 0.625, 0.8125, 0.6875, 0.875, 0.625, 0.3125, 0.6875, 0.6875, 0.625, 0.5, 0.8125, 0.8125, 0.5, 0.625, 0.75, 0.375, 0.8125, 0.625, 0.5, 0.6875, 0.6875, 0.5, 0.6875, 0.4375, 0.8125, 0.375, 0.625, 0.6875, 0.6875, 0.625, 0.6875, 0.625, 0.375, 0.8125, 0.6875, 0.5, 0.75, 0.5, 0.4375, 0.8125, 0.8125, 0.6875, 0.625, 0.5, 0.4375, 0.8125, 0.75, 0.5625, 0.6875, 0.6875, 0.5, 0.5, 0.625, 0.625, 0.4375, 0.5625, 0.75, 0.4375, 0.4375, 0.8125, 0.8125, 0.75, 0.75, 0.4375, 0.6875, 0.4375, 0.3125, 0.5625, 0.5625, 0.5625, 0.625, 0.5, 0.625, 0.5, 0.75, 0.375, 0.625, 0.625, 0.6875, 0.8125, 0.9375, 0.875, 0.5625, 0.75, 0.8125, 0.625, 0.625, 0.8125, 0.75, 0.625, 0.4375, 0.4375, 0.625, 0.6875, 0.4375, 0.625, 0.75, 0.9375, 0.4375, 0.75, 0.6875, 0.5, 0.6875, 0.8125, 0.5625, 0.5625, 0.75, 0.5625, 0.75, 0.8125, 0.5, 0.5, 0.625, 0.625, 0.4375, 0.5625, 0.5, 0.6875, 0.1875, 0.6875, 0.8125, 0.8125, 0.625, 0.4375, 0.6875, 0.6875, 0.4375, 0.5625, 0.125, 0.5, 0.4375, 0.4375, 0.75, 0.75, 0.5625, 0.5625, 0.5625, 0.1875, 0.625, 0.625, 0.8125, 0.5625, 0.5, 0.75, 0.4375, 0.875, 0.8125, 0.6875, 0.5, 0.75, 0.5, 0.4375, 0.3125, 0.4375, 0.6875, 0.75, 0.5625, 0.5, 0.4375, 0.625, 0.75, 0.5, 0.75, 0.625, 0.9375, 0.75, 0.4375, 0.6875, 0.875, 0.6875, 0.4375, 0.75, 0.75, 0.625, 0.625, 0.375, 0.4375, 0.5625, 0.75, 0.5, 0.5, 0.8125, 0.8125, 0.4375, 0.625, 0.75, 0.5625, 0.5, 0.5625, 0.75, 0.8125, 0.5625, 0.8125, 0.625, 0.75, 0.5, 0.5625, 0.625, 0.75, 0.625, 0.5, 0.4375, 0.5, 0.6875, 0.6875, 0.4375, 0.625, 0.6875, 0.625, 0.5, 0.5625, 0.75, 0.9375, 0.5, 0.6875, 0.6875, 0.125, 0.6875, 0.625, 0.875, 0.5, 0.5625, 0.6875, 0.6875, 0.875, 0.75, 0.75, 0.75, 0.75, 0.75, 0.6875, 0.5625, 0.5625, 0.6875, 0.625, 0.4375, 0.625, 0.75, 0.4375, 0.5625, 0.8125, 0.875, 0.625, 0.4375, 0.625, 0.75, 0.5, 0.75, 0.5, 0.625, 0.6875, 0.75, 0.4375, 0.5, 0.8125, 0.6875, 0.4375, 0.6875, 0.625, 0.8125, 0.125, 0.5, 0.875, 0.8125, 0.8125, 0.5, 0.5, 0.6875, 0.6875, 0.4375, 0.4375, 0.6875, 0.5625, 0.625, 0.8125, 0.5625, 0.5, 0.4375, 0.8125, 0.625, 0.6875, 0.8125, 0.75, 0.4375, 0.75, 0.75, 0.75, 0.6875, 0.6875, 0.5625, 0.8125, 0.5, 0.625, 0.5, 0.5, 0.625, 0.5625, 0.75, 0.5, 0.125, 0.75, 0.8125, 0.625, 0.5, 0.5, 0.0, 0.75, 0.625, 0.6875, 0.625, 0.75, 0.75, 0.75, 0.75, 0.6875, 0.625, 0.5, 0.6875, 0.8125, 0.8125, 0.8125, 0.75, 0.5625, 0.3125, 0.5, 0.8125, 0.4375, 0.625, 0.625, 0.5625, 1.0, 0.625, 0.5, 0.4375, 0.8125, 0.4375, 0.8125, 0.875, 0.8125, 0.5625, 0.1875, 0.5625, 0.75, 0.625, 0.8125, 0.625, 0.4375, 0.6875, 0.625, 0.5, 0.75, 0.5, 0.8125, 0.625, 0.6875, 0.75, 0.375, 0.75, 0.875, 0.625, 0.8125, 0.75, 0.5625, 0.4375, 0.75, 0.6875, 0.5625, 0.625, 0.75, 0.5, 0.8125, 0.5, 0.75, 0.4375, 0.5, 0.8125, 0.5625, 0.5625, 0.6875, 0.5, 0.5, 0.8125, 0.6875, 0.625, 0.8125, 0.5625, 0.5, 0.8125, 0.6875, 0.5, 0.5625, 0.5, 0.75, 0.625, 0.875, 0.5625, 0.625, 0.5, 0.375, 0.6875, 0.6875, 0.4375, 0.625, 0.4375, 0.8125, 0.75, 0.75, 0.625, 0.5625, 0.5625, 0.6875, 0.75, 0.8125, 0.4375, 0.5625, 0.4375, 0.4375, 0.375, 0.75, 0.75, 0.5, 0.6875, 0.75, 0.75, 0.6875, 0.4375, 0.5625, 0.75, 0.5625, 0.6875, 0.8125, 0.5625, 0.625, 0.9375, 0.5, 0.75, 0.75, 0.625, 0.625, 0.6875, 0.625, 0.4375, 0.625, 0.375, 0.6875, 0.625, 0.625, 0.75, 0.3125, 0.5625, 0.375, 0.5, 0.8125, 0.8125, 0.6875, 0.75, 0.6875, 0.8125, 0.4375, 0.5625, 0.5, 0.625, 0.75, 0.4375, 0.5625, 0.75, 0.6875, 0.8125, 0.75, 0.75, 0.75, 0.5, 0.625, 0.625, 0.5625, 0.5, 0.6875, 0.5, 0.75, 0.5, 0.625, 0.9375, 0.5625, 0.8125, 0.5, 0.5, 0.5625, 0.5, 0.625, 0.5, 0.5625, 0.75, 0.8125, 0.5625, 0.625, 0.5625, 0.625, 0.3125, 0.75, 0.125, 0.5625, 0.8125, 0.4375, 0.75, 0.4375, 0.1875, 0.625, 0.75, 0.5625, 0.75, 0.6875, 0.75, 0.3125, 0.875, 0.6875, 0.875, 0.6875, 0.4375, 0.75, 0.5, 0.75, 0.5625, 0.4375, 0.375, 0.75, 0.4375, 0.875, 0.6875, 0.9375, 0.625, 0.4375, 0.75, 0.6875, 0.8125, 0.4375, 0.5625, 0.6875, 0.625, 0.6875, 0.6875, 0.5625, 0.4375, 0.25, 0.625, 0.5625, 0.75, 0.5625, 0.6875, 0.75, 0.5, 0.75, 0.6875, 0.5625, 0.5, 0.75, 0.5, 0.6875, 0.5625, 0.5625, 0.875, 0.75, 0.4375, 0.5625, 0.6875, 0.6875, 0.625, 0.75, 0.4375, 0.625, 0.75, 0.4375, 0.5, 0.6875, 0.5, 0.625, 0.5625, 0.6875, 0.625, 0.5, 0.75, 0.625, 0.5625, 0.75, 0.625, 0.5625, 0.5, 0.625, 0.8125, 0.5, 0.625, 0.6875, 0.625, 0.625, 0.75, 0.75, 0.6875, 0.6875, 0.375, 0.75, 0.6875, 0.75, 0.3125, 0.1875, 0.75, 0.125, 0.4375, 0.5625, 0.4375, 0.625, 0.875, 0.6875, 0.6875, 0.5625, 0.625, 0.6875, 0.8125, 0.4375, 0.625, 0.75, 0.375, 0.5625, 0.625, 0.625, 0.8125, 0.5, 0.875, 0.5, 0.6875, 0.75, 0.75, 0.5, 0.625, 0.4375, 0.75, 0.5625, 0.625, 0.4375, 0.5625, 0.125, 0.6875, 0.5625, 0.5, 0.4375, 0.8125, 0.5625, 0.6875, 0.6875, 0.5, 0.6875, 0.4375, 0.5, 0.5625, 0.5625, 0.5, 0.6875, 0.75, 0.5625, 0.5, 0.5625, 0.375, 0.5625, 0.75, 0.75, 0.5, 0.6875, 0.4375, 0.8125, 0.625, 0.4375, 0.4375, 0.625, 0.4375, 0.8125, 0.5, 0.625, 0.5625, 0.5, 0.6875, 0.75, 0.8125, 0.6875, 0.4375, 0.8125, 0.75, 0.625, 0.6875, 0.625, 0.4375, 0.5625, 0.4375, 0.625, 0.4375, 0.375, 0.5, 0.625, 0.5625, 0.6875, 0.6875, 0.5, 0.5, 0.6875, 0.4375, 0.625, 0.5, 0.6875, 0.0625, 0.625, 0.625, 0.125, 0.5, 0.5, 0.125, 0.4375, 0.5625, 0.5, 0.4375, 0.75, 0.4375, 0.9375, 0.6875, 0.5, 0.75, 0.6875, 0.5, 0.3125, 0.5, 0.5, 0.625, 0.5, 0.6875, 0.625, 0.6875, 0.6875, 0.8125, 0.8125, 0.375, 0.4375, 0.4375, 0.5, 0.5625, 0.75, 0.75, 0.4375, 0.5625, 0.4375, 0.6875, 0.5625, 0.375, 0.4375, 0.5, 0.4375, 0.4375, 0.5, 0.625, 0.375, 0.5625, 0.5, 0.0625, 0.4375, 0.8125, 0.625, 0.8125, 0.375, 0.6875, 0.5, 0.5625, 0.875, 0.5625, 0.75, 0.0625, 0.5625, 0.8125, 0.6875, 0.8125, 0.6875, 0.5, 0.625, 0.625, 0.4375, 0.8125, 0.9375, 0.625, 0.5625, 0.75, 0.5, 0.5, 0.5, 0.8125, 0.625, 0.625, 0.75, 0.625, 0.8125, 0.5625, 0.75, 0.75, 0.6875, 0.5, 0.75, 0.75, 0.5, 0.5, 0.5625, 0.5, 0.5625, 0.625, 0.625, 0.8125, 0.5, 0.8125, 0.4375, 0.8125, 0.5, 0.5625, 0.75, 0.8125, 0.75, 0.9375, 0.8125, 0.75, 0.4375, 0.75, 0.75, 0.75, 0.75, 0.75, 0.625, 0.625, 0.4375, 0.4375, 0.4375, 0.5625, 0.4375, 0.75, 0.4375, 0.75, 0.875, 0.375, 0.5, 0.625, 0.625, 0.4375, 0.4375, 0.625, 0.8125, 0.75, 0.3125, 0.875, 0.6875, 0.8125, 0.6875, 0.25, 0.875, 0.625, 0.75, 0.5625, 0.625, 0.6875, 0.8125, 0.375, 0.8125, 0.4375, 0.75, 0.625, 0.8125, 0.75, 0.5625, 0.5625, 0.8125, 0.8125, 0.375, 0.8125, 0.5, 0.5625, 0.75, 0.5, 0.6875, 0.5, 0.8125, 0.5, 0.8125, 0.8125, 0.6875, 0.5, 0.625, 0.625, 0.625, 0.75, 0.625, 0.6875, 0.5625, 0.75, 0.4375, 0.5625, 0.625, 0.5625, 0.5625, 0.8125, 0.75, 0.5625, 0.8125, 0.4375, 0.5, 0.625, 0.875, 0.1875, 0.875, 0.75, 0.4375, 0.375, 0.6875, 0.5, 0.4375, 0.625, 0.5625, 0.75, 0.8125, 0.8125, 0.5625, 0.625, 0.5, 0.75, 0.5625, 0.4375, 0.5, 0.6875, 0.75, 0.75, 0.75, 0.5625, 0.8125, 0.8125, 0.4375, 0.6875, 0.625, 0.5, 0.75, 0.6875, 0.5, 0.75, 0.5, 0.75, 0.5, 0.3125, 1.0, 0.6875, 0.875, 0.625, 0.5, 0.5, 0.5, 0.625, 0.625, 0.8125, 0.6875, 0.5625, 0.6875, 0.625, 0.8125, 0.5625, 0.6875, 0.3125, 0.75, 0.4375, 0.5, 0.5625, 0.5625, 0.8125, 0.5, 0.75, 0.625, 0.3125, 0.5, 0.25, 0.5, 0.8125, 0.75, 0.875, 0.4375, 0.625, 0.625, 0.5, 0.625, 0.625, 0.5625, 0.6875, 0.8125, 0.625, 0.6875, 0.6875, 0.75, 0.6875, 0.5625, 0.5, 0.4375, 0.5625, 0.4375, 0.6875, 0.4375, 0.625, 0.4375, 0.75, 0.5, 0.625, 0.4375, 0.75, 0.5, 0.125, 0.4375, 0.75, 0.875, 0.75, 0.5625, 0.6875, 0.75, 0.5, 0.625, 0.5625, 0.3125, 0.5625, 0.8125, 0.6875, 0.6875, 0.8125, 0.75, 0.5, 0.3125, 0.9375, 0.5, 0.75, 0.5625, 0.625, 0.8125, 0.125, 0.125, 0.5625, 0.6875, 0.5625, 0.4375, 0.375, 0.5, 0.4375, 0.75, 0.375, 0.5, 0.375, 0.5, 0.625, 0.5625, 0.8125, 0.6875, 0.625, 0.4375, 0.5625, 0.5625, 0.8125, 0.625, 0.625, 0.5, 0.5, 0.4375, 0.375, 0.625, 0.6875, 0.8125, 0.6875, 0.75, 0.5, 0.75, 0.6875, 0.6875, 0.5625, 0.625, 0.1875, 0.125, 0.4375, 0.625, 0.5, 0.75, 0.875, 0.6875, 0.75, 0.6875, 0.625, 0.5625, 0.625, 0.6875, 0.9375, 0.375, 0.4375, 0.5, 0.8125, 0.5625, 0.75, 0.625, 0.6875, 0.625, 0.8125, 0.75, 0.625, 0.75, 0.625, 0.5625, 0.5, 0.75, 0.625, 0.75, 0.5, 0.4375, 0.625, 0.5, 0.5625, 0.5625, 0.75, 0.875, 0.6875, 0.6875, 0.8125, 0.8125, 0.5625, 0.8125, 0.75, 0.4375, 0.5625, 0.4375, 0.9375, 0.5, 0.6875, 0.125, 0.6875, 0.625, 0.8125, 0.625, 0.4375, 0.6875, 0.4375, 0.875, 0.5, 0.8125, 0.6875, 0.6875, 0.6875, 0.5625, 0.6875, 0.625, 0.625, 0.6875, 0.5, 0.4375, 0.5, 0.5, 0.1875, 0.8125, 0.75, 0.75, 0.4375, 0.5, 0.5625, 0.625, 0.6875, 0.5, 0.6875, 0.8125, 0.75, 0.6875, 0.5625, 0.5625, 0.4375, 0.5, 0.6875, 0.4375, 0.6875, 0.6875, 0.75, 0.5, 0.5625, 0.75, 0.5, 0.6875, 0.625, 0.6875, 0.375, 0.0625, 0.75, 0.75, 0.6875, 0.4375, 0.625, 0.4375, 0.25, 0.5625, 0.6875, 0.5, 0.5625, 0.75, 0.6875, 0.625, 0.625, 0.8125, 0.75, 0.5, 0.625, 0.6875, 0.75, 0.4375, 0.5, 0.4375, 0.6875, 0.625, 0.75, 0.75, 0.5, 0.6875, 0.75, 0.6875, 0.4375, 0.5, 0.125, 0.8125, 0.875, 0.75, 0.5, 0.75, 0.25, 0.625, 0.75, 0.5625, 0.625, 0.5, 0.6875, 0.625, 0.75, 0.75, 1.0, 0.625, 0.625, 0.6875, 0.75, 0.5625, 0.375, 0.4375, 0.6875, 0.75, 0.6875, 0.5625, 0.75, 0.6875, 0.6875, 0.875, 0.6875, 0.75, 0.75, 0.75, 0.375, 0.8125, 0.6875, 0.5625, 0.8125, 0.8125, 0.4375, 0.6875, 0.6875, 0.625, 0.4375, 0.6875, 0.5, 0.5, 0.5, 0.75, 0.375, 0.8125, 0.875, 0.4375, 0.6875, 0.75, 0.8125, 0.625, 0.4375, 0.625, 0.4375, 0.6875, 0.4375, 0.5625, 0.5625, 0.5625, 0.6875, 0.75, 0.6875, 0.5625, 0.5, 0.375, 0.5, 0.5, 0.4375, 0.6875, 0.8125, 0.625, 0.5, 0.4375, 0.75, 0.625, 0.5625, 0.75, 0.1875, 0.3125, 0.6875, 0.5625, 0.5, 0.5625, 0.6875, 0.5625, 0.5, 0.0625, 0.375, 0.5625, 0.4375, 0.6875, 0.6875, 0.4375, 0.4375, 0.8125, 0.5, 0.8125, 0.6875, 0.5, 0.8125, 0.6875, 0.5, 0.6875, 0.6875, 0.6875, 0.625, 0.4375, 0.75, 0.75, 0.75, 0.5, 0.6875, 0.6875, 0.625, 0.6875, 0.5, 0.75, 0.5, 0.5625, 0.625, 0.125, 0.625, 0.6875, 0.4375, 0.4375, 0.4375, 0.75, 0.3125, 0.6875, 0.6875, 0.8125, 0.75, 0.375, 0.5, 0.0, 0.6875, 0.625, 0.5, 0.625, 0.4375, 0.6875, 0.75, 0.5, 0.4375, 0.4375, 0.375, 0.6875, 0.5, 0.6875, 0.6875, 0.75, 0.5625, 0.8125, 0.1875, 0.75, 0.5, 0.625, 0.75, 0.8125, 0.625, 0.6875, 0.625, 0.75, 0.3125, 0.625, 0.4375, 0.625, 0.5625, 0.4375, 0.4375, 0.625, 0.75, 0.75, 0.6875, 0.5, 0.6875, 0.625, 0.5, 0.75, 0.5, 0.75, 0.375, 0.8125, 0.75, 0.5, 0.4375, 0.6875, 0.4375, 0.75, 0.4375, 0.625, 0.625, 0.125, 0.75, 0.4375, 0.5625, 0.5625, 0.5, 0.8125, 0.4375, 0.8125, 0.4375, 0.625, 0.5, 0.4375, 0.75, 0.5, 0.8125, 0.75, 0.75, 0.75, 0.5625, 0.5625, 0.5, 0.5, 0.6875, 0.875, 0.9375, 0.4375, 0.625, 0.6875, 0.625, 0.5625, 0.75, 0.5625, 0.75, 0.5625, 0.125, 0.3125, 0.125, 0.625, 0.5, 0.75, 0.6875, 0.75, 0.6875, 0.8125, 0.5625, 0.6875, 0.5, 0.1875, 0.625, 0.375, 0.375, 0.5, 0.75, 0.9375, 0.5, 0.5625, 0.8125, 0.6875, 0.3125, 0.5, 0.625, 0.625, 0.0, 0.5, 0.5625, 0.75, 0.4375, 0.0, 0.75, 0.4375, 0.6875, 0.75, 0.8125, 0.4375, 0.75, 0.625, 0.4375, 0.8125, 0.4375, 0.625, 0.75, 0.5, 0.75, 0.8125, 0.6875, 0.5, 0.6875, 0.4375, 0.625, 0.6875, 0.3125, 0.625, 0.5625, 0.125, 0.75, 0.75, 0.5, 0.6875, 0.5, 0.75, 0.6875, 0.4375, 0.75, 0.75, 0.8125, 0.875, 0.75, 0.75, 0.625, 0.5625, 0.625, 0.5, 0.625, 0.75, 0.625, 0.75, 0.8125, 0.75, 0.25, 0.375, 0.8125, 0.625, 0.625, 0.5625, 0.875, 0.75, 0.6875, 0.5625, 0.625, 0.625, 0.6875, 0.75, 0.75, 0.4375, 0.625, 0.5625, 0.4375, 0.375, 0.8125, 0.6875, 0.5, 0.75, 0.5, 0.4375, 0.6875, 0.625, 0.75, 0.8125, 0.4375, 0.75, 0.5, 0.625, 0.4375, 0.8125, 0.6875, 0.5625, 0.625, 0.1875, 0.75, 0.6875, 0.75, 0.5625, 0.5, 0.75, 0.625, 0.6875, 0.5, 0.375, 0.5625, 0.8125, 0.5625, 0.6875, 0.625, 0.4375, 0.6875, 0.625, 0.625, 0.5, 0.625, 0.4375, 0.75, 0.4375, 0.5625, 0.625, 0.8125, 0.4375, 0.75, 0.8125, 0.75, 0.625, 0.4375, 0.5625, 0.5625, 0.8125, 0.625, 0.625, 0.75, 0.875, 0.5, 0.5625, 0.5, 0.625, 0.6875, 0.375, 0.375, 0.6875, 0.8125, 0.6875, 0.4375, 0.8125, 0.625, 0.6875, 0.625, 0.8125, 0.375, 0.75, 0.625, 0.4375, 0.4375, 0.625, 0.0625, 0.25, 0.25, 0.8125, 0.4375, 0.5625, 0.4375, 0.4375, 0.125, 0.625, 0.5, 0.6875, 0.5625, 0.8125, 0.75, 0.5, 0.5, 0.5625, 0.5, 0.5625, 0.375, 0.5625, 0.625, 0.4375, 0.375, 0.5, 0.8125, 0.4375, 0.4375, 0.4375, 0.6875, 0.5, 0.75, 0.75, 0.8125, 0.625, 0.4375, 0.5625, 0.6875, 0.6875, 0.75, 0.625, 0.6875, 0.75, 0.5625, 0.4375, 0.625, 0.75, 0.875, 0.0625, 0.4375, 0.75, 0.5625, 0.5, 0.9375, 0.5625, 0.6875, 0.4375, 0.5, 0.125, 0.75, 0.8125, 0.5, 0.5625, 0.5, 0.4375, 0.5, 0.5, 0.375, 0.5625, 0.3125, 0.625, 0.4375, 0.75, 0.125, 0.5625, 0.5, 0.375, 0.625, 0.4375, 0.375, 0.6875, 0.5625, 0.375, 0.5, 0.5, 0.5, 0.875, 0.75, 0.625, 0.5, 0.8125, 0.5625, 0.8125, 0.9375, 0.375, 0.6875, 0.875, 0.5, 0.8125, 0.9375, 0.8125, 0.75, 0.6875, 0.8125, 0.4375, 0.75, 0.75, 0.5, 0.4375, 0.1875, 0.5625, 0.6875, 0.6875, 0.125, 0.8125, 0.5, 0.6875, 0.75, 0.75, 0.6875, 0.6875, 0.6875, 0.8125, 0.75, 0.4375, 0.625, 0.4375, 0.75, 0.3125, 0.375, 0.5625, 0.875, 0.5625, 0.5, 0.5, 0.75, 0.375, 0.625, 0.4375, 0.6875, 0.625, 0.6875, 0.625, 0.625, 0.875, 0.75, 0.4375, 0.8125, 0.5, 0.5625, 0.5, 0.625, 0.75, 0.4375, 0.6875, 0.5, 0.6875, 0.625, 0.5, 0.625, 0.5625, 0.75, 0.5, 0.6875, 0.8125, 0.4375, 0.5625, 0.5625, 0.8125, 0.5625, 0.6875, 0.75, 0.5, 0.6875, 0.3125, 0.5625, 0.125, 0.5, 0.5625, 0.4375, 0.625, 0.625, 0.625, 0.625, 0.5, 0.625, 0.5, 0.75, 0.375, 0.5, 0.6875, 0.5, 0.625, 0.75, 0.4375, 0.5, 0.8125, 0.75, 0.625, 0.75, 0.5625, 0.625, 0.5625, 0.4375, 0.75, 0.4375, 0.75, 0.75, 0.5, 0.5, 0.5, 0.875, 0.5, 0.75, 0.75, 0.75, 0.625, 0.5625, 0.75, 1.0, 0.625, 0.625, 0.625, 0.3125, 0.6875, 0.8125, 0.6875, 0.8125, 0.875, 0.5, 0.6875, 1.0, 0.8125, 0.5, 0.75, 0.4375, 0.8125, 0.25, 0.375, 0.75, 0.8125, 0.8125, 0.875, 0.75, 0.75, 0.75, 0.6875, 0.5625, 0.6875, 0.625, 0.625, 0.6875, 0.875, 0.4375, 0.5, 0.0, 0.625, 0.75, 0.5625, 0.0625, 0.75, 0.6875, 0.4375, 0.8125, 0.75, 0.5625, 0.625, 0.4375, 0.4375, 0.6875, 0.5, 0.6875, 0.875, 0.4375, 0.5625, 0.75, 0.9375, 0.6875, 0.4375, 0.75, 0.6875, 0.8125, 0.4375, 0.125, 0.8125, 0.5625, 0.5, 0.625, 0.6875, 0.8125, 0.4375, 0.5, 0.5, 0.75, 0.5, 0.75, 0.6875, 0.5625, 0.4375, 0.5, 0.8125, 0.5, 0.625, 0.5, 0.5, 0.625, 0.3125, 0.625, 0.5, 0.8125, 0.8125, 0.875, 0.8125, 0.625, 0.6875, 0.5625, 0.5, 0.9375, 0.75, 0.75, 0.6875, 0.6875, 0.6875, 0.625, 0.5, 0.875, 0.625, 0.625, 0.5625, 0.5625, 0.9375, 0.4375, 0.6875, 0.5625, 0.5, 0.4375, 0.5, 0.75, 0.4375, 0.625, 0.75, 0.6875, 0.75, 0.8125, 0.6875, 0.4375, 0.6875, 0.5625, 0.75, 0.5625, 0.75, 0.75, 0.3125, 0.75, 0.625, 0.5, 0.5625, 0.375, 0.5625, 0.5, 0.75, 0.5, 0.75, 0.6875, 0.625, 0.75, 0.4375, 0.5, 0.625, 0.6875, 0.8125, 0.5625, 0.5, 0.5625, 0.8125, 0.625, 0.4375, 0.4375, 0.8125, 0.75, 0.75, 0.5625, 0.8125, 0.625, 0.4375, 0.5625, 0.6875, 0.75, 0.5, 0.75, 0.5, 0.6875, 0.4375, 0.5625, 0.75, 0.5, 0.6875, 0.6875, 0.5, 0.625, 0.5, 0.375, 0.625, 0.8125, 0.5, 0.4375, 0.875, 0.5, 0.25, 0.6875, 0.9375, 0.75, 0.75, 0.6875, 0.3125, 0.375, 0.5, 0.4375, 0.5625, 0.6875, 0.75, 0.5625, 0.75, 0.6875, 0.9375, 0.6875, 0.6875, 0.5625, 0.5, 0.5625, 0.6875, 0.75, 0.8125, 0.5625, 0.5, 0.5625, 0.8125, 0.5625, 0.75, 0.75, 0.4375, 0.5625, 0.5, 0.4375, 0.5, 0.75, 0.375, 0.625, 0.4375, 0.6875, 0.75, 0.75, 0.4375, 0.5625, 0.5, 0.625, 0.75, 0.8125, 0.8125, 0.6875, 0.125, 0.625, 0.5625, 0.6875, 0.8125, 0.6875, 0.5625, 0.6875, 0.75, 0.75, 0.6875, 0.6875, 0.75, 0.25, 0.75, 0.1875, 0.5, 0.8125, 0.75, 0.75, 0.8125, 0.5, 0.5, 0.0, 0.875, 0.5, 0.5625, 0.75, 0.5, 0.8125, 0.4375, 0.625, 0.75, 0.1875, 0.625, 0.75, 0.0, 0.8125, 0.75, 0.6875, 0.5, 0.6875, 0.5, 0.625, 0.625, 0.5, 0.5, 0.5, 0.75, 0.6875, 0.625, 0.625, 0.5625, 0.8125, 0.5625, 0.5625, 0.5, 0.8125, 0.625, 0.4375, 0.8125, 0.5, 0.9375, 0.5625, 0.5, 0.625, 0.8125, 0.5625, 0.5625, 0.75, 0.6875, 0.5, 0.625, 0.8125, 0.375, 0.75, 0.4375, 0.625, 0.5, 0.625, 0.5625, 1.0, 0.75, 0.75, 0.125, 0.625, 0.4375, 0.6875, 0.75, 0.75, 0.6875, 0.5625, 0.875, 0.6875, 0.4375, 0.625, 0.6875, 0.4375, 0.5625, 0.4375, 0.875, 0.4375, 0.8125, 0.5, 0.5, 0.6875, 0.6875, 0.75, 0.5625, 0.375, 0.6875, 0.6875, 0.8125, 0.8125, 0.75, 0.4375, 0.6875, 0.625, 0.4375, 0.625, 0.6875, 0.625, 0.5, 0.375, 0.75, 0.625, 0.75, 0.5625, 0.4375, 0.4375, 0.75, 0.4375, 0.75, 0.625, 0.4375, 0.75, 0.75, 0.3125, 0.625, 0.6875, 0.5, 0.5, 0.625, 0.8125, 0.4375, 0.625, 0.4375, 0.5, 0.5, 0.4375, 0.625, 0.6875, 0.4375, 0.375, 0.75, 0.0, 0.5625, 0.75, 0.8125, 0.75, 0.8125, 0.6875, 0.6875, 0.75, 0.75, 0.8125, 0.4375, 0.4375, 0.625, 0.5625, 0.5, 0.4375, 0.75, 0.625, 0.8125, 0.6875, 0.625, 0.5625, 0.75, 0.4375, 0.6875, 0.6875, 0.5625, 0.5625, 0.4375, 0.4375, 0.6875, 0.75, 0.6875, 0.6875, 0.5625, 0.375, 0.625, 0.5625, 0.875, 0.6875, 0.75, 0.5625, 0.625, 0.75, 0.5, 0.5, 0.625, 0.75, 0.6875, 0.75, 0.75, 0.75, 0.6875, 0.5, 0.5625, 0.6875, 0.4375, 0.8125, 0.4375, 0.6875, 0.6875, 0.8125, 0.6875, 0.6875, 0.5, 0.8125, 0.625, 0.6875, 0.6875, 0.5, 0.8125, 0.75, 0.75, 0.6875, 0.5625, 0.1875, 0.5, 0.5, 0.6875, 0.5625, 0.75, 0.5, 0.5, 0.8125, 0.75, 0.75, 0.375, 0.5, 0.75, 0.4375, 0.4375, 0.375, 0.6875, 0.5625, 0.375, 0.6875, 0.75, 0.75, 0.875, 0.6875, 0.4375, 0.4375, 0.3125, 0.6875, 0.6875, 0.75, 0.5625, 0.5625, 0.6875, 0.5, 0.5625, 0.5, 0.6875, 0.4375, 0.5, 0.8125, 0.9375, 0.6875, 0.625, 0.6875, 0.625, 0.5625, 0.625, 0.5625, 0.6875, 0.5625, 0.625, 0.6875, 0.625, 0.75, 0.4375, 0.375, 0.75, 0.75, 0.625, 0.625, 0.625, 0.125, 0.625, 0.6875, 0.625, 0.5, 0.5625, 0.875, 0.75, 0.75, 0.875, 0.4375, 0.75, 0.625, 0.5, 0.875, 0.625, 0.75, 0.5625, 0.5, 0.4375, 0.4375, 0.5625, 0.4375, 0.75, 0.75, 0.8125, 0.6875, 0.75, 0.375, 0.0, 0.5, 0.625, 0.625, 0.6875, 0.625, 0.5, 0.5, 0.625, 0.8125, 0.8125, 0.8125, 0.8125, 0.75, 0.8125, 0.5, 0.4375, 0.4375, 0.75, 0.5625, 0.6875, 0.5, 0.6875, 0.6875, 0.625, 0.4375, 0.625, 0.625, 0.75, 0.625, 0.5625, 0.75, 0.5, 0.6875, 0.0, 0.75, 0.5625, 0.875, 0.4375, 0.5, 0.5, 0.75, 0.4375, 0.625, 0.75, 0.625, 0.625, 0.375, 0.5625, 0.625, 0.4375, 0.4375, 0.75, 0.75, 0.4375, 0.75, 0.5625, 0.8125, 0.4375, 0.75, 0.875, 0.5, 0.625, 0.375, 0.75, 0.5625, 0.625, 0.6875, 0.75, 0.4375, 0.5625, 0.5, 0.625, 0.625, 0.75, 0.6875, 0.6875, 0.625, 0.75, 0.625, 0.5, 0.75, 0.5625, 0.625, 0.625, 0.75, 0.25, 0.6875, 0.5625, 0.8125, 0.625, 0.625, 0.5625, 0.4375, 0.6875, 0.4375, 0.4375, 0.625, 0.8125, 0.625, 0.5625, 0.5625, 0.5, 0.625, 0.4375, 0.625, 0.6875, 0.8125, 0.5625, 0.5625, 0.375, 0.375, 0.5, 0.375, 0.75, 0.6875, 0.75, 0.4375, 0.5625, 0.75, 0.625, 0.4375, 0.5625, 0.375, 0.75, 0.4375, 0.75, 0.4375, 0.75, 0.5625, 0.625, 0.6875, 0.625, 0.375, 0.625, 0.5625, 0.5, 0.625, 0.5625, 0.375, 0.75, 0.5, 0.8125, 0.5, 0.8125, 0.75, 0.8125, 0.8125, 0.8125, 0.4375, 0.5625, 0.75, 0.8125, 0.5, 0.625, 0.3125, 0.6875, 0.625, 0.625, 0.5625, 0.625, 0.75, 0.4375, 0.75, 0.5, 0.8125, 0.5625, 0.0, 0.5625, 0.6875, 0.875, 0.6875, 0.6875, 0.6875, 0.8125, 0.625, 0.75, 0.4375, 0.5, 0.8125, 0.4375, 0.625, 0.5625, 0.375, 0.6875, 0.375, 0.625, 0.3125, 0.375, 0.5625, 0.4375, 0.4375, 0.8125, 0.5625, 0.5625, 0.625, 0.625, 0.8125, 0.75, 0.8125, 0.6875, 0.625, 0.75, 0.5, 0.5625, 1.0, 0.5, 0.75, 0.875, 0.6875, 0.625, 0.6875, 0.75, 0.8125, 0.6875, 0.4375, 0.625, 0.75, 0.5, 0.4375, 0.8125, 0.5625, 0.5625, 0.5, 0.625, 0.625, 1.0, 0.5, 0.4375, 0.5625, 0.75, 0.5, 0.5625, 0.625, 0.625, 0.6875, 0.8125, 0.5, 0.4375, 0.75, 0.4375, 0.6875, 0.75, 0.4375, 0.875, 0.625, 0.4375, 0.375, 0.4375, 0.5625, 0.75, 0.75, 0.6875, 0.75, 0.5, 0.6875, 0.4375, 0.6875, 1.0, 0.5625, 0.8125, 0.5, 0.6875, 0.875, 0.5625, 0.75, 0.125, 0.6875, 0.875, 0.8125, 0.5625, 0.8125, 0.5, 0.625, 0.4375, 0.375, 0.6875, 0.5, 0.5, 0.8125, 0.6875, 0.75, 0.375, 0.5, 0.4375, 0.75, 0.5, 0.5, 0.5, 0.5625, 0.625, 0.5, 0.5, 0.4375, 0.625, 0.75, 0.4375, 0.75, 0.8125, 0.75, 0.625, 0.8125, 0.6875, 0.4375, 0.75, 0.6875, 0.25, 0.625, 0.4375, 0.5625, 0.5625, 0.75, 0.5, 0.75, 0.75, 0.5625, 0.8125, 0.6875, 0.6875, 0.5, 0.625, 0.5625, 0.6875, 0.625, 0.75, 0.5, 0.375, 0.75, 0.6875, 0.6875, 0.3125, 0.5, 0.375, 0.625, 0.625, 0.875, 0.75, 0.625, 0.5625, 0.5, 0.6875, 0.8125, 0.625, 0.3125, 0.75, 0.5, 0.5625, 0.5625, 0.75, 0.75, 0.8125, 0.3125, 0.4375, 0.75, 0.5625, 0.4375, 0.625, 0.75, 0.6875, 0.5, 0.9375, 0.4375, 0.8125, 0.375, 0.4375, 0.5, 0.625, 0.5625, 0.75, 0.5, 0.6875, 0.5625, 0.8125, 0.75, 0.5625, 0.8125, 0.4375, 0.75, 0.5625, 0.75, 0.75, 0.4375, 0.6875, 0.3125, 0.5, 0.75, 0.625, 0.8125, 0.5, 0.4375, 0.75, 0.5625, 0.5, 0.75, 0.5625, 0.8125, 0.75, 0.6875, 0.5, 0.625, 0.625, 0.125, 0.625, 0.75, 0.5625, 0.625, 0.875, 0.625, 0.5, 0.4375, 0.6875, 0.125, 0.75, 0.5, 0.5, 0.8125, 0.8125, 0.5625, 0.5625, 0.6875, 0.6875, 0.5, 0.5, 0.625, 0.6875, 0.6875, 0.4375, 0.4375, 0.625, 0.75, 0.75, 0.8125, 0.5, 0.4375, 0.8125, 0.5, 0.0, 0.75, 0.625, 0.4375, 0.5, 0.625, 0.625, 0.6875, 0.625, 0.8125, 0.5, 0.5625, 0.4375, 0.8125, 0.6875, 0.4375, 0.625, 0.5, 0.625, 0.3125, 0.5, 0.625, 0.5, 0.5, 0.8125, 0.4375, 0.8125, 0.6875, 0.8125, 0.625, 0.75, 0.75, 0.6875, 0.75, 0.5, 0.375, 0.6875, 0.625, 0.5625, 0.5, 0.625, 0.5625, 0.625, 0.6875, 0.6875, 0.75, 0.3125, 0.5, 0.5, 0.4375, 0.75, 0.5625, 0.75, 0.6875, 0.75, 0.25, 0.6875, 0.4375, 0.625, 0.8125, 0.625, 0.8125, 0.4375, 0.625, 0.8125, 0.75, 0.75, 0.5, 0.625, 0.75, 0.5, 0.5625, 0.5625, 0.4375, 0.75, 0.4375, 0.5625, 0.6875, 0.5, 0.4375, 0.4375, 0.6875, 0.4375, 0.5625, 0.6875, 0.25, 0.5625, 0.75, 0.5, 0.625, 0.75, 0.5, 0.4375, 0.75, 0.625, 0.625, 0.75, 0.375, 0.5, 0.4375, 0.8125, 0.5, 0.6875, 0.75, 0.6875, 0.8125, 0.75, 0.125, 0.625, 0.625, 0.6875, 0.8125, 0.5, 0.625, 0.6875, 0.625, 0.5625, 0.4375, 0.5625, 0.4375, 0.5, 0.8125, 0.6875, 0.8125, 0.5, 0.5, 0.875, 0.375, 0.6875, 0.6875, 0.625, 0.5, 0.75, 0.4375, 0.8125, 0.75, 0.5625, 0.5625, 0.6875, 0.6875, 0.5, 0.625, 0.8125, 0.75, 0.625, 0.1875, 0.75, 0.6875, 0.8125, 0.375, 0.625, 0.5, 0.75, 0.5, 0.6875, 0.5, 0.75, 0.4375, 0.8125, 0.5625, 0.3125, 0.0625, 0.5, 0.6875, 0.75, 0.4375, 0.4375, 0.5625, 0.375, 0.75, 0.75, 0.375, 0.75, 0.625, 0.5625, 0.5625, 0.8125, 0.6875, 0.5625, 0.8125, 0.75, 0.875, 0.625, 0.6875, 0.875, 0.5, 0.6875, 0.625, 0.375, 0.375, 0.5, 0.4375, 0.6875, 0.5, 0.3125, 0.875, 0.6875, 0.5625, 0.6875, 0.4375, 0.4375, 0.375, 0.5625, 0.5, 0.5625, 0.5, 0.75, 0.75, 0.5, 0.75, 0.75, 0.6875, 0.5625, 0.8125, 0.6875, 0.6875, 0.625, 0.5, 0.5625, 0.75, 0.75, 0.4375, 0.5, 0.5, 0.75, 0.8125, 0.625, 0.75, 0.75, 0.6875, 0.4375, 0.5, 0.5, 0.0, 0.4375, 0.5625, 0.625, 0.5, 0.75, 0.6875, 0.375, 0.5625, 0.5, 0.6875, 0.75, 0.5, 0.625, 0.8125, 0.375, 0.5, 0.625, 0.5, 0.4375, 0.375, 0.625, 0.8125, 0.6875, 0.5625, 0.75, 0.4375, 0.5625, 0.5, 0.75, 0.6875, 0.5, 0.4375, 0.625, 0.75, 0.375, 0.625, 0.8125, 0.625, 0.5625, 0.625, 0.4375, 0.5, 0.6875, 0.625, 0.6875, 0.75, 0.6875, 0.5, 0.375, 0.5, 0.6875, 0.5, 0.8125, 0.8125, 0.625, 0.6875, 0.4375, 0.6875, 0.6875, 0.625, 0.625, 0.5, 0.625, 0.5, 0.625, 0.75, 0.75, 0.4375, 0.75, 0.75, 0.75, 0.75, 0.5625, 0.6875, 0.6875, 0.6875, 0.5, 0.5, 0.375, 0.625, 0.75, 0.75, 0.5, 0.5, 0.5, 0.8125, 0.75, 0.75, 0.8125, 0.5625, 0.75, 0.6875, 0.5, 0.75, 0.6875, 0.6875, 0.625, 0.3125, 0.8125, 0.6875, 0.5625, 0.5, 0.5, 0.75, 0.5, 0.8125, 0.8125, 0.5625, 0.625, 0.875, 0.75, 0.375, 0.9375, 0.5, 0.5625, 0.4375, 0.5, 0.4375, 0.625, 0.75, 0.5625, 0.625, 0.5, 0.6875, 0.8125, 0.625, 0.875, 0.5, 0.625, 0.4375, 0.8125, 0.5625, 0.4375, 0.75, 0.75, 0.5625, 0.5625, 0.5625, 0.6875, 0.5, 0.5, 0.5, 0.6875, 0.6875, 0.75, 0.5, 0.75, 0.5, 0.6875, 0.8125, 0.75, 0.8125, 0.75, 0.6875, 0.5625, 0.6875, 0.5, 0.8125, 0.75, 0.5, 0.5, 0.4375, 0.5625, 0.125, 0.75, 0.625, 0.9375, 0.625, 0.5, 0.75, 0.75, 0.8125, 0.5, 0.8125, 0.875, 0.5625, 0.625, 0.4375, 0.5625, 0.4375, 0.75, 0.0625, 0.75, 0.5, 0.8125, 0.6875, 0.6875, 0.5, 0.75, 0.375, 0.75, 0.5625, 0.625, 0.5625, 0.6875, 0.6875, 0.75, 0.75, 0.5625, 0.25, 0.8125, 0.5, 0.6875, 0.8125, 0.625, 0.8125, 0.8125, 0.6875, 0.25, 0.8125, 0.75, 0.625, 0.6875, 0.6875, 0.75, 0.375, 0.3125, 0.5, 0.5625, 0.8125, 0.375, 0.5625, 0.5625, 0.1875, 0.5, 0.6875, 0.6875, 0.0, 0.625, 0.5, 0.8125, 0.5625, 0.8125, 0.5, 0.625, 0.3125, 0.6875, 0.625, 0.625, 0.625, 0.75, 0.6875, 0.625, 0.25, 0.5625, 0.625, 0.625, 0.5625, 0.5625, 0.4375, 0.5, 0.75, 0.75, 0.5, 0.125, 0.8125, 0.6875, 0.5625, 0.5625, 0.75, 0.875, 0.4375, 0.6875, 0.6875, 0.4375, 1.0, 0.6875, 0.75, 0.75, 0.5625, 0.8125, 0.6875, 0.75, 0.75, 0.6875, 0.75, 0.5625, 0.4375, 0.625, 0.75, 0.75, 0.5, 0.5625, 0.3125, 0.625, 0.4375, 0.125, 0.75, 0.75, 0.75, 0.375, 0.4375, 0.6875, 0.6875, 0.8125, 0.625, 0.8125, 0.25, 0.75, 0.3125, 0.5625, 0.8125, 0.9375, 0.5, 0.5, 0.8125, 0.5625, 0.5, 0.8125, 0.8125, 0.875, 0.6875, 0.75, 0.25, 0.4375, 0.6875, 0.5625, 0.8125, 0.75, 0.4375, 0.5, 0.5, 0.75, 0.75, 0.6875, 0.625, 0.5, 0.6875, 0.5625, 0.8125, 0.75, 0.5625, 0.5625, 0.875, 0.625, 0.5, 0.6875, 0.875, 0.875, 0.5, 0.75, 0.5, 0.625, 0.25, 0.5, 0.5, 0.5, 0.8125, 0.4375, 0.6875, 0.625, 0.4375, 0.8125, 0.625, 0.625, 0.4375, 0.75, 0.75, 0.25, 0.5, 0.5, 0.75, 0.8125, 0.625, 0.5, 0.6875, 0.6875, 0.75, 0.75, 0.6875, 0.5, 0.625, 0.625, 0.625, 0.75, 0.5625, 0.75, 0.5625, 0.625, 0.3125, 0.75, 0.5625, 0.1875, 0.5, 0.625, 0.5625, 0.6875, 0.3125, 0.75, 0.8125, 0.1875, 0.4375, 0.625, 0.6875, 0.5, 0.625, 0.4375, 0.6875, 0.625, 0.6875, 0.5625, 0.5625, 0.625, 0.8125, 0.5, 0.4375, 0.5, 0.8125, 0.5625, 0.5, 0.625, 0.4375, 0.75, 0.4375, 0.6875, 0.8125, 0.375, 0.6875, 0.5625, 0.4375, 0.6875, 0.5, 0.5, 0.625, 0.5, 0.5, 0.3125, 0.875, 0.5, 0.3125, 0.4375, 0.6875, 0.6875, 0.625, 0.5, 0.5, 0.5, 0.5625, 0.625, 0.6875, 0.625, 0.5625, 0.5, 0.5625, 0.3125, 0.6875, 0.3125, 0.75, 0.75, 0.5625, 0.4375, 0.75, 0.5, 0.6875, 0.5625, 0.75, 0.1875, 0.8125, 0.75, 0.6875, 0.5, 0.6875, 0.75, 0.75, 0.5625, 0.6875, 0.4375, 0.875, 0.75, 0.5, 0.6875, 0.6875, 0.6875, 0.625, 0.6875, 0.625, 0.6875, 0.625, 0.5, 0.4375, 0.6875, 0.625, 0.5, 0.6875, 0.4375, 0.8125, 0.8125, 0.5625, 0.125, 0.75, 0.5, 0.6875, 0.875, 0.4375, 0.5, 0.4375, 0.5625, 0.625, 0.6875, 0.5, 0.5, 0.875, 0.8125, 0.6875, 0.5, 0.5625, 0.5625, 0.6875, 0.375, 0.5625, 0.75, 0.625, 0.5625, 0.75, 0.625, 0.5, 0.625, 0.625, 0.625, 0.5, 0.5, 0.75, 0.6875, 0.0, 0.5, 0.4375, 0.4375, 0.6875, 0.75, 0.75, 0.375, 0.75, 0.6875, 0.8125, 0.5, 0.6875, 0.4375, 0.75, 0.75, 0.6875, 0.5625, 0.6875, 0.5625, 0.8125, 0.6875, 0.5, 0.375, 0.5, 0.75, 0.5, 0.375, 0.6875, 0.3125, 0.5, 0.8125, 0.625, 0.4375, 0.5625, 0.75, 0.5625, 0.625, 0.4375, 0.3125, 0.75, 0.625, 0.4375, 0.4375, 0.5, 0.4375, 0.625, 0.4375, 0.5625, 0.6875, 0.5, 0.75, 0.625, 0.6875, 0.4375, 0.625, 0.625, 0.4375, 0.625, 0.8125, 0.8125, 0.6875, 0.8125, 0.375, 0.6875, 0.5, 0.5625, 0.5625, 0.1875, 0.875, 0.5, 0.625, 0.6875, 0.5625, 0.625, 0.5625, 0.75, 0.6875, 0.0, 0.4375, 0.4375, 0.75, 0.5, 0.75, 0.5, 0.5625, 0.25, 0.375, 0.5, 0.875, 0.1875, 0.5, 0.0625, 0.75, 0.5, 0.6875, 0.5, 0.625, 0.4375, 0.5, 0.8125, 0.5, 0.5625, 0.5, 0.875, 0.8125, 0.5625, 0.5625, 0.5, 0.75, 0.6875, 0.5, 0.4375, 0.5, 0.6875, 0.4375, 0.8125, 0.4375, 0.3125, 0.75, 0.5, 0.6875, 0.75, 0.6875, 0.375, 0.6875, 0.75, 0.75, 0.4375, 0.6875, 0.4375, 0.5, 0.6875, 0.75, 0.75, 0.8125, 0.4375, 0.5625, 0.8125, 0.6875, 0.5625, 0.75, 0.5625, 0.625, 0.8125, 0.75, 0.8125, 0.6875, 0.5, 0.625, 0.8125, 0.75, 0.75, 0.6875, 0.75, 0.75, 0.5, 0.625, 0.8125, 0.75, 0.75, 0.625, 0.625, 0.625, 0.625, 0.75, 0.4375, 0.5, 0.5, 0.4375, 0.5625, 0.5, 0.625, 0.5, 0.625, 0.875, 0.5, 0.4375, 0.625, 0.5625, 0.625, 0.625, 0.625, 0.75, 0.75, 0.75, 0.1875, 0.75, 0.5625, 0.375, 0.875, 0.4375, 0.4375, 0.4375, 0.6875, 0.5, 0.8125, 0.375, 0.4375, 0.75, 0.625, 0.4375, 0.5625, 0.4375, 0.625, 0.5625, 0.6875, 0.625, 0.5, 0.5625, 0.5, 0.5, 0.4375, 0.4375, 0.6875, 0.75, 0.5625, 0.8125, 0.625, 0.875, 0.75, 0.5, 0.6875, 0.5, 0.6875, 0.625, 0.625, 0.6875, 0.75, 0.75, 0.5, 0.625, 0.875, 0.625, 0.75, 0.75, 0.75, 0.4375, 0.4375, 0.75, 0.6875, 0.5625, 0.75, 0.25, 0.5625, 0.5, 0.5, 0.5625, 0.375, 0.5625, 0.6875, 0.625, 0.5625, 0.5625, 0.875, 0.6875, 0.75, 0.5, 0.375, 0.75, 0.625, 0.5, 0.625, 0.0, 0.6875, 0.625, 0.5, 0.4375, 0.8125, 0.8125, 0.6875, 0.4375, 0.75, 0.1875, 0.75, 0.4375, 0.5, 0.625, 0.4375, 0.6875, 0.75, 0.6875, 0.1875, 0.75, 0.6875, 0.5625, 0.625, 0.8125, 0.5, 0.5625, 0.4375, 0.75, 0.5625, 0.5, 0.75, 0.4375, 0.4375, 0.75, 0.625, 0.8125, 0.5, 0.5, 0.6875, 0.5625, 0.75, 0.5, 0.6875, 0.5, 0.5625, 0.75, 0.5, 0.4375, 0.4375, 0.625, 0.4375, 0.75, 0.4375, 0.5625, 0.625, 0.5, 0.75, 0.8125, 0.75, 0.875, 0.5, 0.5, 0.5, 0.5625, 0.5, 0.8125, 0.625, 0.5, 0.5625, 0.75, 0.625, 0.5, 0.625, 0.75, 0.75, 0.625, 0.75, 0.5, 0.75, 0.5, 0.75, 0.625, 0.5, 0.5625, 0.5, 0.6875, 0.5625, 0.6875, 0.625, 0.6875, 0.5, 0.1875, 0.375, 0.75, 0.75, 0.625, 0.75, 0.8125, 0.8125, 0.75, 0.6875, 0.6875, 0.5625, 0.6875, 0.5625, 0.5625, 0.75, 0.625, 0.625, 0.5625, 0.875, 0.6875, 0.625, 0.6875, 0.6875, 0.125, 0.5625, 0.4375, 0.5, 0.5, 0.5, 0.5625, 0.5, 0.4375, 0.8125, 0.375, 0.5625, 0.4375, 0.5, 0.375, 0.625, 0.5625, 0.8125, 0.0625, 0.6875, 0.6875, 0.4375, 0.625, 0.625, 0.75, 0.625, 0.5, 0.4375, 0.4375, 0.75, 0.75, 0.8125, 0.375, 0.75, 0.4375, 0.5, 0.6875, 0.5, 0.5, 0.6875, 0.625, 0.375, 0.4375, 0.6875, 0.6875, 0.6875, 0.8125, 0.4375, 0.6875, 0.8125, 0.8125, 0.875, 0.75, 0.75, 0.375, 0.8125, 0.75, 0.5, 0.8125, 0.75, 0.625, 1.0, 0.4375, 0.4375, 0.5625, 0.25, 0.75, 0.8125, 0.6875, 0.6875, 0.875, 0.625, 0.5, 0.6875, 0.4375, 0.375, 0.5, 0.4375, 0.5625, 0.875, 0.4375, 0.75, 0.4375, 0.5, 0.4375, 0.8125, 0.8125, 0.625, 0.4375, 0.6875, 0.4375, 0.5625, 0.5, 0.6875, 0.375, 0.75, 0.4375, 0.4375, 0.625, 0.4375, 0.3125, 0.875, 0.5625, 0.5625, 0.5625, 0.5, 0.625, 0.4375, 0.625, 0.75, 0.6875, 0.8125, 0.5, 0.75, 0.8125, 0.625, 0.4375, 0.625, 0.625, 0.5625, 0.375, 0.8125, 0.6875, 0.625, 0.625, 0.625, 0.6875, 0.625, 0.8125, 0.6875, 0.4375, 0.5, 0.5625, 0.625, 0.6875, 0.4375, 0.625, 0.75, 0.875, 0.5625, 0.8125, 0.6875, 0.375, 0.6875, 0.4375, 0.5, 0.125, 0.6875, 0.5625, 0.6875, 0.75, 0.5, 0.625, 0.5625, 0.6875, 0.0, 0.75, 0.6875, 0.625, 0.5, 0.4375, 0.4375, 0.5, 0.4375, 0.8125, 0.75, 0.75, 0.5, 0.75, 0.5, 0.4375, 0.625, 0.8125, 0.5625, 0.75, 0.625, 0.75, 0.4375, 0.5, 0.625, 0.75, 0.3125, 0.6875, 0.6875, 0.625, 0.8125, 0.5, 0.75, 0.625, 0.5, 0.1875, 0.6875, 0.625, 0.75, 0.6875, 0.5625, 0.5, 0.8125, 0.625, 0.75, 0.6875, 0.75, 0.75, 0.4375, 0.625, 0.5, 0.875, 0.5, 0.625, 0.8125, 0.3125, 0.625, 0.625, 0.5, 0.625, 1.0, 0.625, 0.375, 0.6875, 0.375, 0.75, 0.625, 0.5, 0.6875, 0.6875, 0.4375, 0.75, 0.5, 0.5, 0.3125, 0.8125, 0.4375, 0.5, 0.625, 0.5, 0.5, 0.75, 0.4375, 0.6875, 0.3125, 0.5625, 0.6875, 0.6875, 0.4375, 0.75, 0.5625, 0.625, 0.625, 0.5625, 0.625, 0.4375, 0.8125, 0.4375, 0.5625, 0.1875, 0.6875, 0.75, 0.875, 0.875, 0.5625, 0.5, 0.625, 0.625, 0.75, 0.625, 0.5, 0.5, 0.8125, 0.4375, 0.625, 0.3125, 0.375, 0.5625, 0.5625, 0.8125, 0.8125, 0.0625, 0.6875, 0.8125, 0.8125, 0.625, 0.6875, 0.375, 0.5625, 0.8125, 0.625, 0.8125, 0.6875, 0.5625, 0.4375, 0.5, 0.125, 0.5625, 0.75, 0.5625, 0.6875, 0.625, 0.4375, 0.6875, 0.4375, 0.625, 0.4375, 0.6875, 0.75, 0.1875, 0.6875, 0.6875, 0.6875, 0.625, 0.625, 0.75, 0.75, 0.5625, 0.9375, 0.625, 0.6875, 0.75, 0.625, 0.5625, 0.5625, 0.3125, 0.6875, 0.125, 0.5, 0.5, 0.625, 0.1875, 0.6875, 0.6875, 0.75, 0.625, 0.6875, 0.5, 0.5625, 0.5625, 0.6875, 0.625, 0.5625, 0.875, 0.4375, 0.6875, 0.625, 0.4375, 0.5, 0.75, 0.75, 0.75, 0.75, 0.125, 0.5, 0.6875, 0.375, 0.8125, 0.625, 0.4375, 0.75, 0.75, 0.625, 0.4375, 0.6875, 0.5, 0.75, 0.6875, 0.6875, 0.8125, 0.6875, 0.6875, 0.6875, 0.6875, 0.5, 0.5, 0.75, 0.6875, 0.75, 0.6875, 0.6875, 0.4375, 0.8125, 0.8125, 0.4375, 0.75, 0.1875, 0.6875, 0.6875, 0.75, 0.5, 0.625, 0.625, 0.8125, 0.4375, 0.75, 0.5, 0.8125, 0.5625, 0.75, 0.5625, 0.75, 0.875, 0.75, 0.5625, 0.4375, 0.5, 0.5, 0.5, 0.375, 0.75, 0.5625, 0.5, 0.375, 0.75, 0.75, 0.375, 0.5, 0.6875, 0.8125, 0.625, 0.625, 0.5625, 0.625, 0.75, 0.6875, 0.8125, 0.6875, 0.8125, 0.5, 0.8125, 0.625, 0.4375, 0.6875, 0.5625, 0.5, 0.8125, 0.625, 0.8125, 0.4375, 0.4375, 0.875, 0.6875, 0.625, 0.0, 0.5625, 0.625, 0.1875, 0.75, 0.5, 0.5, 0.625, 0.8125, 0.5625, 0.375, 0.5, 0.875, 0.5, 0.6875, 0.75, 0.8125, 0.5625, 0.5, 0.3125, 0.6875, 0.875, 0.6875, 0.5, 0.6875, 0.5625, 0.5, 0.5625, 0.5, 0.5625, 0.4375, 0.375, 0.5625, 0.625, 0.6875, 0.5, 0.6875, 0.5625, 0.8125, 0.5, 0.5, 0.75, 0.75, 0.625, 0.625, 0.4375, 0.8125, 0.5, 0.6875, 0.5625, 0.375, 0.4375, 0.3125, 0.625, 0.625, 0.5625, 0.5625, 0.25, 0.75, 0.5625, 0.5625, 0.5625, 0.6875, 0.8125, 0.6875, 0.625, 0.75, 0.5625, 0.5, 0.6875, 0.8125, 0.625, 0.875, 0.5, 0.5625, 0.625, 0.4375, 0.6875, 0.4375, 0.6875, 0.5625, 0.625, 0.75, 0.6875, 0.75, 0.9375, 0.75, 0.4375, 0.75, 0.75, 0.8125, 0.625, 0.8125, 0.75, 0.75, 0.4375, 0.875, 0.4375, 0.625, 0.625, 0.6875, 0.8125, 0.75, 0.8125, 0.5, 0.5, 0.6875, 0.6875, 0.6875, 0.75, 0.4375, 0.6875, 0.5625, 0.6875, 0.625, 0.5625, 0.6875, 0.625, 0.625, 0.4375, 0.875, 0.875, 0.5, 0.5, 0.625, 0.5, 0.625, 0.625, 0.875, 0.5, 0.5, 0.5, 0.875, 0.5, 0.5625, 0.625, 0.4375, 0.4375, 0.75, 0.5625, 0.5625, 0.75, 0.375, 0.6875, 0.875, 0.5, 0.5, 0.625, 0.5625, 0.5, 0.6875, 0.6875, 0.5625, 0.75, 0.75, 0.625, 0.625, 0.625, 0.5625, 0.5625, 0.1875, 0.5, 0.1875, 0.5625, 0.625, 0.625, 0.625, 0.5625, 0.75, 0.4375, 0.5625, 0.625, 0.5625, 0.75, 0.5625, 0.5625, 0.75, 0.5, 0.625, 0.8125, 0.625, 0.75, 0.6875, 0.5625, 0.875, 0.6875, 0.875, 0.4375, 0.75, 0.8125, 0.8125, 0.6875, 0.5625, 0.5625, 0.625, 0.625, 0.8125, 0.5625, 0.5, 0.6875, 0.8125, 0.375, 0.4375, 0.4375, 0.5625, 0.5, 0.8125, 0.625, 0.625, 0.75, 0.5625, 0.6875, 0.75, 0.6875, 0.4375, 0.75, 0.625, 0.625, 0.8125, 0.5625, 0.75, 0.4375, 0.625, 0.75, 0.6875, 0.75, 0.5625, 0.75, 0.5625, 0.5625, 0.75, 0.375, 0.5625, 0.6875, 0.375, 0.8125, 0.75, 0.625, 0.75, 0.75, 0.5, 0.4375, 0.5, 0.5, 0.625, 0.6875, 0.5625, 0.1875, 0.6875, 0.875, 0.1875, 0.6875, 0.5, 0.4375, 0.625, 0.75, 0.625, 0.6875, 0.625, 0.8125, 0.75, 0.6875, 0.6875, 0.6875, 0.8125, 0.6875, 0.5625, 0.8125, 0.375, 0.6875, 0.5, 0.75, 0.5625, 0.6875, 0.75, 0.375, 0.5, 0.5625, 0.5, 0.5625, 0.4375, 0.75, 0.5625, 0.75, 0.8125, 0.75, 0.5625, 0.75, 0.6875, 0.5625, 0.75, 0.625, 0.75, 0.8125, 0.6875, 0.5625, 0.375, 0.6875, 0.625, 0.5625, 0.6875, 0.5625, 0.75, 0.625, 0.875, 0.75, 0.5625, 0.6875, 0.5, 0.75, 0.8125, 0.5625, 0.625, 0.3125, 0.625, 0.625, 0.3125, 0.875, 0.5625, 0.5625, 0.75, 0.75, 0.6875, 0.4375, 0.4375, 0.625, 0.6875, 0.5625, 0.5, 0.625, 0.6875, 0.75, 0.75, 0.5625, 0.625, 0.6875, 0.625, 0.4375, 0.5, 0.3125, 0.4375, 0.5625, 0.75, 0.5, 0.6875, 0.625, 0.625, 0.75, 0.5, 0.4375, 0.5, 0.6875, 0.4375, 0.4375, 0.625, 0.625, 0.3125, 0.75, 0.8125, 0.625, 0.5625, 0.625, 0.5, 0.75, 0.6875, 0.75, 0.4375, 0.5, 0.5, 0.5625, 0.5625, 0.5625, 0.625, 0.375, 0.25, 0.5625, 0.5, 0.625, 0.125, 0.4375, 0.75, 0.625, 0.4375, 0.6875, 0.5, 0.3125, 0.875, 0.6875, 0.5625, 0.5625, 0.6875, 0.5625, 0.6875, 0.8125, 0.75, 0.5, 0.5, 0.8125, 0.4375, 0.625, 0.75, 0.5625, 0.5, 0.625, 0.6875, 0.8125, 0.6875, 0.6875, 0.5, 0.5625, 0.625, 0.75, 0.75, 0.75, 0.5625, 0.6875, 0.75, 0.5, 0.5625, 0.6875, 0.6875, 0.5, 0.625, 0.875, 0.75, 0.8125, 0.625, 0.5625, 0.4375, 0.625, 0.6875, 0.5, 0.6875, 0.75, 0.625, 0.625, 0.375, 0.625, 0.4375, 0.5625, 0.625, 0.75, 0.6875, 0.8125, 0.625, 0.625, 0.5, 0.875, 0.75, 0.8125, 0.6875, 0.4375, 0.625, 0.625, 0.4375, 0.5, 0.5, 0.625, 0.5, 0.4375, 0.4375, 0.875, 0.625, 0.4375, 0.5625, 0.625, 0.625, 0.5625, 0.625, 0.75, 0.5, 0.3125, 0.4375, 0.5625, 0.5, 0.8125, 0.75, 0.5, 0.4375, 0.5625, 0.75, 0.6875, 1.0, 0.875, 0.8125, 0.5, 0.6875, 0.6875, 0.6875, 0.6875, 0.125, 0.4375, 0.6875, 0.875, 0.75, 0.5, 0.625, 0.6875, 0.9375, 0.75, 0.5625, 0.625, 0.75, 0.25, 0.5, 0.8125, 0.3125, 0.375, 0.4375, 0.125, 0.625, 0.8125, 0.375, 0.5, 0.5, 0.625, 0.6875, 0.6875, 0.75, 0.625, 0.625, 0.5, 0.75, 0.6875, 0.5, 0.75, 0.4375, 0.75, 0.3125, 0.8125, 0.4375, 0.75, 0.6875, 0.625, 0.75, 0.5, 0.5625, 0.6875, 0.625, 0.6875, 0.375, 0.1875, 0.5, 0.4375, 0.75, 0.75, 0.5, 0.8125, 0.75, 0.8125, 0.75, 0.75, 0.4375, 0.5, 0.4375, 0.5, 0.5625, 0.625, 0.5625, 0.625, 0.625, 0.5625, 0.75, 0.0, 0.625, 0.6875, 0.5, 0.5625, 0.5625, 0.5, 0.4375, 0.4375, 0.5, 0.5625, 0.75, 0.875, 0.5625, 0.625, 0.8125, 0.4375, 0.75, 0.6875, 0.6875, 0.8125, 0.8125, 0.6875, 0.8125, 0.75, 0.75, 0.6875, 0.8125, 0.625, 0.6875, 0.6875, 0.625, 0.75, 0.625, 0.5, 0.625, 0.6875, 0.8125, 0.8125, 0.5625, 0.5625, 0.625, 0.8125, 0.625, 0.6875, 0.5, 0.8125, 0.625, 0.6875, 0.875, 0.5, 0.5625, 0.4375, 0.4375, 0.6875, 0.5, 0.6875, 0.5, 0.625, 0.75, 0.75, 0.6875, 0.5, 0.75, 0.5, 0.4375, 0.5, 0.75, 0.875, 0.5625, 0.4375, 0.6875, 0.5, 0.625, 0.5625, 0.625, 0.8125, 0.625, 0.625, 0.5, 0.6875, 0.375, 0.625, 0.8125, 0.75, 0.25, 0.75, 0.5625, 0.5, 0.6875, 0.5, 0.75, 0.75, 0.4375, 0.5625, 0.625, 0.6875, 0.5, 0.625, 0.5625, 0.5625, 0.625, 0.4375, 0.375, 0.4375, 0.5625, 0.5625, 0.5625, 0.625, 0.6875, 0.6875, 0.5625, 0.1875, 0.4375, 0.5625, 0.5625, 0.5625, 0.4375, 0.5, 0.625, 0.5, 0.6875, 0.75, 0.5, 0.3125, 0.8125, 0.9375, 0.6875, 0.6875, 0.75, 0.5625, 0.75, 0.5, 0.5, 0.5625, 0.625, 0.6875, 0.6875, 0.875, 0.8125, 0.375, 0.625, 0.625, 0.6875, 0.5625, 0.75, 0.6875, 0.8125, 0.75, 0.6875, 0.8125, 0.4375, 0.8125, 0.6875, 0.5625, 0.75, 0.5625, 0.625, 0.4375, 0.625, 0.5, 0.5, 0.8125, 0.4375, 0.625, 0.75, 0.6875, 0.5, 0.5, 0.625, 0.625, 0.625, 0.4375, 0.5, 0.5625, 0.625, 0.5, 0.5, 0.75, 0.4375, 0.5625, 0.5625, 0.5, 0.5, 0.625, 0.8125, 0.5, 0.375, 0.5625, 0.625, 0.75, 0.5625, 0.3125, 0.5, 0.75, 0.5625, 0.5, 0.75, 0.625, 0.5, 0.625, 0.5, 0.5, 0.4375, 0.6875, 0.5, 0.5625, 0.75, 0.75, 0.5, 0.6875, 0.5625, 0.4375, 0.75, 0.6875, 0.5, 0.5, 0.75, 0.4375, 0.5, 0.375, 0.6875, 0.4375, 0.5, 0.625, 0.75, 0.6875, 0.75, 0.6875, 0.5625, 0.75, 0.9375, 0.5, 0.875, 0.625, 0.75, 0.6875, 0.625, 0.875, 0.6875, 0.6875, 0.5, 0.6875, 0.3125, 0.6875, 0.6875, 0.5625, 0.375, 0.5, 0.125, 0.5, 0.6875, 0.75, 0.875, 0.4375, 0.75, 0.5, 0.5625, 0.125, 0.9375, 0.625, 0.6875, 0.6875, 0.75, 0.5625, 0.6875, 0.5, 0.625, 0.75, 0.8125, 0.6875, 0.4375, 0.4375, 0.3125, 0.4375, 0.625, 0.625, 0.6875, 0.8125, 0.8125, 0.75, 0.625, 0.4375, 0.625, 0.6875, 0.625, 0.6875, 0.6875, 0.4375, 0.5625, 0.5, 0.6875, 0.75, 0.5625, 0.0, 0.5, 0.1875, 0.4375, 0.75, 0.625, 0.4375, 0.6875, 0.5, 0.625, 0.5, 0.6875, 0.625, 0.75, 0.625, 0.625, 0.125, 0.625, 0.5625, 0.8125, 0.75, 0.625, 0.75, 0.5, 0.75, 0.3125, 0.75, 0.6875, 0.625, 0.9375, 0.5, 0.75, 0.5, 0.4375, 0.6875, 0.5, 0.6875, 0.8125, 0.3125, 0.75, 0.6875, 0.75, 0.5, 0.5625, 0.625, 0.75, 0.5, 0.75, 0.6875, 0.4375, 0.6875, 0.4375, 0.125, 0.75, 0.6875, 0.6875, 0.6875, 0.4375, 0.8125, 0.8125, 0.5, 0.4375, 0.5, 0.3125, 0.5625, 0.6875, 0.6875, 0.4375, 0.4375, 0.3125, 0.6875, 0.4375, 0.75, 0.625, 0.6875, 0.8125, 0.75, 0.6875, 0.375, 0.5, 1.0, 0.6875, 0.375, 0.6875, 0.5, 0.75, 0.5625, 0.6875, 0.75, 0.8125, 0.375, 0.4375, 0.6875, 0.625, 1.0, 0.4375, 0.125, 0.8125, 0.625, 0.5, 0.5, 0.4375, 0.6875, 0.5, 0.75, 0.75, 0.5, 0.5625, 0.8125, 0.5625, 0.6875, 0.6875, 0.6875, 0.75, 0.75, 0.5, 0.625, 0.625, 0.6875, 0.5, 0.5625, 0.625, 0.625, 0.5, 0.5, 0.625, 1.0, 0.75, 0.625, 0.4375, 0.4375, 0.625, 0.625, 0.75, 0.8125, 0.5, 0.5, 0.4375, 0.6875, 0.8125, 0.75, 0.625, 0.875, 0.625, 0.6875, 0.75, 0.75, 0.875, 0.125, 0.4375, 0.4375, 0.5, 0.6875, 0.1875, 0.4375, 0.5625, 0.4375, 0.3125, 0.5625, 0.4375, 0.625, 0.75, 0.3125, 0.4375, 0.5625, 0.25, 0.625, 0.8125, 0.5625, 0.625, 0.4375, 0.625, 0.125, 0.4375, 0.5625, 0.75, 0.4375, 0.5, 0.5, 0.6875, 0.75, 0.5, 0.6875, 0.5, 0.625, 0.5625, 0.9375, 0.875, 0.6875, 0.75, 0.3125, 0.0, 0.5625, 0.625, 0.4375, 0.75, 0.5625, 0.875, 0.5625, 0.4375, 0.75, 0.1875, 0.6875, 0.6875, 0.4375, 0.375, 0.5625, 0.625, 0.4375, 0.625, 0.5, 0.8125, 0.5, 0.5625, 0.5, 0.625, 0.5625, 0.625, 0.625, 0.5, 0.5, 0.5625, 0.75, 0.75, 0.4375, 0.5625, 0.8125, 0.625, 0.8125, 0.8125, 0.75, 0.4375, 0.75, 0.625, 0.6875, 0.5, 0.625, 0.4375, 0.6875, 0.625, 0.5, 0.25, 0.5625, 0.375, 0.6875, 0.6875, 0.4375, 0.5, 0.6875, 0.875, 0.75, 0.5625, 0.5, 0.625, 0.375, 0.5, 0.8125, 0.6875, 0.8125, 0.5625, 0.6875, 0.4375, 0.5, 0.6875, 0.4375, 0.6875, 0.6875, 0.5, 0.8125, 0.75, 0.75, 0.75, 0.75, 0.6875, 0.4375, 0.5, 0.375, 0.625, 0.625, 0.625, 0.5, 0.5, 0.4375, 0.8125, 0.6875, 0.625, 0.4375, 0.5, 0.5625, 0.125, 0.5, 0.4375, 0.5, 0.5625, 0.625, 0.75, 0.625, 0.8125, 0.625, 0.75, 0.6875, 0.75, 0.625, 0.25, 0.5625, 0.5625, 0.625, 0.75, 0.5625, 0.625, 0.75, 0.5, 0.75, 0.8125, 0.5625, 0.6875, 0.5625, 0.6875, 0.5625, 0.625, 0.75, 0.5625, 0.1875, 0.8125, 0.8125, 0.4375, 0.3125, 0.75, 0.625, 0.6875, 0.625, 0.8125, 0.625, 0.8125, 0.75, 0.4375, 0.625, 0.625, 0.8125, 0.8125, 0.875, 0.4375, 0.75, 0.6875, 0.6875, 0.6875, 0.75, 0.5625, 0.75, 0.625, 0.75, 0.625, 0.625, 0.625, 0.8125, 0.6875, 0.5625, 0.75, 0.75, 0.3125, 0.625, 0.625, 0.3125, 0.5, 0.75, 0.4375, 0.5, 0.0, 0.5, 0.75, 0.5, 0.8125, 0.6875, 0.6875, 0.8125, 0.5625, 0.625, 0.5625, 0.5625, 0.5, 0.5, 0.5, 0.625, 0.5625, 0.5, 0.5, 0.5625, 0.6875, 0.8125, 0.4375, 0.5, 0.5, 0.375, 0.75, 0.8125, 0.625, 0.75, 0.4375, 0.5625, 0.5, 0.75, 0.5, 0.5, 0.8125, 0.5, 0.625, 0.5, 0.75, 0.5, 0.6875, 0.4375, 0.6875, 0.5625, 0.5, 0.75, 0.625, 0.6875, 0.8125, 0.5625, 0.4375, 0.75, 0.6875, 0.5, 0.5, 0.5, 0.625, 0.5, 0.75, 0.625, 0.625, 0.3125, 0.75, 0.625, 0.625, 0.5, 0.875, 0.4375, 0.625, 0.5625, 0.75, 0.625, 0.625, 0.6875, 0.5625, 0.75, 0.5625, 0.75, 0.8125, 0.75, 0.5625, 0.5, 0.625, 0.625, 0.75, 0.8125, 0.625, 0.75, 0.75, 0.75, 0.75, 0.625, 0.75, 0.75, 0.5, 0.75, 0.5, 0.625, 0.75, 0.5625, 0.6875, 0.6875, 0.625, 0.4375, 0.5625, 0.4375, 0.6875, 0.875, 0.625, 0.75, 0.75, 0.75, 0.4375, 0.5625, 0.5625, 0.125, 0.6875, 0.4375, 0.5, 0.5, 0.1875, 0.5, 0.5, 0.4375, 0.625, 0.25, 0.4375, 0.4375, 0.375, 0.75, 0.3125, 0.625, 0.75, 0.75, 0.75, 0.4375, 0.75, 0.4375, 0.625, 0.5, 0.75, 0.375, 0.4375, 0.3125, 0.1875, 0.8125, 0.6875, 0.3125, 0.625, 0.5625, 0.625, 0.6875, 0.8125, 0.4375, 0.4375, 0.8125, 0.75, 0.625, 0.4375, 0.8125, 0.75, 0.75, 0.4375, 0.75, 0.75, 0.4375, 0.4375, 0.6875, 0.3125, 0.5, 0.6875, 0.75, 0.5, 0.4375, 0.6875, 0.4375, 0.75, 0.5625, 0.75, 0.625, 0.8125, 0.75, 0.6875, 0.5, 0.8125, 0.5, 0.6875, 0.6875, 0.8125, 0.5, 0.8125, 0.625, 0.5, 0.6875, 0.4375, 0.625, 0.5625, 0.6875, 0.375, 0.8125, 0.5, 0.6875, 0.5625, 0.25, 0.5625, 0.5625, 0.625, 0.4375, 0.625, 0.4375, 0.4375, 0.625, 0.5625, 0.75, 0.8125, 0.8125, 0.6875, 0.625, 0.5, 0.5, 0.75, 0.75, 1.0, 0.6875, 0.75, 0.5, 0.5, 0.75, 0.5, 0.5, 0.5625, 0.5625, 0.4375, 0.6875, 0.5, 0.6875, 0.5625, 0.6875, 0.3125, 0.75, 0.375, 0.5, 0.5625, 0.6875, 0.875, 0.625, 0.375, 0.625, 0.5, 0.5, 0.5, 0.75, 0.5, 0.75, 0.1875, 0.6875, 0.625, 0.625, 0.8125, 0.4375, 0.5625, 0.5625, 0.6875, 0.3125, 0.75, 0.75, 0.5, 0.75, 0.6875, 0.5, 0.6875, 0.6875, 0.6875, 0.5, 0.6875, 0.5625, 0.5625, 0.75, 0.625, 0.6875, 0.375, 0.625, 0.625, 0.75, 0.625, 0.625, 0.5625, 0.6875, 0.125, 0.625, 0.5625, 0.75, 0.5, 0.3125, 0.6875, 0.8125, 0.875, 0.4375, 0.625, 0.1875, 0.5, 0.625, 0.5, 0.3125, 0.4375, 0.4375, 0.4375, 0.875, 0.25, 0.5, 0.6875, 0.625, 0.4375, 0.5, 0.75, 0.5625, 0.6875, 0.6875, 0.6875, 0.75, 0.6875, 0.375, 0.625, 0.4375, 0.5, 0.75, 0.3125, 0.5625, 0.5, 0.8125, 0.5625, 0.5, 0.6875, 0.9375, 0.75, 0.4375, 0.625, 0.625, 0.75, 0.625, 0.75, 0.6875, 0.6875, 0.875, 0.625, 0.5, 0.125, 0.5, 0.5, 0.8125, 0.9375, 0.4375, 0.8125, 0.75, 0.5, 0.625, 0.6875, 0.375, 0.625, 0.5625, 0.4375, 0.75, 0.8125, 0.4375, 0.625, 0.75, 0.75, 0.75, 0.6875, 0.5, 0.6875, 0.625, 0.5625, 0.4375, 0.75, 0.375, 0.5, 0.75, 0.5625, 0.4375, 0.5, 0.375, 0.25, 0.75, 0.625, 0.5, 0.5625, 0.625, 0.5, 0.625, 0.8125, 0.375, 0.625, 0.75, 0.4375, 0.625, 0.75, 0.625, 0.5, 0.75, 0.6875, 0.4375, 0.5, 0.625, 0.875, 0.5, 0.6875, 0.75, 0.75, 0.6875, 0.625, 0.6875, 0.4375, 0.8125, 0.4375, 0.6875, 0.625, 0.4375, 0.0625, 0.6875, 0.8125, 0.5, 0.6875, 0.5, 0.5625, 0.375, 0.5, 0.5625, 0.625, 0.1875, 0.4375, 0.4375, 0.5, 0.0625, 0.4375, 0.6875, 0.6875, 0.5, 0.5, 0.125, 0.8125, 0.5, 0.5, 0.8125, 0.375, 0.5625, 0.4375, 0.875, 0.75, 0.5, 0.6875, 0.5625, 0.4375, 0.6875, 0.4375, 0.8125, 0.5, 0.625, 0.875, 0.4375, 0.5, 0.75, 0.6875, 0.75, 0.6875, 0.5625, 0.75, 0.625, 0.75, 0.5, 0.5625, 0.4375, 0.6875, 0.5625, 0.625, 0.625, 0.75, 0.8125, 0.4375, 0.8125, 0.4375, 0.5, 0.75, 0.75, 0.625, 0.75, 0.625, 0.625, 0.5, 0.4375, 0.5625, 0.5625, 0.625, 0.75, 0.4375, 0.5, 0.1875, 0.625, 0.8125, 0.6875, 0.8125, 0.8125, 0.5, 0.3125, 0.5625, 0.6875, 0.75, 0.375, 0.125, 0.75, 0.8125, 0.75, 0.25, 0.75, 0.6875, 0.625, 0.5625, 0.5625, 0.6875, 0.375, 0.6875, 0.5, 0.6875, 0.6875, 0.625, 0.75, 0.5625, 0.625, 0.5625, 0.5625, 0.4375, 0.6875, 0.8125, 0.5, 0.5625, 0.625, 0.5, 0.5625, 0.5625, 0.75, 0.75, 0.5, 0.6875, 0.75, 0.5625, 0.5625, 0.6875, 0.75, 0.625, 0.375, 0.6875, 0.75, 0.625, 0.4375, 0.375, 0.25, 0.8125, 0.625, 0.5, 1.0, 0.4375, 0.625, 0.6875, 0.375, 0.8125, 0.8125, 0.4375, 0.75, 0.5625, 0.5, 0.625, 0.5625, 0.5625, 0.5, 0.625, 0.6875, 0.4375, 0.75, 0.6875, 0.75, 0.75, 0.5, 0.375, 0.0, 0.5, 0.6875, 0.6875, 0.6875, 0.75, 0.4375, 0.75, 0.75, 0.4375, 0.375, 0.625, 0.6875, 0.8125, 0.6875, 0.5, 0.0, 0.75, 0.4375, 0.5, 0.5, 0.75, 0.5, 0.6875, 0.625, 0.75, 0.75, 0.625, 0.5625, 0.5625, 0.625, 0.5, 0.6875, 0.5625, 0.5625, 0.8125, 0.625, 0.625, 0.375, 0.0, 0.75, 0.75, 0.5625, 0.875, 0.375, 0.6875, 0.5625, 0.4375, 0.4375, 0.4375, 0.625, 0.75, 0.625, 0.5, 0.5625, 0.75, 0.6875, 0.6875, 0.5, 0.5, 0.625, 0.625, 0.6875, 0.625, 0.625, 0.625, 0.375, 0.75, 0.8125, 0.5625, 0.625, 0.25, 0.6875, 0.375, 0.5625, 0.375, 0.625, 0.5, 0.5625, 0.4375, 0.4375, 0.5, 0.75, 0.8125, 0.5, 0.625, 0.6875, 0.6875, 0.6875, 0.6875, 0.8125, 0.5625, 0.5625, 0.6875, 0.5, 0.8125, 0.5625, 0.5625, 0.375, 0.75, 0.5625, 0.5625, 0.5, 0.4375, 0.5625, 0.625, 0.4375, 0.5, 0.625, 0.4375, 0.5625, 0.25, 0.6875, 0.8125, 0.6875, 0.75, 0.75, 1.0, 0.4375, 0.4375, 0.375, 0.5625, 0.5625, 0.5625, 0.5, 0.8125, 0.75, 0.5625, 0.0, 0.8125, 0.6875, 0.5625, 0.625, 0.5, 0.75, 0.8125, 0.75, 0.0, 0.5625, 0.75, 0.6875, 0.6875, 0.625, 0.5, 0.5, 0.5, 0.4375, 0.375, 0.375, 0.625, 0.625, 0.5, 0.4375, 0.625, 0.4375, 0.75, 0.4375, 0.4375, 0.5, 0.75, 0.8125, 0.625, 0.375, 0.5625, 0.75, 0.6875, 0.5, 0.625, 0.5, 0.5, 0.75, 0.6875, 0.6875, 0.4375, 0.4375, 0.5625, 0.625, 0.5, 0.5625, 0.75, 0.625, 0.75, 0.75, 0.625, 0.6875, 0.5625, 0.625, 0.25, 0.25, 0.75, 0.625, 0.75, 0.25, 0.4375, 0.6875, 0.5, 0.5625, 0.375, 0.4375, 0.5, 0.5, 1.0, 0.6875, 0.6875, 0.875, 0.5625, 0.8125, 0.6875, 0.5625, 0.6875, 0.875, 0.75, 0.6875, 0.5, 0.75, 0.5, 0.8125, 0.5, 0.8125, 0.5625, 0.6875, 0.5625, 0.8125, 0.375, 0.9375, 0.625, 0.5, 0.75, 0.5625, 0.8125, 0.625, 0.5, 0.4375, 0.625, 0.75, 0.5625, 0.5, 0.75, 0.8125, 0.75, 0.8125, 0.5625, 0.8125, 0.75, 0.625, 0.75, 0.25, 0.5625, 0.6875, 0.5, 0.625, 0.125, 0.625, 0.3125, 0.5625, 0.75, 0.4375, 0.5625, 0.4375, 0.875, 0.625, 0.625, 0.25, 0.5625, 0.6875, 0.6875, 0.5625, 0.625, 0.4375, 0.75, 0.4375, 0.625, 0.6875, 0.75, 0.4375, 0.75, 0.5625, 0.4375, 0.4375, 0.6875, 0.75, 0.5, 0.625, 0.5625, 0.5625, 0.5625, 0.5625, 0.6875, 0.625, 0.875, 0.8125, 0.625, 0.5625, 0.625, 0.5625, 0.5, 0.625, 0.625, 0.5625, 0.6875, 0.6875, 0.75, 0.4375, 0.625, 0.5625, 0.5625, 0.5625, 0.375, 0.4375, 0.75, 0.625, 0.8125, 0.75, 0.8125, 0.625, 0.8125, 0.75, 0.8125, 0.8125, 0.5, 0.75, 0.5625, 0.625, 0.625, 0.4375, 0.75, 0.875, 0.5, 0.625, 0.6875, 0.625, 0.5625, 0.4375, 0.75, 0.75, 0.625, 0.875, 0.5, 0.4375, 0.4375, 0.4375, 0.6875, 0.5625, 0.5625, 0.3125, 0.6875, 0.8125, 0.5625, 0.75, 0.75, 0.5625, 0.8125, 0.4375, 0.625, 0.6875, 0.0625, 0.5, 0.625, 0.5, 0.6875, 0.75, 0.625, 0.8125, 0.8125, 0.5, 0.75, 0.5, 0.5625, 0.6875, 0.5, 0.75, 0.5, 0.6875, 0.75, 0.5625, 0.5, 0.6875, 0.4375, 0.125, 0.8125, 0.625, 0.75, 0.625, 0.5625, 0.8125, 0.6875, 0.6875, 0.4375, 0.6875, 0.5625, 0.75, 0.5625, 0.6875, 0.4375, 0.625, 0.625, 0.75, 0.375, 0.75, 0.5625, 0.5, 0.5, 0.6875, 0.4375, 0.4375, 0.6875, 0.875, 0.875, 0.9375, 0.375, 0.5, 0.8125, 0.75, 0.1875, 0.5, 0.3125, 0.75, 0.625, 0.5, 0.5, 0.4375, 0.4375, 0.8125, 0.625, 0.5, 0.4375, 0.375, 0.6875, 0.5625, 0.5, 0.75, 0.6875, 0.6875, 0.75, 0.8125, 0.6875, 0.5, 0.8125, 0.6875, 0.5625, 0.625, 0.6875, 0.625, 0.875, 0.75, 0.6875, 0.625, 0.4375, 0.6875, 0.625, 0.5, 0.625, 0.6875, 0.6875, 0.75, 0.375, 0.75, 0.6875, 0.5625, 0.5, 0.25, 0.75, 0.75, 0.75, 0.5625, 0.625, 0.75, 0.5625, 0.8125, 0.4375, 0.5, 0.5625, 0.375, 0.5, 0.375, 0.625, 0.4375, 0.4375, 0.375, 0.75, 0.5, 0.625, 0.5, 0.5, 0.625, 0.6875, 0.6875, 0.6875, 0.5625, 0.5, 0.5, 0.5625, 0.8125, 0.5625, 0.5625, 0.75, 0.5, 0.5625, 0.375, 0.75, 0.625, 0.625, 0.8125, 0.625, 0.625, 0.625, 0.8125, 0.75, 0.5625, 0.6875, 0.8125, 0.75, 0.6875, 0.75, 0.4375, 0.0, 0.75, 0.5, 0.375, 0.625, 0.75, 0.5625, 0.625, 0.625, 0.5, 0.75, 0.5, 0.5, 0.8125, 0.75, 0.5, 0.5, 0.5, 0.6875, 0.5, 0.4375, 0.8125, 0.6875, 0.8125, 0.5, 0.625, 0.625, 0.75, 0.8125, 0.625, 0.75, 0.5, 0.6875, 0.6875, 0.8125, 0.625, 0.8125, 0.75, 0.6875, 0.8125, 0.5625, 0.875, 0.5, 0.5625, 0.5, 0.5625, 0.5, 0.5625, 0.5625, 0.6875, 0.4375, 0.5, 0.375, 0.6875, 0.4375, 0.6875, 0.6875, 0.8125, 0.375, 0.8125, 0.4375, 0.6875, 0.4375, 0.625, 0.6875, 0.875, 0.75, 0.4375, 0.6875, 0.75, 0.5, 0.75, 0.25, 0.625, 0.8125, 0.3125, 0.8125, 0.5, 0.625, 0.625, 0.5, 0.625, 0.5, 0.8125, 0.5625, 0.5625, 0.6875, 0.5625, 0.625, 0.8125, 0.5625, 0.5625, 0.6875, 0.625, 0.375, 0.4375, 0.6875, 0.5625, 0.6875, 0.75, 0.4375, 0.5625, 0.75, 0.875, 0.5, 0.8125, 0.75, 0.5, 0.6875, 0.4375, 0.6875, 0.625, 0.625, 0.6875, 0.4375, 0.75, 0.75, 0.75, 0.4375, 0.75, 0.5, 0.8125, 0.625, 0.6875, 0.6875, 0.6875, 0.6875, 0.625, 0.75, 0.75, 0.75, 0.4375, 0.4375, 0.4375, 0.125, 0.5, 0.6875, 0.5, 0.75, 0.3125, 0.625, 0.4375, 0.5, 0.375, 0.5625, 0.375, 0.6875, 0.6875, 0.6875, 0.625, 0.25, 0.4375, 0.5, 0.625, 0.4375, 0.8125, 0.0625, 0.5, 0.8125, 0.5, 1.0, 0.875, 0.125, 0.625, 0.5, 0.6875, 0.8125, 0.75, 0.625, 0.8125, 0.8125, 0.625, 0.8125, 0.4375, 0.8125, 0.625, 0.5625, 0.6875, 0.75, 0.5625, 0.8125, 0.6875, 0.5, 0.75, 0.5625, 0.4375, 0.625, 0.4375, 0.75, 0.0, 0.6875, 0.5, 0.75, 0.625, 0.5625, 0.5, 0.625, 0.8125, 0.8125, 0.6875, 0.5, 0.5625, 0.8125, 0.875, 0.8125, 0.875, 0.625, 0.5625, 0.8125, 0.5, 0.75, 0.5, 0.4375, 0.5, 0.5, 0.6875, 0.3125, 0.4375, 0.75, 0.6875, 0.8125, 0.0, 0.5625, 0.625, 0.75, 0.1875, 0.6875, 0.75, 0.625, 0.5, 0.75, 0.5, 0.625, 0.5, 0.75, 0.75, 0.75, 0.5, 0.4375, 0.625, 0.625, 0.4375, 0.875, 0.5625, 0.5, 0.1875, 0.6875, 0.5, 0.6875, 0.625, 0.625, 0.5, 0.5, 0.6875, 0.375, 0.5625, 0.5625, 0.625, 0.8125, 0.5625, 0.75, 0.5, 0.5, 0.875, 0.6875, 0.625, 0.625, 0.5, 0.4375, 0.625, 0.5, 0.4375, 0.4375, 0.75, 0.4375, 0.75, 0.5625, 0.4375, 0.375, 0.8125, 0.75, 0.8125, 0.4375, 0.5625, 0.75, 0.25, 0.5625, 0.6875, 0.4375, 0.25, 0.6875, 0.8125, 0.4375, 0.625, 0.625, 0.5, 0.6875, 0.375, 0.4375, 0.5625, 0.6875, 0.8125, 0.6875, 0.75, 0.75, 0.75, 0.625, 0.4375, 0.5, 0.5, 0.5, 0.75, 0.75, 0.625, 0.625, 0.1875, 0.625, 0.4375, 0.5, 0.375, 0.4375, 0.6875, 0.75, 0.875, 0.5, 0.6875, 0.4375, 0.4375, 0.6875, 0.5, 0.5625, 0.625, 0.6875, 0.6875, 0.8125, 0.1875, 0.5, 0.5, 0.4375, 0.625, 0.75, 0.5, 0.5, 0.6875, 0.75, 0.375, 0.5625, 0.5, 0.6875, 0.5, 0.5625, 0.4375, 0.1875, 0.5, 0.5625, 0.625, 0.6875, 0.875, 0.625, 0.5, 0.625, 0.6875, 0.8125, 0.1875, 0.5625, 0.5625, 0.5625, 0.5, 0.5625, 0.75, 0.75, 0.75, 0.4375, 0.125, 0.375, 0.6875, 0.6875, 0.8125, 0.625, 0.625, 0.4375, 0.5625, 0.8125, 0.5, 0.75, 0.6875, 0.5625, 0.5625, 0.4375, 0.625, 0.75, 0.6875, 0.5, 0.75, 0.6875, 0.375, 0.5625, 0.8125, 0.4375, 0.5, 0.4375, 0.6875, 0.4375, 0.5, 0.75, 0.4375, 0.6875, 0.625, 0.6875, 0.5, 0.625, 0.625, 0.6875, 0.5, 0.5625, 0.5625, 0.4375, 0.8125, 0.875, 0.375, 0.625, 0.6875, 0.625, 0.5, 0.6875, 0.5, 0.6875, 0.875, 0.3125, 0.625, 0.5625, 0.75, 0.6875, 0.625, 0.4375, 0.9375, 0.6875, 0.5625, 0.6875, 0.5, 0.75, 0.5, 0.625, 0.375, 0.8125, 0.4375, 0.5, 0.6875, 0.4375, 0.4375, 0.375, 0.5625, 0.5, 0.625, 0.75, 0.625, 0.25, 0.625, 0.4375, 0.4375, 0.875, 0.625, 0.5, 0.5, 0.6875, 0.5, 0.375, 0.4375, 0.625, 0.6875, 0.625, 0.5, 0.8125, 0.375, 0.625, 0.5, 0.75, 0.5625, 0.6875, 0.75, 0.75, 0.3125, 0.5625, 0.4375, 0.6875, 0.8125, 0.4375, 0.75, 0.6875, 0.5625, 0.5625, 0.625, 0.625, 0.6875, 0.625, 0.6875, 0.6875, 0.5, 0.6875, 0.5, 0.75, 0.625, 0.625, 0.625, 0.5625, 0.625, 0.375, 0.625, 0.6875, 0.75, 0.5, 0.5, 0.625, 0.8125, 0.8125, 0.75, 0.5625, 0.625, 0.625, 0.375, 0.6875, 0.75, 0.5, 0.5625, 0.75, 0.5, 0.5625, 0.6875, 0.625, 0.3125, 0.5625, 0.75, 0.5625, 0.6875, 0.8125, 0.4375, 0.75, 0.125, 0.8125, 0.75, 0.5625, 0.8125, 0.375, 0.6875, 0.5625, 0.5, 0.4375, 0.75, 0.375, 0.6875, 0.5625, 0.3125, 0.625, 0.8125, 0.5, 0.75, 0.5625, 0.75, 0.5625, 0.8125, 0.6875, 0.5625, 0.6875, 0.75, 0.4375, 0.5625, 0.625, 0.6875, 0.75, 0.8125, 0.8125, 0.5, 0.4375, 0.4375, 0.6875, 0.75, 0.6875, 0.875, 0.5625, 0.8125, 0.6875, 0.375, 0.6875, 0.8125, 0.4375, 0.75, 0.5, 0.5625, 0.6875, 0.5625, 0.5, 0.4375, 0.75, 0.625, 0.5, 0.5625, 0.5, 0.8125, 0.6875, 0.4375, 0.625, 0.8125, 0.1875, 0.75, 0.625, 0.5625, 0.5625, 0.6875, 0.75, 0.75, 0.75, 0.1875, 0.625, 0.625, 0.375, 0.5, 0.625, 0.75, 0.5, 0.4375, 0.5, 0.5, 0.6875, 0.75, 0.5, 0.8125, 0.5, 0.625, 0.75, 0.8125, 0.75, 0.625, 0.5, 0.375, 0.375, 0.75, 0.375, 0.5, 0.8125, 0.5625, 0.75, 0.5625, 0.5625, 0.625, 0.5, 0.4375, 0.6875, 0.8125, 0.6875, 0.75, 0.5625, 0.625, 0.5, 0.1875, 0.625, 0.75, 0.6875, 0.5625, 0.625, 0.5, 0.9375, 0.75, 0.75, 0.4375, 0.75, 0.5625, 0.4375, 0.4375, 0.5, 0.5, 0.5, 0.6875, 0.75, 0.5, 0.5, 0.75, 0.8125, 0.8125, 0.8125, 0.75, 0.75, 0.4375, 0.4375, 0.6875, 0.75, 0.6875, 0.5, 0.8125, 0.4375, 0.8125, 0.4375, 0.6875, 0.5625, 0.625, 0.5, 0.6875, 0.625, 0.6875, 0.625, 0.625, 0.5625, 0.625, 0.6875, 0.75, 0.5625, 0.125, 0.6875, 0.625, 0.75, 0.6875, 0.625, 0.8125, 0.75, 0.875, 0.5, 0.5, 0.9375, 0.4375, 0.8125, 0.25, 0.5, 0.625, 0.3125, 0.8125, 0.5, 0.6875, 0.5, 0.75, 0.4375, 0.75, 0.625, 0.8125, 0.3125, 0.75, 0.5625, 0.5625, 0.5625, 0.75, 0.4375, 0.6875, 0.5625, 0.5625, 0.8125, 0.5625, 0.1875, 0.625, 0.6875, 0.6875, 0.4375, 0.5625, 0.375, 0.0625, 0.5, 0.5625, 0.625, 0.4375, 0.5, 0.8125, 0.5, 0.75, 0.75, 0.625, 0.75, 0.625, 0.625, 0.375, 0.75, 0.625, 0.625, 0.5625, 0.5625, 0.8125, 0.5625, 0.4375, 0.5625, 0.75, 0.4375, 0.6875, 0.4375, 0.625, 0.8125, 0.625, 0.4375, 0.1875, 0.5625, 0.8125, 0.625, 0.75, 0.4375, 0.6875, 0.5625, 0.4375, 0.625, 0.4375, 0.4375, 0.4375, 0.6875, 0.6875, 0.75, 0.5, 0.625, 0.875, 0.5, 0.6875, 0.5625, 0.625, 0.5625, 0.4375, 0.6875, 0.125, 0.4375, 0.4375, 0.625, 0.625, 0.5625, 0.4375, 0.6875, 0.8125, 0.4375, 0.6875, 0.5, 0.75, 0.4375, 0.5, 0.75, 0.4375, 0.75, 0.6875, 0.6875, 0.75, 0.6875, 0.625, 0.6875, 0.8125, 0.5625, 0.8125, 0.625, 0.6875, 0.875, 0.5625, 0.75, 0.625, 0.625, 0.6875, 0.25, 0.625, 0.75, 0.4375, 0.5625, 0.625, 0.3125, 0.8125, 0.6875, 0.0, 0.5, 0.5625, 0.6875, 0.875, 0.6875, 0.75, 0.6875, 0.6875, 0.5, 0.625, 0.5, 0.5625, 0.875, 0.625, 0.6875, 0.4375, 0.5625, 0.8125, 0.6875, 0.75, 0.5, 0.5, 0.75, 0.6875, 0.75, 0.3125, 0.75, 0.5, 0.25, 0.5, 0.8125, 0.3125, 0.75, 1.0, 0.6875, 0.75, 0.5, 0.4375, 0.25, 0.6875, 0.625, 0.4375, 0.6875, 0.5625, 0.75, 0.5625, 0.4375, 0.375, 0.4375, 0.75, 0.5625, 0.1875, 0.625, 0.75, 0.75, 0.625, 0.875, 0.8125, 0.3125, 0.625, 0.0, 0.5625, 0.5, 0.625, 0.75, 0.625, 0.625, 0.75, 0.625, 0.5, 0.5, 0.75, 0.5625, 0.6875, 0.8125, 0.125, 0.75, 0.5, 0.5, 0.6875, 0.5625, 0.75, 0.75, 0.5, 0.375, 0.4375, 0.6875, 0.6875, 0.75, 0.5, 0.375, 0.4375, 0.4375, 0.8125, 0.5625, 0.625, 0.5625, 0.5625, 0.75, 0.625, 0.6875, 0.6875, 0.625, 0.6875, 0.3125, 0.625, 0.75, 0.75, 0.5, 0.8125, 0.5, 0.75, 0.5, 0.6875, 0.875, 0.625, 0.5625, 0.6875, 0.8125, 0.75, 0.75, 0.5625, 0.375, 0.5625, 0.5, 0.6875, 0.8125, 0.5, 0.5, 0.75, 0.4375, 0.4375, 0.75, 0.8125, 0.5, 0.8125, 0.75, 0.6875, 0.5, 0.375, 0.5625, 0.6875, 0.625, 0.4375, 0.6875, 0.625, 0.375, 0.875, 0.5625, 0.625, 0.5, 0.5625, 0.5625, 0.75, 0.6875, 0.6875, 0.125, 0.625, 0.1875, 0.375, 0.3125, 0.4375, 0.375, 0.5, 0.75, 0.625, 0.625, 0.875, 0.625, 0.4375, 0.5625, 0.0, 0.75, 0.4375, 0.5625, 0.75, 0.5625, 0.5625, 0.5, 0.6875, 0.5625, 0.5625, 0.9375, 0.5625, 0.5625, 0.6875, 0.5, 0.75, 0.5625, 0.625, 0.625, 0.4375, 0.6875, 0.6875, 0.5625, 0.75, 0.8125, 0.75, 0.4375, 0.75, 0.3125, 0.4375, 0.5625, 0.5625, 0.75, 0.625, 0.75, 0.5625, 0.5, 0.8125, 0.625, 0.625, 0.6875, 0.4375, 0.0625, 0.75, 0.625, 0.125, 0.8125, 0.8125, 0.5, 0.6875, 0.6875, 0.5, 0.8125, 0.625, 0.6875, 0.75, 0.5, 0.5625, 0.6875, 0.5, 0.5, 0.4375, 0.25, 0.5625, 0.75, 0.4375, 0.5, 0.75, 0.5625, 0.4375, 0.8125, 0.5, 0.75, 0.75, 0.5, 0.625, 0.75, 0.4375, 0.625, 0.375, 0.5625, 0.5625, 0.4375, 0.25, 0.6875, 0.5625, 0.75, 0.4375, 0.625, 0.8125, 0.6875, 0.6875, 0.75, 0.8125, 0.4375, 0.5625, 0.6875, 0.5625, 0.6875, 0.75, 0.5625, 0.5, 0.6875, 0.625, 0.5, 0.4375, 0.5625, 0.75, 0.75, 0.8125, 0.625, 0.5, 0.75, 0.8125, 0.3125, 0.8125, 0.4375, 0.75, 0.5, 0.625, 0.8125, 0.6875, 0.75, 0.625, 0.625, 0.6875, 0.75, 0.75, 0.8125, 0.5, 0.75, 0.5625, 0.3125, 0.3125, 0.75, 0.4375, 0.6875, 0.6875, 0.8125, 0.5, 0.5625, 0.75, 0.6875, 0.5, 0.75, 0.6875, 0.4375, 0.5625, 0.375, 0.75, 0.375, 0.8125, 0.4375, 0.875, 0.75, 0.625, 0.4375, 0.6875, 0.4375, 0.4375, 0.3125, 0.4375, 0.625, 0.8125, 0.5625, 0.6875, 0.75, 0.6875, 0.75, 0.5625, 0.5, 0.75, 0.625, 0.4375, 0.75, 0.5, 0.625, 0.8125, 0.625, 0.625, 0.75, 0.5, 0.4375, 0.75, 0.75, 0.375, 0.625, 0.6875, 0.6875, 0.8125, 0.5625, 0.75, 0.6875, 0.75, 0.5625, 0.75, 0.5625, 0.375, 0.5, 0.6875, 0.5625, 0.8125, 0.5625, 0.5, 0.875, 0.4375, 0.6875, 0.4375, 0.375, 0.5625, 0.875, 0.5, 0.3125, 0.5, 0.5625, 0.4375, 0.625, 0.75, 0.875, 0.75, 0.8125, 0.6875, 0.5625, 0.75, 0.6875, 0.6875, 0.5, 0.75, 0.6875, 0.4375, 0.75, 0.6875, 0.3125, 0.375, 0.125, 0.6875, 0.3125, 0.875, 0.1875, 0.5, 0.4375, 0.5625, 0.375, 0.375, 0.5625, 0.8125, 0.4375, 0.8125, 0.75, 0.1875, 0.625, 0.625, 0.75, 0.5, 0.75, 0.25, 0.6875, 0.625, 0.75, 0.6875, 0.625, 0.5, 0.75, 0.5625, 0.75, 0.5625, 0.625, 0.625, 0.5, 0.75, 0.625, 0.6875, 0.75, 0.75, 0.6875, 0.6875, 0.4375, 0.6875, 0.4375, 0.75, 0.4375, 0.4375, 0.5, 0.5, 0.5625, 0.8125, 0.8125, 0.625, 0.5, 0.75, 0.5, 0.6875, 0.625, 0.4375, 0.625, 0.5, 0.4375, 0.4375, 0.625, 0.625, 0.8125, 0.5625, 0.5625, 0.625, 0.4375, 0.8125, 0.8125, 0.8125, 0.625, 0.75, 0.4375, 0.8125, 0.75, 0.5, 0.5, 0.75, 0.625, 0.5625, 0.75, 0.8125, 0.625, 0.625, 0.5, 0.8125, 0.0, 0.5, 0.5, 0.75, 0.5, 0.5625, 0.6875, 0.75, 0.625, 0.625, 0.8125, 0.5, 0.75, 0.75, 0.4375, 0.4375, 0.75, 0.25, 0.375, 0.5, 0.75, 0.5, 0.1875, 0.8125, 0.625, 0.6875, 0.8125, 0.75, 0.9375, 0.5, 0.8125, 0.5, 0.6875, 0.6875, 0.1875, 0.375, 0.75, 0.5, 0.5625, 0.75, 0.6875, 0.75, 0.875, 0.4375, 0.8125, 0.75, 0.5, 0.5625, 0.5, 0.6875, 0.6875, 0.625, 0.6875, 0.625, 0.5625, 0.625, 0.4375, 0.625, 0.5, 0.5, 0.25, 0.75, 0.875, 0.75, 0.75, 0.5625, 0.4375, 0.8125, 0.6875, 0.5, 0.75, 0.25, 0.9375, 0.5, 0.4375, 0.5, 0.4375, 0.6875, 0.6875, 0.5, 0.6875, 0.5, 0.75, 0.75, 0.6875, 0.5625, 0.75, 0.5625, 0.6875, 0.6875, 0.6875, 0.8125, 0.75, 0.4375, 0.6875, 0.5625, 0.5625, 0.6875, 0.5625, 0.75, 0.5625, 0.75, 0.5, 0.8125, 0.6875, 0.4375, 0.5, 0.5, 0.4375, 0.6875, 0.5, 0.4375, 0.8125, 0.125, 0.8125, 0.75, 0.4375, 0.4375, 0.375, 0.1875, 0.75, 0.8125, 0.6875, 0.5625, 0.75, 0.5, 0.6875, 0.5625, 0.0625, 0.5625, 0.75, 0.8125, 0.5625, 0.75, 0.6875, 0.6875, 0.5, 0.625, 0.8125, 0.625, 0.5, 0.5, 0.75, 0.4375, 0.6875, 0.6875, 0.125, 0.875, 0.5, 0.5, 0.75, 0.5625, 0.625, 0.4375, 0.6875, 0.8125, 0.125, 0.75, 0.75, 0.75, 0.4375, 0.8125, 0.4375, 0.625, 0.125, 0.6875, 0.375, 0.6875, 0.4375, 0.6875, 0.5, 0.6875, 0.5625, 0.6875, 0.5625, 0.5625, 0.75, 0.5, 0.8125, 0.5, 0.5, 0.4375, 0.25, 0.5, 0.5, 0.75, 0.5625, 0.75, 0.75, 0.6875, 0.75, 0.5, 0.4375, 0.625, 0.4375, 0.4375, 0.75, 0.5625, 0.25, 0.375, 0.6875, 0.75, 0.8125, 0.625, 0.6875, 0.5, 0.625, 0.625, 0.5625, 0.5625, 0.3125, 0.875, 0.8125, 0.75, 0.6875, 0.75, 0.8125, 0.75, 0.5, 0.5, 0.8125, 0.5625, 0.5, 0.4375, 0.75, 0.6875, 0.4375, 0.5, 0.625, 0.3125, 0.625, 0.75, 0.75, 0.6875, 0.625, 0.5, 0.625, 0.6875, 0.3125, 0.5, 0.6875, 0.5, 0.4375, 0.5625, 0.6875, 0.4375, 0.5, 0.625, 0.6875, 0.5625, 0.5, 0.4375, 0.625, 0.6875, 0.4375, 0.75, 0.375, 0.625, 0.5625, 0.75, 0.5625, 0.25, 0.5625, 0.6875, 0.5625, 0.625, 0.625, 0.625, 0.75, 0.5, 0.625, 0.5, 0.625, 0.5, 0.6875, 0.75, 0.4375, 0.5625, 0.6875, 0.5625, 0.5, 0.8125, 0.9375, 0.75, 0.4375, 0.5, 0.5, 0.75, 0.6875, 0.5625, 0.6875, 0.625, 0.75, 0.6875, 0.8125, 0.6875, 0.6875, 0.5625, 0.4375, 0.4375, 0.125, 0.75, 0.625, 0.625, 0.5, 0.4375, 0.875, 0.625, 0.5, 0.5625, 0.625, 0.625, 0.4375, 0.5, 0.625, 0.5625, 0.5, 0.375, 0.625, 0.6875, 0.625, 0.625, 0.5, 0.5, 0.5625, 0.75, 0.625, 0.625, 0.5625, 0.75, 0.75, 0.8125, 0.6875, 0.5, 0.25, 0.625, 0.75, 0.75, 0.75, 0.75, 0.75, 0.5625, 0.5625, 0.8125, 0.8125, 0.5625, 0.5, 0.5625, 0.25, 0.5, 0.75, 0.75, 0.5, 0.0625, 0.5, 0.375, 0.5625, 0.5, 0.75, 0.6875, 0.75, 0.5625, 0.625, 0.5, 0.75, 0.5, 0.6875, 0.875, 0.6875, 0.875, 0.6875, 0.5, 0.25, 0.6875, 0.4375, 0.8125, 0.375, 0.4375, 0.8125, 0.75, 0.4375, 0.625, 0.5, 0.3125, 0.75, 0.6875, 0.6875, 0.75, 0.75, 0.8125, 0.75, 0.5, 0.4375, 0.6875, 0.625, 0.5, 0.625, 0.75, 0.75, 0.5625, 0.75, 0.625, 0.4375, 0.625, 0.5, 0.6875, 0.4375, 0.75, 0.5, 0.5625, 0.4375, 0.625, 0.5, 0.625, 0.75, 0.75, 0.5625, 0.5625, 0.5625, 0.75, 0.5625, 0.625, 0.6875, 0.5625, 0.5625, 0.625, 0.4375, 0.8125, 0.5, 0.5, 0.4375, 0.75, 0.6875, 0.5, 0.5, 0.5625, 0.625, 0.625, 0.625, 0.6875, 0.5, 0.4375, 0.5625, 0.6875, 0.75, 0.6875, 0.75, 0.5, 0.625, 0.5, 0.6875, 0.625, 0.875, 0.4375, 0.6875, 0.75, 0.75, 0.4375, 0.6875, 0.5, 0.5, 0.75, 0.6875, 0.75, 0.75, 0.6875, 0.625, 0.6875, 0.5625, 0.5625, 0.5, 0.75, 0.8125, 0.8125, 0.75, 0.5, 0.5625, 0.875, 0.6875, 0.5, 0.75, 0.6875, 0.4375, 0.6875, 0.75, 0.5625, 0.6875, 0.4375, 0.625, 0.75, 0.5625, 0.6875, 0.8125, 0.6875, 0.625, 0.75, 0.625, 0.5, 0.8125, 0.5, 0.75, 0.5, 0.5625, 0.5625, 0.625, 0.625, 0.6875, 0.6875, 0.8125, 0.5, 0.625, 0.625, 0.5625, 0.6875, 0.5625, 0.75, 0.1875, 0.625, 0.625, 0.4375, 0.6875, 0.5625, 0.6875, 0.375, 0.5, 0.625, 0.625, 0.75, 0.5625, 0.5625, 0.5625, 0.4375, 0.375, 0.6875, 0.75, 0.75, 0.5625, 0.0, 0.5625, 0.625, 0.5, 0.3125, 0.375, 0.75, 0.4375, 0.6875, 0.75, 0.5, 0.625, 0.4375, 0.0625, 0.625, 0.6875, 0.75, 0.625, 0.6875, 0.5, 0.6875, 0.5, 0.75, 0.8125, 0.5, 0.75, 0.625, 0.5, 0.6875, 0.6875, 0.5, 0.875, 0.8125, 0.4375, 0.4375, 0.4375, 0.75, 0.4375, 0.125, 0.6875, 0.625, 0.6875, 0.8125, 0.75, 0.625, 0.6875, 0.625, 0.4375, 0.5, 0.5, 0.5625, 0.625, 0.5, 0.625, 0.875, 0.625, 0.625, 0.5625, 0.6875, 0.8125, 0.9375, 0.5625, 0.625, 0.4375, 0.6875, 0.8125, 0.625, 0.4375, 0.4375, 0.6875, 0.625, 0.625, 0.75, 0.5, 0.5, 0.75, 0.5, 0.5, 0.5625, 0.6875, 0.625, 0.8125, 0.625, 0.6875, 0.3125, 0.5, 0.5625, 0.8125, 0.5625, 0.625, 0.625, 0.625, 0.5, 0.5625, 0.375, 0.8125, 0.5, 0.625, 0.625, 0.75, 0.625, 0.75, 0.75, 0.6875, 0.75, 0.5625, 0.625, 0.625, 0.75, 0.8125, 0.75, 0.1875, 0.75, 0.4375, 0.6875, 0.5, 0.5625, 0.3125, 0.6875, 0.6875, 0.5625, 0.5, 0.625, 0.625, 0.8125, 0.625, 1.0, 0.4375, 0.5625, 0.75, 0.8125, 0.375, 0.5625, 0.625, 0.5625, 0.875, 0.5, 0.625, 0.5, 0.6875, 0.6875, 0.4375, 0.625, 0.6875, 0.875, 0.625, 0.5, 0.75, 0.5, 0.6875, 0.5, 0.4375, 0.75, 0.5625, 0.5625, 0.625, 0.6875, 0.625, 0.625, 0.625, 0.75, 0.4375, 0.5625, 0.6875, 0.5, 0.625, 0.5, 0.4375, 0.3125, 0.5, 0.5, 0.5, 0.4375, 0.8125, 0.5625, 0.75, 0.75, 0.8125, 0.625, 0.5, 0.8125, 0.5625, 0.625, 0.6875, 0.8125, 0.625, 0.25, 0.75, 0.8125, 0.4375, 0.4375, 0.5625, 0.5, 0.75, 0.75, 0.5, 0.4375, 0.5, 0.75, 0.375, 0.4375, 0.8125, 0.5625, 0.6875, 0.5, 0.625, 0.4375, 0.5, 0.8125, 0.75, 0.5, 0.375, 0.5625, 0.4375, 0.5625, 0.625, 0.0, 0.6875, 0.75, 0.5625, 0.5, 0.4375, 0.75, 0.3125, 0.25, 0.75, 0.375, 0.5625, 0.6875, 0.875, 0.5, 0.4375, 0.3125, 0.5625, 0.5625, 0.4375, 0.5625, 0.6875, 0.5, 0.5, 0.625, 0.75, 0.6875, 0.5, 0.8125, 0.75, 0.6875, 0.625, 0.4375, 0.8125, 0.75, 0.4375, 0.5, 0.5, 0.5, 0.4375, 0.9375, 0.5, 0.75, 0.5625, 0.5, 0.6875, 0.5625, 0.5, 0.8125, 0.75, 0.5625, 0.75, 0.375, 0.5, 0.3125, 0.6875, 0.8125, 0.8125, 0.4375, 0.5, 0.75, 0.4375, 0.75, 0.5625, 0.0, 0.5, 0.625, 0.6875, 0.625, 0.75, 0.5, 0.5, 0.125, 0.75, 0.75, 0.6875, 0.0, 0.75, 0.5, 0.5, 0.8125, 0.5, 0.375, 0.4375, 0.4375, 0.6875, 0.5, 0.5625, 0.5, 0.5, 0.4375, 0.6875, 0.6875, 0.5, 0.625, 0.5625, 0.625, 0.5625, 0.5625, 0.4375, 0.25, 0.6875, 0.5, 0.5, 0.8125, 0.4375, 0.4375, 0.0, 0.3125, 0.6875, 0.8125, 0.5, 0.1875, 0.8125, 0.8125, 0.6875, 0.5625, 0.4375, 0.5, 0.5, 0.6875, 0.625, 0.75, 0.4375, 0.75, 0.4375, 0.75, 0.6875, 0.75, 0.75, 0.1875, 0.5625, 0.75, 0.6875, 0.625, 0.6875, 0.625, 0.5, 0.4375, 0.4375, 0.5, 0.625, 0.5, 0.625, 0.4375, 0.75, 0.6875, 0.875, 0.875, 0.75, 0.4375, 0.5, 0.5625, 0.6875, 0.9375, 0.6875, 0.4375, 0.5, 0.5, 0.625, 0.75, 0.75, 0.5, 0.75, 0.5, 0.5, 0.5625, 0.8125, 0.4375, 0.25, 0.375, 0.8125, 0.5, 0.5, 0.6875, 0.5, 0.6875, 0.625, 0.75, 0.4375, 0.75, 0.9375, 0.5625, 0.4375, 0.75, 0.75, 0.625, 0.5625, 0.6875, 0.5, 0.4375, 0.5, 0.5625, 0.6875, 0.5, 0.875, 0.6875, 0.5, 0.6875, 0.6875, 0.625, 0.5625, 0.8125, 0.5, 0.5625, 0.375, 0.6875, 0.5625, 0.5, 0.6875, 0.625, 0.75, 0.5625, 0.5, 0.5625, 0.75, 0.6875, 0.8125, 0.5, 0.75, 0.75, 0.5, 0.625, 0.625, 0.75, 0.4375, 0.6875, 0.8125, 0.6875, 0.875, 0.4375, 0.8125, 0.0, 0.0, 0.6875, 0.4375, 0.5, 0.625, 0.8125, 0.75, 0.5, 0.4375, 0.5, 0.6875, 0.5625, 0.75, 0.4375, 0.375, 0.375, 0.625, 0.75, 0.4375, 0.5625, 0.625, 0.75, 0.75, 0.375, 0.4375, 0.6875, 0.4375, 0.625, 0.4375, 0.75, 0.8125, 0.5, 0.4375, 0.5, 0.6875, 0.125, 0.6875, 0.625, 0.75, 0.3125, 0.625, 0.5625, 0.5625, 0.6875, 0.375, 0.5625, 0.75, 0.5, 0.5625, 0.75, 0.4375, 0.4375, 0.8125, 0.8125, 0.8125, 0.625, 0.8125, 0.625, 0.625, 0.6875, 0.5625, 0.4375, 0.75, 0.5, 0.4375, 0.4375, 0.5, 0.8125, 0.5, 0.6875, 0.4375, 0.6875, 0.5, 0.4375, 0.75, 0.4375, 0.3125, 0.5625, 0.0, 0.5, 0.625, 0.8125, 0.4375, 0.4375, 0.75, 0.75, 0.75, 0.6875, 0.5, 0.625, 0.5, 0.5, 0.75, 0.75, 0.5, 0.4375, 0.6875, 0.5625, 0.625, 0.4375, 0.8125, 0.6875, 0.6875, 0.75, 0.875, 0.5, 0.25, 0.5, 0.5, 0.625, 0.5625, 0.8125, 0.5625, 0.6875, 0.5625, 0.5, 0.6875, 0.6875, 0.5, 0.5625, 0.5625, 0.5625, 0.5625, 0.375, 0.625, 0.5625, 0.75, 0.5, 0.75, 0.625, 0.6875, 0.8125, 0.4375, 0.6875, 0.8125, 0.4375, 0.5, 0.75, 0.5, 0.6875, 0.625, 0.625, 0.75, 0.4375, 0.5, 0.6875, 0.4375, 0.625, 0.625, 0.8125, 0.4375, 0.6875, 0.6875, 0.5, 0.625, 0.75, 0.4375, 0.6875, 0.5, 0.5625, 0.25, 0.75, 0.5, 0.6875, 0.625, 0.6875, 0.4375, 0.5, 0.875, 0.6875, 0.75, 0.75, 0.6875, 0.625, 0.5, 0.5625, 0.625, 0.8125, 0.375, 0.4375, 0.75, 0.6875, 0.6875, 0.8125, 0.0625, 0.875, 0.6875, 0.75, 0.625, 0.625, 0.75, 0.5, 0.5625, 0.875, 0.5, 0.625, 0.5, 0.3125, 0.625, 0.4375, 0.3125, 0.8125, 0.5625, 0.75, 0.75, 0.8125, 0.6875, 0.5625, 0.6875, 0.4375, 0.8125, 0.5625, 0.5, 0.5625, 0.5625, 0.8125, 0.5625, 0.875, 0.5, 0.75, 0.6875, 0.625, 0.8125, 0.75, 0.75, 0.5625, 0.5, 0.5625, 0.5625, 0.75, 0.6875, 0.8125, 0.4375, 0.5625, 0.5625, 0.5, 0.4375, 0.5625, 0.8125, 0.8125, 0.5, 0.625, 0.75, 0.75, 0.6875, 0.625, 0.6875, 0.0625, 0.625, 0.75, 0.5, 0.5, 0.625, 0.8125, 0.8125, 0.75, 0.625, 0.5, 0.6875, 0.625, 0.4375, 0.4375, 0.625, 0.8125, 0.6875, 0.6875, 0.5, 0.3125, 1.0, 0.6875, 0.4375, 0.5625, 0.625, 0.5, 0.8125, 0.375, 0.75, 0.5, 0.6875, 0.625, 0.4375, 0.5, 0.6875, 0.6875, 0.875, 0.6875, 0.6875, 0.4375, 0.6875, 0.5625, 0.625, 0.3125, 0.8125, 0.75, 0.4375, 0.8125, 0.375, 0.5625, 0.625, 0.6875, 0.5, 0.4375, 0.75, 0.6875, 0.6875, 0.75, 0.1875, 0.5625, 0.4375, 0.4375, 0.625, 0.6875, 0.5625, 0.5625, 0.625, 0.6875, 0.0, 0.375, 0.6875, 0.5, 0.4375, 0.375, 0.4375, 0.5, 0.5625, 0.25, 0.75, 0.4375, 0.25, 0.5, 0.6875, 0.625, 0.4375, 0.4375, 0.5, 0.5, 0.75, 0.6875, 0.75, 0.8125, 0.6875, 0.8125, 0.3125, 0.5, 0.6875, 0.5, 0.4375, 0.625, 0.8125, 0.625, 0.75, 0.4375, 0.8125, 0.5625, 0.6875, 0.6875, 0.625, 0.4375, 0.6875, 0.75, 0.75, 0.625, 0.4375, 0.375, 0.8125, 0.75, 0.3125, 0.75, 0.625, 0.5, 0.6875, 0.75, 0.75, 0.625, 0.625, 0.6875, 0.5, 0.4375, 0.6875, 0.75, 0.4375, 0.75, 0.5, 0.6875, 0.6875, 0.5625, 0.6875, 0.6875, 0.4375, 0.75, 0.4375, 0.625, 0.625, 0.25, 0.875, 0.75, 0.8125, 0.5625, 0.5, 0.5, 0.5, 0.6875, 0.625, 0.75, 0.5, 0.625, 0.5625, 0.5, 0.5625, 0.5, 0.75, 0.8125, 0.625, 0.5, 0.5, 0.6875, 0.8125, 0.5, 0.6875, 0.4375, 0.6875, 0.5, 0.8125, 0.5, 0.5625, 0.8125, 0.625, 0.625, 0.5625, 0.5, 0.8125, 0.375, 0.75, 0.5625, 0.75, 0.25, 0.5, 0.3125, 0.6875, 0.4375, 0.5, 0.4375, 0.5, 0.5, 0.5625, 0.5, 0.6875, 0.5625, 0.625, 0.5, 0.8125, 0.8125, 0.5625, 0.5625, 0.5625, 0.625, 0.5, 0.1875, 0.4375, 0.8125, 0.8125, 0.5, 0.5, 0.625, 0.5, 0.5, 0.6875, 0.5, 0.4375, 0.6875, 0.5, 0.5625, 0.6875, 0.5, 0.75, 0.625, 0.875, 0.625, 0.75, 0.5, 0.75, 0.4375, 0.625, 0.5, 0.6875, 0.875, 0.5625, 0.6875, 0.5, 0.75, 0.6875, 0.5, 0.8125, 0.25, 0.6875, 0.75, 0.4375, 0.75, 0.5625, 0.6875, 0.75, 0.75, 0.5625, 0.625, 0.625, 0.5, 0.75, 0.6875, 0.8125, 0.75, 0.75, 0.5625, 0.6875, 0.375, 0.75, 0.75, 0.5, 0.5625, 0.8125, 0.6875, 0.75, 0.625, 0.125, 0.8125, 0.5625, 0.875, 0.8125, 0.625, 0.4375, 0.625, 0.5625, 0.4375, 0.625, 0.4375, 0.625, 0.75, 0.8125, 0.4375, 0.625, 0.875, 0.875, 0.75, 0.5625, 0.875, 0.6875, 0.4375, 0.875, 0.4375, 0.5625, 0.8125, 0.75, 0.625, 0.5625, 0.375, 0.75, 0.5625, 0.75, 0.8125, 0.625, 0.5, 0.4375]\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.42857142857143, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train, x_valid, y_train, y_valid = train_test_split(img_path, Y, test_size=0.30)\n",
        "# print(len(x_train),len(x_valid))\n",
        "# print(len(y_train),len(y_valid))"
      ],
      "metadata": {
        "id": "b7KU67sh1FVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train_angle[:10])\n",
        "print(y_valid_angle[:10])"
      ],
      "metadata": {
        "id": "Kq-MwttsziCP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5266e79-8ec8-4363-ac52-be9ff67357da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.75, 0.6875, 0.1875, 0.5625, 0.75, 0.6875, 0.4375, 0.5, 0.8125, 0.5]\n",
            "[0.5, 0.6875, 0.4375, 0.5, 0.75, 0.6875, 0.6875, 0.1875, 0.6875, 0.75]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image(path):\n",
        "    '''Read the image's path and return the image'''\n",
        "    image = tf.io.read_file(path)\n",
        "    image = tf.image.decode_png(image, channels=3)\n",
        "    image = image/255\n",
        "    #tf.reshape(image,(224,224,3))\n",
        "    return image"
      ],
      "metadata": {
        "id": "cTR4I8islJZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Data Generator"
      ],
      "metadata": {
        "id": "YWcgWwH3uUx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "IDEAs \n",
        "    1 - Get the paths of all the images if the image is not corrupt then generate its label and then make pd dataframe with - content , angle , speed\n",
        "    2 - Send the pd data frame to image data generator - get the image from the first column and second column will have the labels\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "vi9Rk1FHuYso",
        "outputId": "426e382b-ddcd-4c1a-ba2a-27e298fe275e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIDEAs \\n    1 - Get the paths of all the images if the image is not corrupt then generate its label and then make pd dataframe with - content , angle , speed\\n    2 - Send the pd data frame to image data generator - get the image from the first column and second column will have the labels\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def image_data_generator(image_paths, steering_angles, batch_size):\n",
        "    while True:\n",
        "        batch_images = []\n",
        "        batch_steering_angles = []\n",
        "        for i in range(batch_size):\n",
        "            random_index = random.randint(0, len(image_paths) - 1)\n",
        "            image_path = image_paths[random_index]\n",
        "            steering_angle = steering_angles[random_index]\n",
        "            image = process_image(image_path)\n",
        "            batch_images.append(image)\n",
        "            batch_steering_angles.append(steering_angle)\n",
        "        yield( np.asarray(batch_images), np.asarray(batch_steering_angles))"
      ],
      "metadata": {
        "id": "6xYucWW4k5J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer Learning Code"
      ],
      "metadata": {
        "id": "ShWBLx1klxkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transfer Learning(Angel)"
      ],
      "metadata": {
        "id": "jyhZ2vLXfY70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Angle model\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "incept_angle= InceptionV3(input_shape = (224,224,3),weights='imagenet',include_top=False)\n",
        "incept_angle.trainable = False"
      ],
      "metadata": {
        "id": "PCcLXywPlr6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, GlobalAveragePooling2D \n",
        "\n",
        "for layer in incept_angle.layers[290:]: \n",
        "    layer.trainable = True \n",
        "x = GlobalAveragePooling2D()(incept_angle.output)\n",
        "x = Flatten()(x)\n",
        "x = Dense(1024, activation ='relu')(x)#,kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(x)\n",
        "# let's add a fully-connected layer as first layer\n",
        "x = Dense(256, activation ='relu')(x)#,kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(x)\n",
        "x = Dense(64, activation ='relu')(x)#,kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(x)\n",
        "x = Dense(32, activation ='relu')(x)#,kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(x)\n",
        "# x = Dense( 32, activation ='relu')(x)\n",
        "pred_angle = Dense(1)(x)"
      ],
      "metadata": {
        "id": "ScMXDYwpm29e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "my_inc_angle = Model(inputs = incept_angle.input,outputs=pred_angle)\n",
        "my_inc_angle.summary()\n",
        "my_inc_angle.compile(optimizer = 'adam',loss='mse',metrics=['mse'])\n",
        "my_inc_angle.optimizer.learning_rate = 0.000005"
      ],
      "metadata": {
        "id": "nFEiNeGkoyTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a011e5d4-5348-4223-801d-44f5f0d007b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 111, 111, 32  864         ['input_1[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 111, 111, 32  96         ['conv2d[0][0]']                 \n",
            " alization)                     )                                                                 \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 111, 111, 32  0           ['batch_normalization[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 109, 109, 32  9216        ['activation[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 109, 109, 32  96         ['conv2d_1[0][0]']               \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 109, 109, 32  0           ['batch_normalization_1[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 109, 109, 64  18432       ['activation_1[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 109, 109, 64  192        ['conv2d_2[0][0]']               \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 109, 109, 64  0           ['batch_normalization_2[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 54, 54, 64)   0           ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 54, 54, 80)   5120        ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 54, 54, 80)  240         ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 54, 54, 80)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 52, 52, 192)  138240      ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 52, 52, 192)  576        ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 52, 52, 192)  0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 25, 25, 192)  0          ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 25, 25, 64)   12288       ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 25, 25, 64)  192         ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 25, 25, 64)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 25, 25, 48)   9216        ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 25, 25, 96)   55296       ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 25, 25, 48)  144         ['conv2d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 25, 25, 96)  288         ['conv2d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 25, 25, 48)   0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 25, 25, 96)   0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " average_pooling2d (AveragePool  (None, 25, 25, 192)  0          ['max_pooling2d_1[0][0]']        \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 25, 25, 64)   12288       ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 25, 25, 64)   76800       ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 25, 25, 96)   82944       ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 25, 25, 32)   6144        ['average_pooling2d[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 25, 25, 64)  192         ['conv2d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 25, 25, 64)  192         ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 25, 25, 96)  288         ['conv2d_10[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 25, 25, 32)  96          ['conv2d_11[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 25, 25, 64)   0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 25, 25, 64)   0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 25, 25, 96)   0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 25, 25, 32)   0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " mixed0 (Concatenate)           (None, 25, 25, 256)  0           ['activation_5[0][0]',           \n",
            "                                                                  'activation_7[0][0]',           \n",
            "                                                                  'activation_10[0][0]',          \n",
            "                                                                  'activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 25, 25, 64)   16384       ['mixed0[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 25, 25, 64)  192         ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 25, 25, 48)   12288       ['mixed0[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 25, 25, 96)   55296       ['activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 25, 25, 48)  144         ['conv2d_13[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 25, 25, 96)  288         ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 25, 25, 48)   0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 25, 25, 96)   0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d_1 (AveragePo  (None, 25, 25, 256)  0          ['mixed0[0][0]']                 \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 25, 25, 64)   16384       ['mixed0[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 25, 25, 64)   76800       ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 25, 25, 96)   82944       ['activation_16[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 25, 25, 64)   16384       ['average_pooling2d_1[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 25, 25, 64)  192         ['conv2d_12[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 25, 25, 64)  192         ['conv2d_14[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 25, 25, 96)  288         ['conv2d_17[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 25, 25, 64)  192         ['conv2d_18[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 25, 25, 96)   0           ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " mixed1 (Concatenate)           (None, 25, 25, 288)  0           ['activation_12[0][0]',          \n",
            "                                                                  'activation_14[0][0]',          \n",
            "                                                                  'activation_17[0][0]',          \n",
            "                                                                  'activation_18[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 25, 25, 64)   18432       ['mixed1[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 25, 25, 64)  192         ['conv2d_22[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 25, 25, 48)   13824       ['mixed1[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 25, 25, 96)   55296       ['activation_22[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 25, 25, 48)  144         ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 25, 25, 96)  288         ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, 25, 25, 48)   0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, 25, 25, 96)   0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d_2 (AveragePo  (None, 25, 25, 288)  0          ['mixed1[0][0]']                 \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 25, 25, 64)   18432       ['mixed1[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 25, 25, 64)   76800       ['activation_20[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 25, 25, 96)   82944       ['activation_23[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 25, 25, 64)   18432       ['average_pooling2d_2[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 25, 25, 64)  192         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 25, 25, 64)  192         ['conv2d_21[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 25, 25, 96)  288         ['conv2d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 25, 25, 64)  192         ['conv2d_25[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_19 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " activation_21 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, 25, 25, 96)   0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " activation_25 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " mixed2 (Concatenate)           (None, 25, 25, 288)  0           ['activation_19[0][0]',          \n",
            "                                                                  'activation_21[0][0]',          \n",
            "                                                                  'activation_24[0][0]',          \n",
            "                                                                  'activation_25[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 25, 25, 64)   18432       ['mixed2[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 25, 25, 64)  192         ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_27 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 25, 25, 96)   55296       ['activation_27[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 25, 25, 96)  288         ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_28 (Activation)     (None, 25, 25, 96)   0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 12, 12, 384)  995328      ['mixed2[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 12, 12, 96)   82944       ['activation_28[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 12, 12, 384)  1152       ['conv2d_26[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 12, 12, 96)  288         ['conv2d_29[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_26 (Activation)     (None, 12, 12, 384)  0           ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " activation_29 (Activation)     (None, 12, 12, 96)   0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 12, 12, 288)  0          ['mixed2[0][0]']                 \n",
            "                                                                                                  \n",
            " mixed3 (Concatenate)           (None, 12, 12, 768)  0           ['activation_26[0][0]',          \n",
            "                                                                  'activation_29[0][0]',          \n",
            "                                                                  'max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 12, 12, 128)  98304       ['mixed3[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 12, 12, 128)  384        ['conv2d_34[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_34 (Activation)     (None, 12, 12, 128)  0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 12, 12, 128)  114688      ['activation_34[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 12, 12, 128)  384        ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_35 (Activation)     (None, 12, 12, 128)  0           ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 12, 12, 128)  98304       ['mixed3[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 12, 12, 128)  114688      ['activation_35[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 12, 12, 128)  384        ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 12, 12, 128)  384        ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_31 (Activation)     (None, 12, 12, 128)  0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " activation_36 (Activation)     (None, 12, 12, 128)  0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 12, 12, 128)  114688      ['activation_31[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 12, 12, 128)  114688      ['activation_36[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 12, 12, 128)  384        ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 12, 12, 128)  384        ['conv2d_37[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_32 (Activation)     (None, 12, 12, 128)  0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " activation_37 (Activation)     (None, 12, 12, 128)  0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d_3 (AveragePo  (None, 12, 12, 768)  0          ['mixed3[0][0]']                 \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 12, 12, 192)  147456      ['mixed3[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 12, 12, 192)  172032      ['activation_32[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 12, 12, 192)  172032      ['activation_37[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 12, 12, 192)  147456      ['average_pooling2d_3[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 12, 12, 192)  576        ['conv2d_30[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 12, 12, 192)  576        ['conv2d_33[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 12, 12, 192)  576        ['conv2d_38[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 12, 12, 192)  576        ['conv2d_39[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_30 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " activation_33 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " activation_38 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " activation_39 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " mixed4 (Concatenate)           (None, 12, 12, 768)  0           ['activation_30[0][0]',          \n",
            "                                                                  'activation_33[0][0]',          \n",
            "                                                                  'activation_38[0][0]',          \n",
            "                                                                  'activation_39[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 12, 12, 160)  122880      ['mixed4[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 12, 12, 160)  480        ['conv2d_44[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_44 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 12, 12, 160)  179200      ['activation_44[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 12, 12, 160)  480        ['conv2d_45[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_45 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 12, 12, 160)  122880      ['mixed4[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 12, 12, 160)  179200      ['activation_45[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 12, 12, 160)  480        ['conv2d_41[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 12, 12, 160)  480        ['conv2d_46[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_41 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " activation_46 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 12, 12, 160)  179200      ['activation_41[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 12, 12, 160)  179200      ['activation_46[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 12, 12, 160)  480        ['conv2d_42[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 12, 12, 160)  480        ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_42 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " activation_47 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d_4 (AveragePo  (None, 12, 12, 768)  0          ['mixed4[0][0]']                 \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 12, 12, 192)  147456      ['mixed4[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 12, 12, 192)  215040      ['activation_42[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 12, 12, 192)  215040      ['activation_47[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 12, 12, 192)  147456      ['average_pooling2d_4[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 12, 12, 192)  576        ['conv2d_40[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 12, 12, 192)  576        ['conv2d_43[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 12, 12, 192)  576        ['conv2d_48[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_49 (BatchN  (None, 12, 12, 192)  576        ['conv2d_49[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_40 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " activation_43 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " activation_48 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            " activation_49 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_49[0][0]'] \n",
            "                                                                                                  \n",
            " mixed5 (Concatenate)           (None, 12, 12, 768)  0           ['activation_40[0][0]',          \n",
            "                                                                  'activation_43[0][0]',          \n",
            "                                                                  'activation_48[0][0]',          \n",
            "                                                                  'activation_49[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 12, 12, 160)  122880      ['mixed5[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_54 (BatchN  (None, 12, 12, 160)  480        ['conv2d_54[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_54 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_54[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 12, 12, 160)  179200      ['activation_54[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_55 (BatchN  (None, 12, 12, 160)  480        ['conv2d_55[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_55 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_55[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 12, 12, 160)  122880      ['mixed5[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 12, 12, 160)  179200      ['activation_55[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_51 (BatchN  (None, 12, 12, 160)  480        ['conv2d_51[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_56 (BatchN  (None, 12, 12, 160)  480        ['conv2d_56[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_51 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_51[0][0]'] \n",
            "                                                                                                  \n",
            " activation_56 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_56[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 12, 12, 160)  179200      ['activation_51[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 12, 12, 160)  179200      ['activation_56[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_52 (BatchN  (None, 12, 12, 160)  480        ['conv2d_52[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_57 (BatchN  (None, 12, 12, 160)  480        ['conv2d_57[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_52 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_52[0][0]'] \n",
            "                                                                                                  \n",
            " activation_57 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_57[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d_5 (AveragePo  (None, 12, 12, 768)  0          ['mixed5[0][0]']                 \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 12, 12, 192)  147456      ['mixed5[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 12, 12, 192)  215040      ['activation_52[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 12, 12, 192)  215040      ['activation_57[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 12, 12, 192)  147456      ['average_pooling2d_5[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_50 (BatchN  (None, 12, 12, 192)  576        ['conv2d_50[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_53 (BatchN  (None, 12, 12, 192)  576        ['conv2d_53[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_58 (BatchN  (None, 12, 12, 192)  576        ['conv2d_58[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_59 (BatchN  (None, 12, 12, 192)  576        ['conv2d_59[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_50 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_50[0][0]'] \n",
            "                                                                                                  \n",
            " activation_53 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_53[0][0]'] \n",
            "                                                                                                  \n",
            " activation_58 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_58[0][0]'] \n",
            "                                                                                                  \n",
            " activation_59 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_59[0][0]'] \n",
            "                                                                                                  \n",
            " mixed6 (Concatenate)           (None, 12, 12, 768)  0           ['activation_50[0][0]',          \n",
            "                                                                  'activation_53[0][0]',          \n",
            "                                                                  'activation_58[0][0]',          \n",
            "                                                                  'activation_59[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 12, 12, 192)  147456      ['mixed6[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_64 (BatchN  (None, 12, 12, 192)  576        ['conv2d_64[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_64 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_64[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_65 (Conv2D)             (None, 12, 12, 192)  258048      ['activation_64[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_65 (BatchN  (None, 12, 12, 192)  576        ['conv2d_65[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_65 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_65[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 12, 12, 192)  147456      ['mixed6[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_66 (Conv2D)             (None, 12, 12, 192)  258048      ['activation_65[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_61 (BatchN  (None, 12, 12, 192)  576        ['conv2d_61[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_66 (BatchN  (None, 12, 12, 192)  576        ['conv2d_66[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_61 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_61[0][0]'] \n",
            "                                                                                                  \n",
            " activation_66 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_66[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 12, 12, 192)  258048      ['activation_61[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_67 (Conv2D)             (None, 12, 12, 192)  258048      ['activation_66[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_62 (BatchN  (None, 12, 12, 192)  576        ['conv2d_62[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_67 (BatchN  (None, 12, 12, 192)  576        ['conv2d_67[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_62 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_62[0][0]'] \n",
            "                                                                                                  \n",
            " activation_67 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_67[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d_6 (AveragePo  (None, 12, 12, 768)  0          ['mixed6[0][0]']                 \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 12, 12, 192)  147456      ['mixed6[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 12, 12, 192)  258048      ['activation_62[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_68 (Conv2D)             (None, 12, 12, 192)  258048      ['activation_67[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_69 (Conv2D)             (None, 12, 12, 192)  147456      ['average_pooling2d_6[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_60 (BatchN  (None, 12, 12, 192)  576        ['conv2d_60[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_63 (BatchN  (None, 12, 12, 192)  576        ['conv2d_63[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_68 (BatchN  (None, 12, 12, 192)  576        ['conv2d_68[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_69 (BatchN  (None, 12, 12, 192)  576        ['conv2d_69[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_60 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_60[0][0]'] \n",
            "                                                                                                  \n",
            " activation_63 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_63[0][0]'] \n",
            "                                                                                                  \n",
            " activation_68 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_68[0][0]'] \n",
            "                                                                                                  \n",
            " activation_69 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_69[0][0]'] \n",
            "                                                                                                  \n",
            " mixed7 (Concatenate)           (None, 12, 12, 768)  0           ['activation_60[0][0]',          \n",
            "                                                                  'activation_63[0][0]',          \n",
            "                                                                  'activation_68[0][0]',          \n",
            "                                                                  'activation_69[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_72 (Conv2D)             (None, 12, 12, 192)  147456      ['mixed7[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_72 (BatchN  (None, 12, 12, 192)  576        ['conv2d_72[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_72 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_72[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_73 (Conv2D)             (None, 12, 12, 192)  258048      ['activation_72[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_73 (BatchN  (None, 12, 12, 192)  576        ['conv2d_73[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_73 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_73[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_70 (Conv2D)             (None, 12, 12, 192)  147456      ['mixed7[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_74 (Conv2D)             (None, 12, 12, 192)  258048      ['activation_73[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_70 (BatchN  (None, 12, 12, 192)  576        ['conv2d_70[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_74 (BatchN  (None, 12, 12, 192)  576        ['conv2d_74[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_70 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_70[0][0]'] \n",
            "                                                                                                  \n",
            " activation_74 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_74[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_71 (Conv2D)             (None, 5, 5, 320)    552960      ['activation_70[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_75 (Conv2D)             (None, 5, 5, 192)    331776      ['activation_74[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_71 (BatchN  (None, 5, 5, 320)   960         ['conv2d_71[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_75 (BatchN  (None, 5, 5, 192)   576         ['conv2d_75[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_71 (Activation)     (None, 5, 5, 320)    0           ['batch_normalization_71[0][0]'] \n",
            "                                                                                                  \n",
            " activation_75 (Activation)     (None, 5, 5, 192)    0           ['batch_normalization_75[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPooling2D)  (None, 5, 5, 768)   0           ['mixed7[0][0]']                 \n",
            "                                                                                                  \n",
            " mixed8 (Concatenate)           (None, 5, 5, 1280)   0           ['activation_71[0][0]',          \n",
            "                                                                  'activation_75[0][0]',          \n",
            "                                                                  'max_pooling2d_3[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_80 (Conv2D)             (None, 5, 5, 448)    573440      ['mixed8[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_80 (BatchN  (None, 5, 5, 448)   1344        ['conv2d_80[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_80 (Activation)     (None, 5, 5, 448)    0           ['batch_normalization_80[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_77 (Conv2D)             (None, 5, 5, 384)    491520      ['mixed8[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_81 (Conv2D)             (None, 5, 5, 384)    1548288     ['activation_80[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_77 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_77[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_81 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_81[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_77 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_77[0][0]'] \n",
            "                                                                                                  \n",
            " activation_81 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_81[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_78 (Conv2D)             (None, 5, 5, 384)    442368      ['activation_77[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_79 (Conv2D)             (None, 5, 5, 384)    442368      ['activation_77[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_82 (Conv2D)             (None, 5, 5, 384)    442368      ['activation_81[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_83 (Conv2D)             (None, 5, 5, 384)    442368      ['activation_81[0][0]']          \n",
            "                                                                                                  \n",
            " average_pooling2d_7 (AveragePo  (None, 5, 5, 1280)  0           ['mixed8[0][0]']                 \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_76 (Conv2D)             (None, 5, 5, 320)    409600      ['mixed8[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_78 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_78[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_79 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_79[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_82 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_82[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_83 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_83[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_84 (Conv2D)             (None, 5, 5, 192)    245760      ['average_pooling2d_7[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_76 (BatchN  (None, 5, 5, 320)   960         ['conv2d_76[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_78 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_78[0][0]'] \n",
            "                                                                                                  \n",
            " activation_79 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_79[0][0]'] \n",
            "                                                                                                  \n",
            " activation_82 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_82[0][0]'] \n",
            "                                                                                                  \n",
            " activation_83 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_83[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_84 (BatchN  (None, 5, 5, 192)   576         ['conv2d_84[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_76 (Activation)     (None, 5, 5, 320)    0           ['batch_normalization_76[0][0]'] \n",
            "                                                                                                  \n",
            " mixed9_0 (Concatenate)         (None, 5, 5, 768)    0           ['activation_78[0][0]',          \n",
            "                                                                  'activation_79[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 5, 5, 768)    0           ['activation_82[0][0]',          \n",
            "                                                                  'activation_83[0][0]']          \n",
            "                                                                                                  \n",
            " activation_84 (Activation)     (None, 5, 5, 192)    0           ['batch_normalization_84[0][0]'] \n",
            "                                                                                                  \n",
            " mixed9 (Concatenate)           (None, 5, 5, 2048)   0           ['activation_76[0][0]',          \n",
            "                                                                  'mixed9_0[0][0]',               \n",
            "                                                                  'concatenate[0][0]',            \n",
            "                                                                  'activation_84[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_89 (Conv2D)             (None, 5, 5, 448)    917504      ['mixed9[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_89 (BatchN  (None, 5, 5, 448)   1344        ['conv2d_89[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_89 (Activation)     (None, 5, 5, 448)    0           ['batch_normalization_89[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_86 (Conv2D)             (None, 5, 5, 384)    786432      ['mixed9[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_90 (Conv2D)             (None, 5, 5, 384)    1548288     ['activation_89[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_86 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_86[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_90 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_90[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_86 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_86[0][0]'] \n",
            "                                                                                                  \n",
            " activation_90 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_90[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_87 (Conv2D)             (None, 5, 5, 384)    442368      ['activation_86[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_88 (Conv2D)             (None, 5, 5, 384)    442368      ['activation_86[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_91 (Conv2D)             (None, 5, 5, 384)    442368      ['activation_90[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_92 (Conv2D)             (None, 5, 5, 384)    442368      ['activation_90[0][0]']          \n",
            "                                                                                                  \n",
            " average_pooling2d_8 (AveragePo  (None, 5, 5, 2048)  0           ['mixed9[0][0]']                 \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_85 (Conv2D)             (None, 5, 5, 320)    655360      ['mixed9[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_87 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_87[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_88 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_88[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_91 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_91[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_92 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_92[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_93 (Conv2D)             (None, 5, 5, 192)    393216      ['average_pooling2d_8[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_85 (BatchN  (None, 5, 5, 320)   960         ['conv2d_85[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_87 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_87[0][0]'] \n",
            "                                                                                                  \n",
            " activation_88 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_88[0][0]'] \n",
            "                                                                                                  \n",
            " activation_91 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_91[0][0]'] \n",
            "                                                                                                  \n",
            " activation_92 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_92[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_93 (BatchN  (None, 5, 5, 192)   576         ['conv2d_93[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_85 (Activation)     (None, 5, 5, 320)    0           ['batch_normalization_85[0][0]'] \n",
            "                                                                                                  \n",
            " mixed9_1 (Concatenate)         (None, 5, 5, 768)    0           ['activation_87[0][0]',          \n",
            "                                                                  'activation_88[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 5, 5, 768)    0           ['activation_91[0][0]',          \n",
            "                                                                  'activation_92[0][0]']          \n",
            "                                                                                                  \n",
            " activation_93 (Activation)     (None, 5, 5, 192)    0           ['batch_normalization_93[0][0]'] \n",
            "                                                                                                  \n",
            " mixed10 (Concatenate)          (None, 5, 5, 2048)   0           ['activation_85[0][0]',          \n",
            "                                                                  'mixed9_1[0][0]',               \n",
            "                                                                  'concatenate_1[0][0]',          \n",
            "                                                                  'activation_93[0][0]']          \n",
            "                                                                                                  \n",
            " global_average_pooling2d (Glob  (None, 2048)        0           ['mixed10[0][0]']                \n",
            " alAveragePooling2D)                                                                              \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 2048)         0           ['global_average_pooling2d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 1024)         2098176     ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 256)          262400      ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 64)           16448       ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 32)           2080        ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 1)            33          ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 24,181,921\n",
            "Trainable params: 4,756,865\n",
            "Non-trainable params: 19,425,056\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = my_inc_angle.fit_generator(image_data_generator(x_train_angle, y_train_angle, batch_size=50),\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=20,\n",
        "                              validation_data = image_data_generator(x_valid_angle,y_valid_angle, batch_size=50),\n",
        "                              validation_steps=500,\n",
        "                              verbose=1,\n",
        "                              shuffle=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sv6h4JWyqH-d",
        "outputId": "ba9670b8-0f1f-44be-bdf7-5b5c47aa4a7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "500/500 [==============================] - 259s 502ms/step - loss: 0.0186 - mse: 0.0186 - val_loss: 0.0153 - val_mse: 0.0153\n",
            "Epoch 2/20\n",
            "500/500 [==============================] - 250s 500ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0128 - val_mse: 0.0128\n",
            "Epoch 3/20\n",
            "500/500 [==============================] - 250s 501ms/step - loss: 0.0068 - mse: 0.0068 - val_loss: 0.0114 - val_mse: 0.0114\n",
            "Epoch 4/20\n",
            "500/500 [==============================] - 253s 507ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0109 - val_mse: 0.0109\n",
            "Epoch 5/20\n",
            "500/500 [==============================] - 253s 507ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0106 - val_mse: 0.0106\n",
            "Epoch 6/20\n",
            "500/500 [==============================] - 251s 503ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0103 - val_mse: 0.0103\n",
            "Epoch 7/20\n",
            "500/500 [==============================] - 251s 502ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0104 - val_mse: 0.0104\n",
            "Epoch 8/20\n",
            "500/500 [==============================] - 252s 504ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0099 - val_mse: 0.0099\n",
            "Epoch 9/20\n",
            "500/500 [==============================] - 252s 505ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0099 - val_mse: 0.0099\n",
            "Epoch 10/20\n",
            "500/500 [==============================] - 252s 505ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0099 - val_mse: 0.0099\n",
            "Epoch 11/20\n",
            "500/500 [==============================] - 249s 499ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0098 - val_mse: 0.0098\n",
            "Epoch 12/20\n",
            "500/500 [==============================] - 253s 507ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0099 - val_mse: 0.0099\n",
            "Epoch 13/20\n",
            "500/500 [==============================] - 253s 506ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0099 - val_mse: 0.0099\n",
            "Epoch 14/20\n",
            "500/500 [==============================] - 253s 507ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0097 - val_mse: 0.0097\n",
            "Epoch 15/20\n",
            "500/500 [==============================] - 253s 507ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0096 - val_mse: 0.0096\n",
            "Epoch 16/20\n",
            "500/500 [==============================] - 252s 505ms/step - loss: 9.9512e-04 - mse: 9.9512e-04 - val_loss: 0.0095 - val_mse: 0.0095\n",
            "Epoch 17/20\n",
            "500/500 [==============================] - 251s 504ms/step - loss: 9.2920e-04 - mse: 9.2920e-04 - val_loss: 0.0095 - val_mse: 0.0095\n",
            "Epoch 18/20\n",
            "500/500 [==============================] - 251s 503ms/step - loss: 8.9423e-04 - mse: 8.9423e-04 - val_loss: 0.0090 - val_mse: 0.0090\n",
            "Epoch 19/20\n",
            "500/500 [==============================] - 249s 499ms/step - loss: 8.3349e-04 - mse: 8.3349e-04 - val_loss: 0.0097 - val_mse: 0.0097\n",
            "Epoch 20/20\n",
            "500/500 [==============================] - 250s 500ms/step - loss: 8.0982e-04 - mse: 8.0982e-04 - val_loss: 0.0091 - val_mse: 0.0091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_angle[:10])\n",
        "print(y_train_angle[:10])\n",
        "print(x_valid_angle[:10])\n",
        "print(y_valid_angle[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajJtXFvhHd6i",
        "outputId": "3c300973-254f-40e7-81cb-e1ee03f38016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/data/training_data/training_data/2616.png', '/content/data/training_data/training_data/3282.png', '/content/data/training_data/training_data/11990.png', '/content/data/training_data/training_data/1201.png', '/content/data/training_data/training_data/600.png', '/content/data/training_data/training_data/5760.png', '/content/data/training_data/training_data/13.png', '/content/data/training_data/training_data/6294.png', '/content/data/training_data/training_data/11677.png', '/content/data/training_data/training_data/4667.png']\n",
            "[0.75, 0.6875, 0.1875, 0.5625, 0.75, 0.6875, 0.4375, 0.5, 0.8125, 0.5]\n",
            "['/content/data/training_data/training_data/11160.png', '/content/data/training_data/training_data/9088.png', '/content/data/training_data/training_data/30.png', '/content/data/training_data/training_data/10779.png', '/content/data/training_data/training_data/6607.png', '/content/data/training_data/training_data/3225.png', '/content/data/training_data/training_data/6825.png', '/content/data/training_data/training_data/6425.png', '/content/data/training_data/training_data/8995.png', '/content/data/training_data/training_data/4288.png']\n",
            "[0.5, 0.6875, 0.4375, 0.5, 0.75, 0.6875, 0.6875, 0.1875, 0.6875, 0.75]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transfer Learning(Speed)"
      ],
      "metadata": {
        "id": "wUX5vwLZffNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications.inception_v3 import InceptionV3\n",
        "incept_speed= InceptionV3(input_shape = (224,224,3),weights='imagenet',include_top=False)\n",
        "incept_speed.trainable = False"
      ],
      "metadata": {
        "id": "v_mcb7kfM4Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, GlobalAveragePooling2D \n",
        "from keras.regularizers import l1,l2\n",
        "for layer in incept_speed.layers[290:]: \n",
        "    layer.trainable = True \n",
        "y = GlobalAveragePooling2D()(incept_speed.output)\n",
        "# y = Flatten()(incept_speed.output)\n",
        "y = Dense(1024, activation ='relu')(y)\n",
        "# let's add a fully-connected layer as first layer\n",
        "y = Dense(256, activation ='relu')(y)\n",
        "y = Dense(64, activation ='relu')(y)\n",
        "y = Dense(32, activation ='relu')(y)\n",
        "# y = Dense(1024, activation ='relu')#,kernel_regularizer=l1(0.0001), bias_regularizer=l1(0.0001))(y)\n",
        "# y = Dense(256, activation ='relu')#,kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(y)\n",
        "# y = Dense(64, activation ='relu')#,kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(y)\n",
        "# y = Dense(32, activation ='relu')#,kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(y)\n",
        "# y = Dense( 32, activation ='relu')(y)\n",
        "pred_speed = Dense(1,activation ='sigmoid')(y)"
      ],
      "metadata": {
        "id": "lFtUJRSXNScr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "my_inc_speed = Model(inputs = incept_speed.input,outputs=pred_speed)\n",
        "my_inc_speed.summary()\n",
        "loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "my_inc_speed.compile(optimizer = 'adam',loss=\"mse\",metrics=[\"mse\"])\n",
        "my_inc_speed.optimizer.learning_rate = 0.000002"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s4G-lBZfnhw",
        "outputId": "c6808f08-fc2d-4af5-9a13-14312bb1878e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d_94 (Conv2D)             (None, 111, 111, 32  864         ['input_2[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_94 (BatchN  (None, 111, 111, 32  96         ['conv2d_94[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_94 (Activation)     (None, 111, 111, 32  0           ['batch_normalization_94[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_95 (Conv2D)             (None, 109, 109, 32  9216        ['activation_94[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_95 (BatchN  (None, 109, 109, 32  96         ['conv2d_95[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_95 (Activation)     (None, 109, 109, 32  0           ['batch_normalization_95[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_96 (Conv2D)             (None, 109, 109, 64  18432       ['activation_95[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_96 (BatchN  (None, 109, 109, 64  192        ['conv2d_96[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_96 (Activation)     (None, 109, 109, 64  0           ['batch_normalization_96[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_4 (MaxPooling2D)  (None, 54, 54, 64)  0           ['activation_96[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_97 (Conv2D)             (None, 54, 54, 80)   5120        ['max_pooling2d_4[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_97 (BatchN  (None, 54, 54, 80)  240         ['conv2d_97[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_97 (Activation)     (None, 54, 54, 80)   0           ['batch_normalization_97[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_98 (Conv2D)             (None, 52, 52, 192)  138240      ['activation_97[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_98 (BatchN  (None, 52, 52, 192)  576        ['conv2d_98[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_98 (Activation)     (None, 52, 52, 192)  0           ['batch_normalization_98[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d_5 (MaxPooling2D)  (None, 25, 25, 192)  0          ['activation_98[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_102 (Conv2D)            (None, 25, 25, 64)   12288       ['max_pooling2d_5[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_102 (Batch  (None, 25, 25, 64)  192         ['conv2d_102[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_102 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_102[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_100 (Conv2D)            (None, 25, 25, 48)   9216        ['max_pooling2d_5[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_103 (Conv2D)            (None, 25, 25, 96)   55296       ['activation_102[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_100 (Batch  (None, 25, 25, 48)  144         ['conv2d_100[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_103 (Batch  (None, 25, 25, 96)  288         ['conv2d_103[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_100 (Activation)    (None, 25, 25, 48)   0           ['batch_normalization_100[0][0]']\n",
            "                                                                                                  \n",
            " activation_103 (Activation)    (None, 25, 25, 96)   0           ['batch_normalization_103[0][0]']\n",
            "                                                                                                  \n",
            " average_pooling2d_9 (AveragePo  (None, 25, 25, 192)  0          ['max_pooling2d_5[0][0]']        \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_99 (Conv2D)             (None, 25, 25, 64)   12288       ['max_pooling2d_5[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_101 (Conv2D)            (None, 25, 25, 64)   76800       ['activation_100[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_104 (Conv2D)            (None, 25, 25, 96)   82944       ['activation_103[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_105 (Conv2D)            (None, 25, 25, 32)   6144        ['average_pooling2d_9[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_99 (BatchN  (None, 25, 25, 64)  192         ['conv2d_99[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_101 (Batch  (None, 25, 25, 64)  192         ['conv2d_101[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_104 (Batch  (None, 25, 25, 96)  288         ['conv2d_104[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_105 (Batch  (None, 25, 25, 32)  96          ['conv2d_105[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_99 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_99[0][0]'] \n",
            "                                                                                                  \n",
            " activation_101 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_101[0][0]']\n",
            "                                                                                                  \n",
            " activation_104 (Activation)    (None, 25, 25, 96)   0           ['batch_normalization_104[0][0]']\n",
            "                                                                                                  \n",
            " activation_105 (Activation)    (None, 25, 25, 32)   0           ['batch_normalization_105[0][0]']\n",
            "                                                                                                  \n",
            " mixed0 (Concatenate)           (None, 25, 25, 256)  0           ['activation_99[0][0]',          \n",
            "                                                                  'activation_101[0][0]',         \n",
            "                                                                  'activation_104[0][0]',         \n",
            "                                                                  'activation_105[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_109 (Conv2D)            (None, 25, 25, 64)   16384       ['mixed0[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_109 (Batch  (None, 25, 25, 64)  192         ['conv2d_109[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_109 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_109[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_107 (Conv2D)            (None, 25, 25, 48)   12288       ['mixed0[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_110 (Conv2D)            (None, 25, 25, 96)   55296       ['activation_109[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_107 (Batch  (None, 25, 25, 48)  144         ['conv2d_107[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_110 (Batch  (None, 25, 25, 96)  288         ['conv2d_110[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_107 (Activation)    (None, 25, 25, 48)   0           ['batch_normalization_107[0][0]']\n",
            "                                                                                                  \n",
            " activation_110 (Activation)    (None, 25, 25, 96)   0           ['batch_normalization_110[0][0]']\n",
            "                                                                                                  \n",
            " average_pooling2d_10 (AverageP  (None, 25, 25, 256)  0          ['mixed0[0][0]']                 \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_106 (Conv2D)            (None, 25, 25, 64)   16384       ['mixed0[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_108 (Conv2D)            (None, 25, 25, 64)   76800       ['activation_107[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_111 (Conv2D)            (None, 25, 25, 96)   82944       ['activation_110[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_112 (Conv2D)            (None, 25, 25, 64)   16384       ['average_pooling2d_10[0][0]']   \n",
            "                                                                                                  \n",
            " batch_normalization_106 (Batch  (None, 25, 25, 64)  192         ['conv2d_106[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_108 (Batch  (None, 25, 25, 64)  192         ['conv2d_108[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_111 (Batch  (None, 25, 25, 96)  288         ['conv2d_111[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_112 (Batch  (None, 25, 25, 64)  192         ['conv2d_112[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_106 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_106[0][0]']\n",
            "                                                                                                  \n",
            " activation_108 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_108[0][0]']\n",
            "                                                                                                  \n",
            " activation_111 (Activation)    (None, 25, 25, 96)   0           ['batch_normalization_111[0][0]']\n",
            "                                                                                                  \n",
            " activation_112 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_112[0][0]']\n",
            "                                                                                                  \n",
            " mixed1 (Concatenate)           (None, 25, 25, 288)  0           ['activation_106[0][0]',         \n",
            "                                                                  'activation_108[0][0]',         \n",
            "                                                                  'activation_111[0][0]',         \n",
            "                                                                  'activation_112[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_116 (Conv2D)            (None, 25, 25, 64)   18432       ['mixed1[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_116 (Batch  (None, 25, 25, 64)  192         ['conv2d_116[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_116 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_116[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_114 (Conv2D)            (None, 25, 25, 48)   13824       ['mixed1[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_117 (Conv2D)            (None, 25, 25, 96)   55296       ['activation_116[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_114 (Batch  (None, 25, 25, 48)  144         ['conv2d_114[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_117 (Batch  (None, 25, 25, 96)  288         ['conv2d_117[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_114 (Activation)    (None, 25, 25, 48)   0           ['batch_normalization_114[0][0]']\n",
            "                                                                                                  \n",
            " activation_117 (Activation)    (None, 25, 25, 96)   0           ['batch_normalization_117[0][0]']\n",
            "                                                                                                  \n",
            " average_pooling2d_11 (AverageP  (None, 25, 25, 288)  0          ['mixed1[0][0]']                 \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_113 (Conv2D)            (None, 25, 25, 64)   18432       ['mixed1[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_115 (Conv2D)            (None, 25, 25, 64)   76800       ['activation_114[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_118 (Conv2D)            (None, 25, 25, 96)   82944       ['activation_117[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_119 (Conv2D)            (None, 25, 25, 64)   18432       ['average_pooling2d_11[0][0]']   \n",
            "                                                                                                  \n",
            " batch_normalization_113 (Batch  (None, 25, 25, 64)  192         ['conv2d_113[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_115 (Batch  (None, 25, 25, 64)  192         ['conv2d_115[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_118 (Batch  (None, 25, 25, 96)  288         ['conv2d_118[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_119 (Batch  (None, 25, 25, 64)  192         ['conv2d_119[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_113 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_113[0][0]']\n",
            "                                                                                                  \n",
            " activation_115 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_115[0][0]']\n",
            "                                                                                                  \n",
            " activation_118 (Activation)    (None, 25, 25, 96)   0           ['batch_normalization_118[0][0]']\n",
            "                                                                                                  \n",
            " activation_119 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_119[0][0]']\n",
            "                                                                                                  \n",
            " mixed2 (Concatenate)           (None, 25, 25, 288)  0           ['activation_113[0][0]',         \n",
            "                                                                  'activation_115[0][0]',         \n",
            "                                                                  'activation_118[0][0]',         \n",
            "                                                                  'activation_119[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_121 (Conv2D)            (None, 25, 25, 64)   18432       ['mixed2[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_121 (Batch  (None, 25, 25, 64)  192         ['conv2d_121[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_121 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_121[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_122 (Conv2D)            (None, 25, 25, 96)   55296       ['activation_121[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_122 (Batch  (None, 25, 25, 96)  288         ['conv2d_122[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_122 (Activation)    (None, 25, 25, 96)   0           ['batch_normalization_122[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_120 (Conv2D)            (None, 12, 12, 384)  995328      ['mixed2[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_123 (Conv2D)            (None, 12, 12, 96)   82944       ['activation_122[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_120 (Batch  (None, 12, 12, 384)  1152       ['conv2d_120[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_123 (Batch  (None, 12, 12, 96)  288         ['conv2d_123[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_120 (Activation)    (None, 12, 12, 384)  0           ['batch_normalization_120[0][0]']\n",
            "                                                                                                  \n",
            " activation_123 (Activation)    (None, 12, 12, 96)   0           ['batch_normalization_123[0][0]']\n",
            "                                                                                                  \n",
            " max_pooling2d_6 (MaxPooling2D)  (None, 12, 12, 288)  0          ['mixed2[0][0]']                 \n",
            "                                                                                                  \n",
            " mixed3 (Concatenate)           (None, 12, 12, 768)  0           ['activation_120[0][0]',         \n",
            "                                                                  'activation_123[0][0]',         \n",
            "                                                                  'max_pooling2d_6[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_128 (Conv2D)            (None, 12, 12, 128)  98304       ['mixed3[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_128 (Batch  (None, 12, 12, 128)  384        ['conv2d_128[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_128 (Activation)    (None, 12, 12, 128)  0           ['batch_normalization_128[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_129 (Conv2D)            (None, 12, 12, 128)  114688      ['activation_128[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_129 (Batch  (None, 12, 12, 128)  384        ['conv2d_129[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_129 (Activation)    (None, 12, 12, 128)  0           ['batch_normalization_129[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_125 (Conv2D)            (None, 12, 12, 128)  98304       ['mixed3[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_130 (Conv2D)            (None, 12, 12, 128)  114688      ['activation_129[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_125 (Batch  (None, 12, 12, 128)  384        ['conv2d_125[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_130 (Batch  (None, 12, 12, 128)  384        ['conv2d_130[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_125 (Activation)    (None, 12, 12, 128)  0           ['batch_normalization_125[0][0]']\n",
            "                                                                                                  \n",
            " activation_130 (Activation)    (None, 12, 12, 128)  0           ['batch_normalization_130[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_126 (Conv2D)            (None, 12, 12, 128)  114688      ['activation_125[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_131 (Conv2D)            (None, 12, 12, 128)  114688      ['activation_130[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_126 (Batch  (None, 12, 12, 128)  384        ['conv2d_126[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_131 (Batch  (None, 12, 12, 128)  384        ['conv2d_131[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_126 (Activation)    (None, 12, 12, 128)  0           ['batch_normalization_126[0][0]']\n",
            "                                                                                                  \n",
            " activation_131 (Activation)    (None, 12, 12, 128)  0           ['batch_normalization_131[0][0]']\n",
            "                                                                                                  \n",
            " average_pooling2d_12 (AverageP  (None, 12, 12, 768)  0          ['mixed3[0][0]']                 \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_124 (Conv2D)            (None, 12, 12, 192)  147456      ['mixed3[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_127 (Conv2D)            (None, 12, 12, 192)  172032      ['activation_126[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_132 (Conv2D)            (None, 12, 12, 192)  172032      ['activation_131[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_133 (Conv2D)            (None, 12, 12, 192)  147456      ['average_pooling2d_12[0][0]']   \n",
            "                                                                                                  \n",
            " batch_normalization_124 (Batch  (None, 12, 12, 192)  576        ['conv2d_124[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_127 (Batch  (None, 12, 12, 192)  576        ['conv2d_127[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_132 (Batch  (None, 12, 12, 192)  576        ['conv2d_132[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_133 (Batch  (None, 12, 12, 192)  576        ['conv2d_133[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_124 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_124[0][0]']\n",
            "                                                                                                  \n",
            " activation_127 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_127[0][0]']\n",
            "                                                                                                  \n",
            " activation_132 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_132[0][0]']\n",
            "                                                                                                  \n",
            " activation_133 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_133[0][0]']\n",
            "                                                                                                  \n",
            " mixed4 (Concatenate)           (None, 12, 12, 768)  0           ['activation_124[0][0]',         \n",
            "                                                                  'activation_127[0][0]',         \n",
            "                                                                  'activation_132[0][0]',         \n",
            "                                                                  'activation_133[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_138 (Conv2D)            (None, 12, 12, 160)  122880      ['mixed4[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_138 (Batch  (None, 12, 12, 160)  480        ['conv2d_138[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_138 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_138[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_139 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_138[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_139 (Batch  (None, 12, 12, 160)  480        ['conv2d_139[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_139 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_139[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_135 (Conv2D)            (None, 12, 12, 160)  122880      ['mixed4[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_140 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_139[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_135 (Batch  (None, 12, 12, 160)  480        ['conv2d_135[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_140 (Batch  (None, 12, 12, 160)  480        ['conv2d_140[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_135 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_135[0][0]']\n",
            "                                                                                                  \n",
            " activation_140 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_140[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_136 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_135[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_141 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_140[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_136 (Batch  (None, 12, 12, 160)  480        ['conv2d_136[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_141 (Batch  (None, 12, 12, 160)  480        ['conv2d_141[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_136 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_136[0][0]']\n",
            "                                                                                                  \n",
            " activation_141 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_141[0][0]']\n",
            "                                                                                                  \n",
            " average_pooling2d_13 (AverageP  (None, 12, 12, 768)  0          ['mixed4[0][0]']                 \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_134 (Conv2D)            (None, 12, 12, 192)  147456      ['mixed4[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_137 (Conv2D)            (None, 12, 12, 192)  215040      ['activation_136[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_142 (Conv2D)            (None, 12, 12, 192)  215040      ['activation_141[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_143 (Conv2D)            (None, 12, 12, 192)  147456      ['average_pooling2d_13[0][0]']   \n",
            "                                                                                                  \n",
            " batch_normalization_134 (Batch  (None, 12, 12, 192)  576        ['conv2d_134[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_137 (Batch  (None, 12, 12, 192)  576        ['conv2d_137[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_142 (Batch  (None, 12, 12, 192)  576        ['conv2d_142[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_143 (Batch  (None, 12, 12, 192)  576        ['conv2d_143[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_134 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_134[0][0]']\n",
            "                                                                                                  \n",
            " activation_137 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_137[0][0]']\n",
            "                                                                                                  \n",
            " activation_142 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_142[0][0]']\n",
            "                                                                                                  \n",
            " activation_143 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_143[0][0]']\n",
            "                                                                                                  \n",
            " mixed5 (Concatenate)           (None, 12, 12, 768)  0           ['activation_134[0][0]',         \n",
            "                                                                  'activation_137[0][0]',         \n",
            "                                                                  'activation_142[0][0]',         \n",
            "                                                                  'activation_143[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_148 (Conv2D)            (None, 12, 12, 160)  122880      ['mixed5[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_148 (Batch  (None, 12, 12, 160)  480        ['conv2d_148[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_148 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_148[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_149 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_148[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_149 (Batch  (None, 12, 12, 160)  480        ['conv2d_149[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_149 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_149[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_145 (Conv2D)            (None, 12, 12, 160)  122880      ['mixed5[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_150 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_149[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_145 (Batch  (None, 12, 12, 160)  480        ['conv2d_145[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_150 (Batch  (None, 12, 12, 160)  480        ['conv2d_150[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_145 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_145[0][0]']\n",
            "                                                                                                  \n",
            " activation_150 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_150[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_146 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_145[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_151 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_150[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_146 (Batch  (None, 12, 12, 160)  480        ['conv2d_146[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_151 (Batch  (None, 12, 12, 160)  480        ['conv2d_151[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_146 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_146[0][0]']\n",
            "                                                                                                  \n",
            " activation_151 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_151[0][0]']\n",
            "                                                                                                  \n",
            " average_pooling2d_14 (AverageP  (None, 12, 12, 768)  0          ['mixed5[0][0]']                 \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_144 (Conv2D)            (None, 12, 12, 192)  147456      ['mixed5[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_147 (Conv2D)            (None, 12, 12, 192)  215040      ['activation_146[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_152 (Conv2D)            (None, 12, 12, 192)  215040      ['activation_151[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_153 (Conv2D)            (None, 12, 12, 192)  147456      ['average_pooling2d_14[0][0]']   \n",
            "                                                                                                  \n",
            " batch_normalization_144 (Batch  (None, 12, 12, 192)  576        ['conv2d_144[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_147 (Batch  (None, 12, 12, 192)  576        ['conv2d_147[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_152 (Batch  (None, 12, 12, 192)  576        ['conv2d_152[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_153 (Batch  (None, 12, 12, 192)  576        ['conv2d_153[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_144 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_144[0][0]']\n",
            "                                                                                                  \n",
            " activation_147 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_147[0][0]']\n",
            "                                                                                                  \n",
            " activation_152 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_152[0][0]']\n",
            "                                                                                                  \n",
            " activation_153 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_153[0][0]']\n",
            "                                                                                                  \n",
            " mixed6 (Concatenate)           (None, 12, 12, 768)  0           ['activation_144[0][0]',         \n",
            "                                                                  'activation_147[0][0]',         \n",
            "                                                                  'activation_152[0][0]',         \n",
            "                                                                  'activation_153[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_158 (Conv2D)            (None, 12, 12, 192)  147456      ['mixed6[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_158 (Batch  (None, 12, 12, 192)  576        ['conv2d_158[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_158 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_158[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_159 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_158[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_159 (Batch  (None, 12, 12, 192)  576        ['conv2d_159[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_159 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_159[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_155 (Conv2D)            (None, 12, 12, 192)  147456      ['mixed6[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_160 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_159[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_155 (Batch  (None, 12, 12, 192)  576        ['conv2d_155[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_160 (Batch  (None, 12, 12, 192)  576        ['conv2d_160[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_155 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_155[0][0]']\n",
            "                                                                                                  \n",
            " activation_160 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_160[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_156 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_155[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_161 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_160[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_156 (Batch  (None, 12, 12, 192)  576        ['conv2d_156[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_161 (Batch  (None, 12, 12, 192)  576        ['conv2d_161[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_156 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_156[0][0]']\n",
            "                                                                                                  \n",
            " activation_161 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_161[0][0]']\n",
            "                                                                                                  \n",
            " average_pooling2d_15 (AverageP  (None, 12, 12, 768)  0          ['mixed6[0][0]']                 \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_154 (Conv2D)            (None, 12, 12, 192)  147456      ['mixed6[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_157 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_156[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_162 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_161[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_163 (Conv2D)            (None, 12, 12, 192)  147456      ['average_pooling2d_15[0][0]']   \n",
            "                                                                                                  \n",
            " batch_normalization_154 (Batch  (None, 12, 12, 192)  576        ['conv2d_154[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_157 (Batch  (None, 12, 12, 192)  576        ['conv2d_157[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_162 (Batch  (None, 12, 12, 192)  576        ['conv2d_162[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_163 (Batch  (None, 12, 12, 192)  576        ['conv2d_163[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_154 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_154[0][0]']\n",
            "                                                                                                  \n",
            " activation_157 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_157[0][0]']\n",
            "                                                                                                  \n",
            " activation_162 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_162[0][0]']\n",
            "                                                                                                  \n",
            " activation_163 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_163[0][0]']\n",
            "                                                                                                  \n",
            " mixed7 (Concatenate)           (None, 12, 12, 768)  0           ['activation_154[0][0]',         \n",
            "                                                                  'activation_157[0][0]',         \n",
            "                                                                  'activation_162[0][0]',         \n",
            "                                                                  'activation_163[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_166 (Conv2D)            (None, 12, 12, 192)  147456      ['mixed7[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_166 (Batch  (None, 12, 12, 192)  576        ['conv2d_166[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_166 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_166[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_167 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_166[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_167 (Batch  (None, 12, 12, 192)  576        ['conv2d_167[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_167 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_167[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_164 (Conv2D)            (None, 12, 12, 192)  147456      ['mixed7[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_168 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_167[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_164 (Batch  (None, 12, 12, 192)  576        ['conv2d_164[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_168 (Batch  (None, 12, 12, 192)  576        ['conv2d_168[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_164 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_164[0][0]']\n",
            "                                                                                                  \n",
            " activation_168 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_168[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_165 (Conv2D)            (None, 5, 5, 320)    552960      ['activation_164[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_169 (Conv2D)            (None, 5, 5, 192)    331776      ['activation_168[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_165 (Batch  (None, 5, 5, 320)   960         ['conv2d_165[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_169 (Batch  (None, 5, 5, 192)   576         ['conv2d_169[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_165 (Activation)    (None, 5, 5, 320)    0           ['batch_normalization_165[0][0]']\n",
            "                                                                                                  \n",
            " activation_169 (Activation)    (None, 5, 5, 192)    0           ['batch_normalization_169[0][0]']\n",
            "                                                                                                  \n",
            " max_pooling2d_7 (MaxPooling2D)  (None, 5, 5, 768)   0           ['mixed7[0][0]']                 \n",
            "                                                                                                  \n",
            " mixed8 (Concatenate)           (None, 5, 5, 1280)   0           ['activation_165[0][0]',         \n",
            "                                                                  'activation_169[0][0]',         \n",
            "                                                                  'max_pooling2d_7[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_174 (Conv2D)            (None, 5, 5, 448)    573440      ['mixed8[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_174 (Batch  (None, 5, 5, 448)   1344        ['conv2d_174[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_174 (Activation)    (None, 5, 5, 448)    0           ['batch_normalization_174[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_171 (Conv2D)            (None, 5, 5, 384)    491520      ['mixed8[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_175 (Conv2D)            (None, 5, 5, 384)    1548288     ['activation_174[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_171 (Batch  (None, 5, 5, 384)   1152        ['conv2d_171[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_175 (Batch  (None, 5, 5, 384)   1152        ['conv2d_175[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_171 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_171[0][0]']\n",
            "                                                                                                  \n",
            " activation_175 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_175[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_172 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_171[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_173 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_171[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_176 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_175[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_177 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_175[0][0]']         \n",
            "                                                                                                  \n",
            " average_pooling2d_16 (AverageP  (None, 5, 5, 1280)  0           ['mixed8[0][0]']                 \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_170 (Conv2D)            (None, 5, 5, 320)    409600      ['mixed8[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_172 (Batch  (None, 5, 5, 384)   1152        ['conv2d_172[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_173 (Batch  (None, 5, 5, 384)   1152        ['conv2d_173[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_176 (Batch  (None, 5, 5, 384)   1152        ['conv2d_176[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_177 (Batch  (None, 5, 5, 384)   1152        ['conv2d_177[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_178 (Conv2D)            (None, 5, 5, 192)    245760      ['average_pooling2d_16[0][0]']   \n",
            "                                                                                                  \n",
            " batch_normalization_170 (Batch  (None, 5, 5, 320)   960         ['conv2d_170[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_172 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_172[0][0]']\n",
            "                                                                                                  \n",
            " activation_173 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_173[0][0]']\n",
            "                                                                                                  \n",
            " activation_176 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_176[0][0]']\n",
            "                                                                                                  \n",
            " activation_177 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_177[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_178 (Batch  (None, 5, 5, 192)   576         ['conv2d_178[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_170 (Activation)    (None, 5, 5, 320)    0           ['batch_normalization_170[0][0]']\n",
            "                                                                                                  \n",
            " mixed9_0 (Concatenate)         (None, 5, 5, 768)    0           ['activation_172[0][0]',         \n",
            "                                                                  'activation_173[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 5, 5, 768)    0           ['activation_176[0][0]',         \n",
            "                                                                  'activation_177[0][0]']         \n",
            "                                                                                                  \n",
            " activation_178 (Activation)    (None, 5, 5, 192)    0           ['batch_normalization_178[0][0]']\n",
            "                                                                                                  \n",
            " mixed9 (Concatenate)           (None, 5, 5, 2048)   0           ['activation_170[0][0]',         \n",
            "                                                                  'mixed9_0[0][0]',               \n",
            "                                                                  'concatenate_2[0][0]',          \n",
            "                                                                  'activation_178[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_183 (Conv2D)            (None, 5, 5, 448)    917504      ['mixed9[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_183 (Batch  (None, 5, 5, 448)   1344        ['conv2d_183[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_183 (Activation)    (None, 5, 5, 448)    0           ['batch_normalization_183[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_180 (Conv2D)            (None, 5, 5, 384)    786432      ['mixed9[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_184 (Conv2D)            (None, 5, 5, 384)    1548288     ['activation_183[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_180 (Batch  (None, 5, 5, 384)   1152        ['conv2d_180[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_184 (Batch  (None, 5, 5, 384)   1152        ['conv2d_184[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_180 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_180[0][0]']\n",
            "                                                                                                  \n",
            " activation_184 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_184[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_181 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_180[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_182 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_180[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_185 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_184[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_186 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_184[0][0]']         \n",
            "                                                                                                  \n",
            " average_pooling2d_17 (AverageP  (None, 5, 5, 2048)  0           ['mixed9[0][0]']                 \n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_179 (Conv2D)            (None, 5, 5, 320)    655360      ['mixed9[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization_181 (Batch  (None, 5, 5, 384)   1152        ['conv2d_181[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_182 (Batch  (None, 5, 5, 384)   1152        ['conv2d_182[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_185 (Batch  (None, 5, 5, 384)   1152        ['conv2d_185[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_186 (Batch  (None, 5, 5, 384)   1152        ['conv2d_186[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_187 (Conv2D)            (None, 5, 5, 192)    393216      ['average_pooling2d_17[0][0]']   \n",
            "                                                                                                  \n",
            " batch_normalization_179 (Batch  (None, 5, 5, 320)   960         ['conv2d_179[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_181 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_181[0][0]']\n",
            "                                                                                                  \n",
            " activation_182 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_182[0][0]']\n",
            "                                                                                                  \n",
            " activation_185 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_185[0][0]']\n",
            "                                                                                                  \n",
            " activation_186 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_186[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_187 (Batch  (None, 5, 5, 192)   576         ['conv2d_187[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_179 (Activation)    (None, 5, 5, 320)    0           ['batch_normalization_179[0][0]']\n",
            "                                                                                                  \n",
            " mixed9_1 (Concatenate)         (None, 5, 5, 768)    0           ['activation_181[0][0]',         \n",
            "                                                                  'activation_182[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 5, 5, 768)    0           ['activation_185[0][0]',         \n",
            "                                                                  'activation_186[0][0]']         \n",
            "                                                                                                  \n",
            " activation_187 (Activation)    (None, 5, 5, 192)    0           ['batch_normalization_187[0][0]']\n",
            "                                                                                                  \n",
            " mixed10 (Concatenate)          (None, 5, 5, 2048)   0           ['activation_179[0][0]',         \n",
            "                                                                  'mixed9_1[0][0]',               \n",
            "                                                                  'concatenate_3[0][0]',          \n",
            "                                                                  'activation_187[0][0]']         \n",
            "                                                                                                  \n",
            " global_average_pooling2d_1 (Gl  (None, 2048)        0           ['mixed10[0][0]']                \n",
            " obalAveragePooling2D)                                                                            \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 1024)         2098176     ['global_average_pooling2d_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 256)          262400      ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 64)           16448       ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 32)           2080        ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 1)            33          ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 24,181,921\n",
            "Trainable params: 4,756,865\n",
            "Non-trainable params: 19,425,056\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = my_inc_speed.fit_generator(image_data_generator(x_train_speed, y_train_speed, batch_size=50),\n",
        "                              steps_per_epoch=500, epochs=20,\n",
        "                              validation_data = image_data_generator(x_valid_speed,y_valid_speed, batch_size=50),\n",
        "                              validation_steps=500,\n",
        "                              verbose=1, shuffle=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bbbd7cb-711e-42d6-dd03-f945f6d6ea3d",
        "id": "xMUb1hqdfrXs"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "500/500 [==============================] - 257s 506ms/step - loss: 0.1226 - mse: 0.1226 - val_loss: 0.0680 - val_mse: 0.0680\n",
            "Epoch 2/20\n",
            "500/500 [==============================] - 250s 501ms/step - loss: 0.0554 - mse: 0.0554 - val_loss: 0.0459 - val_mse: 0.0459\n",
            "Epoch 3/20\n",
            "500/500 [==============================] - 250s 501ms/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.0374 - val_mse: 0.0374\n",
            "Epoch 4/20\n",
            "500/500 [==============================] - 250s 502ms/step - loss: 0.0307 - mse: 0.0307 - val_loss: 0.0335 - val_mse: 0.0335\n",
            "Epoch 5/20\n",
            "500/500 [==============================] - 250s 500ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0291 - val_mse: 0.0291\n",
            "Epoch 6/20\n",
            "500/500 [==============================] - 250s 500ms/step - loss: 0.0206 - mse: 0.0206 - val_loss: 0.0276 - val_mse: 0.0276\n",
            "Epoch 7/20\n",
            "500/500 [==============================] - 268s 537ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.0245 - val_mse: 0.0245\n",
            "Epoch 8/20\n",
            "500/500 [==============================] - 252s 504ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.0235 - val_mse: 0.0235\n",
            "Epoch 9/20\n",
            "500/500 [==============================] - 252s 505ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0212 - val_mse: 0.0212\n",
            "Epoch 10/20\n",
            "500/500 [==============================] - 251s 504ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0211 - val_mse: 0.0211\n",
            "Epoch 11/20\n",
            "500/500 [==============================] - 252s 504ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.0189 - val_mse: 0.0189\n",
            "Epoch 12/20\n",
            "500/500 [==============================] - 253s 507ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0197 - val_mse: 0.0197\n",
            "Epoch 13/20\n",
            "500/500 [==============================] - 252s 506ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0177 - val_mse: 0.0177\n",
            "Epoch 14/20\n",
            "500/500 [==============================] - 252s 505ms/step - loss: 0.0080 - mse: 0.0080 - val_loss: 0.0191 - val_mse: 0.0191\n",
            "Epoch 15/20\n",
            "500/500 [==============================] - 253s 506ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0171 - val_mse: 0.0171\n",
            "Epoch 16/20\n",
            "500/500 [==============================] - 252s 505ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0177 - val_mse: 0.0177\n",
            "Epoch 17/20\n",
            "500/500 [==============================] - 252s 505ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0163 - val_mse: 0.0163\n",
            "Epoch 18/20\n",
            "500/500 [==============================] - 252s 505ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0162 - val_mse: 0.0162\n",
            "Epoch 19/20\n",
            "500/500 [==============================] - 252s 505ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0162 - val_mse: 0.0162\n",
            "Epoch 20/20\n",
            "500/500 [==============================] - 252s 505ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0165 - val_mse: 0.0165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_speed[:10])\n",
        "print(y_train_speed[:10])\n",
        "print(x_valid_speed[:10])\n",
        "print(y_valid_speed[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RmDzb4fjLfl",
        "outputId": "c736d345-ca5e-4fab-bd68-262c56ab99a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/data/training_data/training_data/5600.png', '/content/data/training_data/training_data/11472.png', '/content/data/training_data/training_data/1128.png', '/content/data/training_data/training_data/13058.png', '/content/data/training_data/training_data/8571.png', '/content/data/training_data/training_data/11016.png', '/content/data/training_data/training_data/12178.png', '/content/data/training_data/training_data/6116.png', '/content/data/training_data/training_data/6253.png', '/content/data/training_data/training_data/2403.png']\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]\n",
            "['/content/data/training_data/training_data/4304.png', '/content/data/training_data/training_data/11321.png', '/content/data/training_data/training_data/7256.png', '/content/data/training_data/training_data/2099.png', '/content/data/training_data/training_data/7138.png', '/content/data/training_data/training_data/9053.png', '/content/data/training_data/training_data/4043.png', '/content/data/training_data/training_data/9708.png', '/content/data/training_data/training_data/10065.png', '/content/data/training_data/training_data/6320.png']\n",
            "[0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "my_inc.save('/content/gdrive/My Drive')"
      ],
      "metadata": {
        "id": "7JCoqToDu7dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_path = '/content/data/test_data/test_data'"
      ],
      "metadata": {
        "id": "Ay8ANASvwWov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corrupted_image_test = []\n",
        "\n",
        "for filename in glob.iglob(testing_path + '**/*.png', recursive=True):\n",
        "    try:\n",
        "        im = Image.open(filename)\n",
        "        im = im.resize((224,224), Image.ANTIALIAS)\n",
        "        im.save(filename , 'png', quality=90)\n",
        "    except:\n",
        "        corrupted_image_test.append(filename)\n"
      ],
      "metadata": {
        "id": "6TVWGyxkcJqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corrupted_image_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZi9mqV-cRec",
        "outputId": "ed21900d-7f05-4900-e291-da36fa717691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_path_test = []\n",
        "import os, sys\n",
        "import glob\n",
        "for filename in glob.iglob(testing_path + '**/*.png', recursive=True):\n",
        "    if filename not in corrupted_image_test:\n",
        "        img_path_test.append(filename)\n",
        "print(len(img_path_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Onztp6dTc-_L",
        "outputId": "b6701b95-8165-4421-9608-650ed393d760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "result_speed_1 = []\n",
        "for i in img_path_test:\n",
        "    img = process_image(i);\n",
        "    img = img.reshape(1,224,224,3)\n",
        "    res_speed = my_inc_speed.predict(img)\n",
        "    result_speed_1.append(res_speed)"
      ],
      "metadata": {
        "id": "KyZK1dGD57bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_result_speed = list(map(lambda x: x.tolist(), result_speed_1))\n",
        "final_result_speed = [item for sublist in final_result_speed for item in sublist]\n",
        "print(final_result_speed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeRGPe1Y6dKa",
        "outputId": "48451a0f-f7b8-461c-b4b7-115b873ecdcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.9908283948898315], [0.9999920129776001], [0.9799683094024658], [0.9988290667533875], [0.9706987738609314], [0.998714804649353], [0.9994538426399231], [0.18334242701530457], [0.9979166388511658], [0.008086180314421654], [0.060349080711603165], [0.9986863732337952], [0.003532665316015482], [0.002943455008789897], [0.7315493822097778], [0.9546142816543579], [0.5762920379638672], [0.23579847812652588], [0.0013195887440815568], [0.0033307704143226147], [0.99250727891922], [0.22234539687633514], [0.0007638867828063667], [0.9882943630218506], [0.9956953525543213], [0.003460899693891406], [0.9998883008956909], [0.0006355594377964735], [0.0009612221037968993], [0.01993577368557453], [0.99958735704422], [0.08928424119949341], [0.9653856754302979], [0.0026106154546141624], [0.025209877640008926], [0.0005579603603109717], [0.013730266131460667], [0.02096942812204361], [0.00364180956967175], [0.8326734304428101], [0.030933517962694168], [0.0017451229505240917], [0.00684500252828002], [0.0021171383559703827], [0.9964795708656311], [0.007341043557971716], [0.9898348450660706], [0.02535073272883892], [0.0009464081376791], [0.0022049115505069494], [0.9338730573654175], [0.9990155696868896], [0.007694809697568417], [0.9603273272514343], [0.37723127007484436], [0.9997344613075256], [0.0049406192265450954], [0.989174485206604], [0.0030365572310984135], [0.0053695314563810825], [0.9980937838554382], [0.9902528524398804], [0.9565073847770691], [0.9956010580062866], [0.9982410669326782], [0.9999337196350098], [0.987633466720581], [0.9876866936683655], [0.011258332058787346], [0.001560034230351448], [0.004408026579767466], [0.9986777901649475], [0.996906578540802], [0.006026286166161299], [0.021341916173696518], [0.008094021119177341], [0.987767219543457], [0.13284559547901154], [0.9965307116508484], [0.9686938524246216], [0.0038407170213758945], [0.0027846579905599356], [0.9590548276901245], [0.36665990948677063], [0.9712264537811279], [0.9805966019630432], [0.007119820918887854], [0.9927484393119812], [0.9976581335067749], [0.9999035596847534], [0.9991269707679749], [0.9264127612113953], [0.9920722842216492], [0.0025998433120548725], [0.12703323364257812], [0.005461052991449833], [0.01233484037220478], [0.004842802416533232], [0.7698250412940979], [0.9926679730415344], [0.09775473922491074], [0.8154700994491577], [0.994070827960968], [0.9981400966644287], [0.011552039533853531], [0.0017871939344331622], [0.0012634041486307979], [0.9997010827064514], [0.9949280023574829], [0.9064077734947205], [0.0021842923015356064], [0.9994262456893921], [0.8646106123924255], [0.998446524143219], [0.007377070840448141], [0.01688208617269993], [0.0009616690804250538], [0.045156363397836685], [0.9875268340110779], [0.04335813224315643], [0.0006108571542426944], [0.9343487024307251], [0.9617703557014465], [0.9809462428092957], [0.007848436944186687], [0.013330521062016487], [0.004486175253987312], [0.9998173117637634], [0.0006782735581509769], [0.0018349102465435863], [0.013865948654711246], [0.592159628868103], [0.22722414135932922], [0.016917847096920013], [0.9994221925735474], [0.0005106099415570498], [0.7079273462295532], [0.9995419979095459], [0.9977388381958008], [0.24619805812835693], [0.9672341346740723], [0.9291187524795532], [0.0018071745289489627], [0.008295378647744656], [0.009074320085346699], [0.9929969906806946], [0.9998098015785217], [0.043422624468803406], [0.8960655331611633], [0.09179080277681351], [0.0033956163097172976], [0.8548586964607239], [0.9981410503387451], [0.008872277103364468], [0.009863492101430893], [0.00851178914308548], [0.2608589231967926], [0.9990123510360718], [0.9996210336685181], [0.9940299987792969], [0.03575650975108147], [0.0027846593875437975], [0.0010705699678510427], [0.009106365963816643], [0.040878474712371826], [0.08622431010007858], [0.0027390550822019577], [0.9955639839172363], [0.9786813855171204], [0.006403423845767975], [0.9991545677185059], [0.9996187686920166], [0.00682494044303894], [0.004348982125520706], [0.003831971902400255], [0.4373491108417511], [0.9999861717224121], [0.9933251738548279], [0.9655902981758118], [0.01753098890185356], [0.06561505049467087], [0.016642019152641296], [0.8958053588867188], [0.9990265369415283], [0.9994472861289978], [0.8582574129104614], [0.04956604912877083], [0.9993941783905029], [0.9953204989433289], [0.2923450171947479], [0.02658182755112648], [0.999919056892395], [0.9967825412750244], [0.004588328301906586], [0.0025189260486513376], [0.03947439044713974], [0.8941898941993713], [0.0011626429622992873], [0.9969618916511536], [0.8701128363609314], [0.9999794960021973], [0.007300288882106543], [0.941389799118042], [0.9966676831245422], [0.0012628037948161364], [0.9967273473739624], [0.0035817334428429604], [0.0032319361343979836], [0.29203328490257263], [0.013272819109261036], [0.06079152598977089], [0.9996598958969116], [0.9910116195678711], [0.9843929409980774], [0.020107107236981392], [0.013225173577666283], [0.9946280121803284], [0.9804091453552246], [0.004180870018899441], [0.008921770378947258], [0.9980534315109253], [0.002347688190639019], [0.9783809781074524], [0.007033245638012886], [0.03279048949480057], [0.25859224796295166], [0.003088937373831868], [0.00210610986687243], [0.00505729578435421], [0.9990711212158203], [0.94887775182724], [0.9999046325683594], [0.9924350380897522], [0.38725465536117554], [0.9993005990982056], [0.003947639372199774], [0.0017532602651044726], [0.9996271133422852], [0.023927727714180946], [0.022243838757276535], [0.9995121955871582], [0.0010727988556027412], [0.9975796341896057], [0.9998273849487305], [0.0025920846965163946], [0.0015052832895889878], [0.9894578456878662], [0.9974150657653809], [0.875560998916626], [0.9864328503608704], [0.0019048767862841487], [0.002635252196341753], [0.9586343169212341], [0.0032678982242941856], [0.9347420334815979], [0.9940385818481445], [0.6990777254104614], [0.04726743698120117], [0.9695144891738892], [0.9987702965736389], [0.9776662588119507], [0.9993581175804138], [0.7822357416152954], [0.13878023624420166], [0.004139299504458904], [0.0025469614192843437], [0.9938170313835144], [0.0024330217856913805], [0.0026618456467986107], [0.9964660406112671], [0.977286696434021], [0.008409474045038223], [0.0038940678350627422], [0.9998677968978882], [0.9997038245201111], [0.5746594667434692], [0.7251064777374268], [0.999240517616272], [0.003264943603426218], [0.9931199550628662], [0.893727719783783], [0.004909899551421404], [0.004093880299478769], [0.9997348189353943], [0.9899027347564697], [0.0015884663444012403], [0.009520313702523708], [0.004774606321007013], [0.853585422039032], [0.9993051290512085], [0.9999425411224365], [0.0016985678812488914], [0.9228010177612305], [0.008707074448466301], [0.9953730702400208], [0.0068272738717496395], [0.011040303856134415], [0.008105571381747723], [0.0018810943001881242], [0.9991507530212402], [0.5210197567939758], [0.9996652603149414], [0.9999178647994995], [0.0097611453384161], [0.04657500609755516], [0.8519645929336548], [0.0016092936275526881], [0.9990144968032837], [0.9225625991821289], [0.9890856146812439], [0.004165193531662226], [0.9971864819526672], [0.019933294504880905], [0.18152765929698944], [0.9868934154510498], [0.002171721076592803], [0.0032261409796774387], [0.0018666444811969995], [0.8855225443840027], [0.9856008887290955], [0.8322543501853943], [0.8341822028160095], [0.004523993004113436], [0.9983137845993042], [0.004155478440225124], [0.0007483348599635065], [0.9990230798721313], [0.01420227624475956], [0.9912228584289551], [0.988445520401001], [0.4489958882331848], [0.0047865682281553745], [0.9992017149925232], [0.8115590810775757], [0.9872272610664368], [0.012960006482899189], [0.9839265942573547], [0.9999279975891113], [0.9860897064208984], [0.9990829229354858], [0.7526168823242188], [0.9991932511329651], [0.4256918132305145], [0.9995231628417969], [0.9829379916191101], [0.9963186979293823], [0.9999116659164429], [0.8663927912712097], [0.0036050721537321806], [0.9963381290435791], [0.9921389222145081], [0.001085416879504919], [0.0027607136871665716], [0.0024624657817184925], [0.00404259841889143], [0.019511474296450615], [0.9935744404792786], [0.9998905658721924], [0.9994375109672546], [0.9117327928543091], [0.0012226059334352612], [0.9985758066177368], [0.9996906518936157], [0.9970884919166565], [0.9589943289756775], [0.12346534430980682], [0.012499954551458359], [0.003844685386866331], [0.9980935454368591], [0.07793443650007248], [0.9991764426231384], [0.9904844760894775], [0.9978317618370056], [0.005581060890108347], [0.9968200922012329], [0.9122390151023865], [0.9674323797225952], [0.0018362401751801372], [0.13342811167240143], [0.002688809996470809], [0.9437410235404968], [0.005293469410389662], [0.00632402952760458], [0.9989069700241089], [0.6182605028152466], [0.11196868866682053], [0.8745407462120056], [0.03366793692111969], [0.9755512475967407], [0.9944962859153748], [0.1499498188495636], [0.0678824782371521], [0.9982959628105164], [0.001352502848021686], [0.9991877675056458], [0.07350717484951019], [0.9951357245445251], [0.2429962009191513], [0.0009066176135092974], [0.9981493949890137], [0.9855337738990784], [0.00370732881128788], [0.9668893218040466], [0.9955624341964722], [0.9996795654296875], [0.9830350875854492], [0.0021980509627610445], [0.9577822089195251], [0.9971018433570862], [0.004157364834100008], [0.15996241569519043], [0.9996824264526367], [0.9883878827095032], [0.9763234257698059], [0.011536656878888607], [0.003042109776288271], [0.5559903979301453], [0.004593634977936745], [0.0024585204664617777], [0.9975948929786682], [0.7996676564216614], [0.005593112204223871], [0.007612927351146936], [0.9551988840103149], [0.9950856566429138], [0.9979775547981262], [0.0033051352947950363], [0.9685381650924683], [0.029565682634711266], [0.9492502212524414], [0.0022388624493032694], [0.9981576800346375], [0.8974165320396423], [0.00435608858242631], [0.8235316872596741], [0.9998527765274048], [0.5253626704216003], [0.9997972846031189], [0.9985501170158386], [0.007079045753926039], [0.9987233281135559], [0.019773773849010468], [0.002165165264159441], [0.99953293800354], [0.9855260848999023], [0.9976624250411987], [0.030361393466591835], [0.012697134166955948], [0.9118853211402893], [0.015291793271899223], [0.9699617028236389], [0.022288529202342033], [0.009786795824766159], [0.0010824166238307953], [0.01953187771141529], [0.0022732149809598923], [0.015853000804781914], [0.07614416629076004], [0.004063761793076992], [0.9986494183540344], [0.9990279674530029], [0.007825364358723164], [0.9992552399635315], [0.0007889309781603515], [0.010751388967037201], [0.1942068189382553], [0.9991406202316284], [0.008576438762247562], [0.017072325572371483], [0.9562444686889648], [0.93382728099823], [0.9973134398460388], [0.9997755885124207], [0.9997259974479675], [0.026661835610866547], [0.9977948665618896], [0.9994389414787292], [0.9974318146705627], [0.9995249509811401], [0.008116885088384151], [0.060620103031396866], [0.9988210797309875], [0.9978408813476562], [0.9886194467544556], [0.7492983937263489], [0.0024766370188444853], [0.9987776875495911], [0.004616405814886093], [0.9996927976608276], [0.02345011942088604], [0.9942270517349243], [0.9789228439331055], [0.00037333372165448964], [0.9693424105644226], [0.05165870860219002], [0.9985172152519226], [0.9998575448989868], [0.9868708848953247], [0.804039716720581], [0.003629069309681654], [0.006856556050479412], [0.0016842768527567387], [0.007157821673899889], [0.9818668365478516], [0.004219146445393562], [0.005070538260042667], [0.017071038484573364], [0.002298341365531087], [0.9980329871177673], [0.24735142290592194], [0.2307426929473877], [0.0017445434350520372], [0.035425521433353424], [0.9782198667526245], [0.8856454491615295], [0.028591418638825417], [0.941341757774353], [0.01756405085325241], [0.004605978727340698], [0.005400720983743668], [0.99685138463974], [0.00701597286388278], [0.007608601823449135], [0.9739438891410828], [0.017747119069099426], [0.9991424083709717], [0.0073526096530258656], [0.0016948605189099908], [0.003808835754171014], [0.9513658285140991], [0.9965716600418091], [0.0006125009385868907], [0.997281551361084], [0.9100494384765625], [0.005801662802696228], [0.05337681993842125], [0.9936876893043518], [0.0006574796861968935], [0.006858936510980129], [0.9990863800048828], [0.9984766840934753], [0.008994405157864094], [0.9873512387275696], [0.007095810025930405], [0.8533986806869507], [0.994517982006073], [0.980536699295044], [0.9873433113098145], [0.9964325428009033], [0.9977446794509888], [0.9998520612716675], [0.999894380569458], [0.5540999174118042], [0.5400951504707336], [0.003554425435140729], [0.0281536728143692], [0.9992503523826599], [0.9253029823303223], [0.9978870749473572], [0.9982842803001404], [0.04971349239349365], [0.08559747040271759], [0.9266459345817566], [0.0003278120420873165], [0.9965335130691528], [0.7497488260269165], [0.0009412706713192165], [0.07494202256202698], [0.7001338601112366], [0.0034566447138786316], [0.5275905132293701], [0.9854884743690491], [0.00191121909301728], [0.0006202836520969868], [0.02047908306121826], [0.018970908597111702], [0.9942219853401184], [0.8974176645278931], [0.9999977350234985], [0.9730209112167358], [0.9995802044868469], [0.004556213039904833], [0.98827064037323], [0.9750822186470032], [0.9978920817375183], [0.006455148104578257], [0.0239933542907238], [0.016444575041532516], [0.0030455118976533413], [0.9988529682159424], [0.46601566672325134], [0.7263445258140564], [0.0005304412916302681], [0.03140271455049515], [0.0016021692426875234], [0.003618278307840228], [0.9986308217048645], [0.9997664093971252], [0.9962443113327026], [0.0006261064554564655], [0.07698331028223038], [0.9983484745025635], [0.988321840763092], [0.9999301433563232], [0.9982043504714966], [0.00989087950438261], [0.004654749762266874], [0.004806095268577337], [0.002209541853517294], [0.6070259213447571], [0.0015694788889959455], [0.09184753894805908], [0.00490118283778429], [0.0017292882548645139], [0.08334458619356155], [0.948827862739563], [0.11261548846960068], [0.16106267273426056], [0.9742727279663086], [0.916111946105957], [0.003902008756995201], [0.9838449358940125], [0.998609185218811], [0.9999638795852661], [0.99978107213974], [0.6610586643218994], [0.003343378659337759], [0.9988208413124084], [0.025809861719608307], [0.9998247027397156], [0.9292943477630615], [0.029540924355387688], [0.9061431288719177], [0.6648449897766113], [0.9910001158714294], [0.003949968609958887], [0.9997722506523132], [0.11863680183887482], [0.0020616149995476007], [0.0022187044378370047], [0.002997073344886303], [0.9980948567390442], [0.00410799402743578], [0.0032685273326933384], [0.9937513470649719], [0.999629020690918], [0.0008514292421750724], [0.9915478825569153], [0.002932601375505328], [0.9676821231842041], [0.023236382752656937], [0.9969764947891235], [0.04785563424229622], [0.12026423960924149], [0.03706146031618118], [0.9798327684402466], [0.9660603404045105], [0.0008074070792645216], [0.9969274401664734], [0.003468324663117528], [0.9961215853691101], [0.9988611936569214], [0.06408065557479858], [0.9941562414169312], [0.016745882108807564], [0.9890833497047424], [0.999015212059021], [0.0011738273315131664], [0.0023456295020878315], [0.9997422099113464], [0.9973483085632324], [0.20721852779388428], [0.9999498128890991], [0.9872003793716431], [0.9981945157051086], [0.9830358028411865], [0.9991886019706726], [0.9998098015785217], [0.0020290608517825603], [0.022087231278419495], [0.9955768585205078], [0.00820352602750063], [0.001997381914407015], [0.9933792352676392], [0.9801552891731262], [0.9784184098243713], [0.4675925374031067], [0.0036661929916590452], [0.9955756664276123], [0.9153379201889038], [0.0016997713828459382], [0.0021792857442051172], [0.0031418665312230587], [0.9932541251182556], [0.997733473777771], [0.9578893780708313], [0.0028756505344063044], [0.0017738372553139925], [0.9998960494995117], [0.0032556497026234865], [0.7988604307174683], [0.9999616146087646], [0.9999246597290039], [0.0028276850935071707], [0.10931191593408585], [0.006788758561015129], [0.006639814004302025], [0.008348377421498299], [0.024688055738806725], [0.9939186573028564], [0.921747088432312], [0.9997207522392273], [0.9996317625045776], [0.9937712550163269], [0.00724633177742362], [0.05026920139789581], [0.9957055449485779], [0.009710654616355896], [0.013537388294935226], [0.8308359384536743], [0.011464504525065422], [0.13811145722866058], [0.9976876974105835], [0.9940219521522522], [0.9994168281555176], [0.9911618232727051], [0.8668370842933655], [0.03677401691675186], [0.9844470024108887], [0.000611168215982616], [0.009227121248841286], [0.9997478127479553], [0.008680309168994427], [0.023584362119436264], [0.9989926218986511], [0.007837019860744476], [0.9971998929977417], [0.004086090251803398], [0.9894425868988037], [0.0025774314999580383], [0.0069437227211892605], [0.06716465204954147], [0.00586297269910574], [0.9826598167419434], [0.012201222591102123], [0.9783605337142944], [0.008417705073952675], [0.9762864708900452], [0.9990395903587341], [0.0011765760136768222], [0.018931949511170387], [0.9969820380210876], [0.9870742559432983], [0.9661155939102173], [0.0011142832227051258], [0.0010774374241009355], [0.9984731078147888], [0.9940975904464722], [0.016282830387353897], [0.9999786615371704], [0.006480678915977478], [0.03572062402963638], [0.999351441860199], [0.005458518397063017], [0.005530491471290588], [0.9965265393257141], [0.0148344486951828], [0.09372570365667343], [0.9975192546844482], [0.9948615431785583], [0.007214909419417381], [0.13951338827610016], [0.999377965927124], [0.0639493465423584], [0.000878318096511066], [0.00134579511359334], [0.003355254651978612], [0.0057643442414700985], [0.029947219416499138], [0.0024165152572095394], [0.999996542930603], [0.0027603430207818747], [0.4296964108943939], [0.9993719458580017], [0.11295369267463684], [0.004749617539346218], [0.026063917204737663], [0.9994188547134399], [0.9565281867980957], [0.045322880148887634], [0.9836005568504333], [0.005535915493965149], [0.9986019730567932], [0.9114900827407837], [0.004612862132489681], [0.014450786635279655], [0.9916362762451172], [0.010239692404866219], [0.0026701856404542923], [0.9445120096206665], [0.9087283611297607], [0.028178920969367027], [0.9995194673538208], [0.9666118025779724], [0.00557665852829814], [0.997965931892395], [0.9994310736656189], [0.972153902053833], [0.992533802986145], [0.020184295251965523], [0.01311462838202715], [0.9994564652442932], [0.0016644916031509638], [0.003123240312561393], [0.9933620691299438], [0.8023386001586914], [0.0009714057669043541], [0.9959473013877869], [0.9206589460372925], [0.8040315508842468], [0.9994331002235413], [0.9996002316474915], [0.5796210169792175], [0.9984838366508484], [0.9647546410560608], [0.006278456188738346], [0.9876091480255127], [0.9725932478904724], [0.9957919120788574], [0.00280065112747252], [0.07469010353088379], [0.9990298748016357], [0.017690509557724], [0.9996138215065002], [0.9957362413406372], [0.056180439889431], [0.04592597112059593], [0.9906992316246033], [0.0013186861760914326], [0.0033533242531120777], [0.017787370830774307], [0.9959567189216614], [0.07721488177776337], [0.0029486455023288727], [0.9607685804367065], [0.9959855675697327], [0.002890773816034198], [0.9898936152458191], [0.0023354992736130953], [0.0025074875447899103], [0.016753695905208588], [0.0025457392912358046], [0.5379832983016968], [0.9998231530189514], [0.0030008030589669943], [0.006130623631179333], [0.0038688178174197674], [0.0053022014908492565], [0.997916042804718], [0.007471915800124407], [0.008962608873844147], [0.0026001352816820145], [0.007233969401568174], [0.9971861243247986], [0.01512722484767437], [0.9932175278663635], [0.9994909763336182], [0.0917799174785614], [0.9871122241020203], [0.0108663160353899], [0.009033113718032837], [0.0032406067475676537], [0.9878652691841125], [0.7620154023170471], [0.9998525381088257], [0.40162283182144165], [0.9727398753166199], [0.09106915444135666], [0.4525585472583771], [0.9873974919319153], [0.9356604814529419], [0.9975479245185852], [0.0025896020233631134], [0.0031950424890965223], [0.022347060963511467], [0.012201652862131596], [0.9801111817359924], [0.0032954595517367125], [0.0028656444046646357], [0.0031850123777985573], [0.0333421416580677], [0.8748140931129456], [0.013124327175319195], [0.0020232717506587505], [0.8547453284263611], [0.9816449880599976], [0.9922025203704834], [0.48064473271369934], [0.0025904655922204256], [0.9977635145187378], [0.34827983379364014], [0.0018555455608293414], [0.9979345798492432], [0.0019039278849959373], [0.05657303333282471], [0.03266378864645958], [0.002438848838210106], [0.9857520461082458], [0.9957459568977356], [0.993374764919281], [0.12806643545627594], [0.9974895715713501], [0.9182602167129517], [0.9902338981628418], [0.01706859841942787], [0.4494684636592865], [0.998806357383728], [0.17833825945854187], [0.9759662747383118], [0.9890523552894592], [0.9167034029960632], [0.9671705961227417], [0.9990845918655396], [0.16255557537078857], [0.9941097497940063], [0.04112089425325394], [0.9931038022041321], [0.9549942016601562], [0.10562627762556076], [0.6567661166191101], [0.9981406927108765], [0.900854229927063], [0.017289193347096443], [0.994586706161499], [0.004261604510247707], [0.003906871657818556], [0.999445378780365], [0.005891054403036833], [0.009102598764002323], [0.0008185617043636739], [0.0036135679110884666], [0.9843722581863403], [0.004650960210710764], [0.9935160875320435], [0.010350191965699196], [0.0056554339826107025], [0.9822485446929932], [0.9971158504486084], [0.9976535439491272], [0.010802233591675758], [0.016532355919480324], [0.006712174508720636], [0.013682249933481216], [0.9788901805877686], [0.009558167308568954], [0.026710795238614082], [0.9998525381088257], [0.028391020372509956], [0.0017287344671785831], [0.0007716085528954864], [0.8311597108840942], [0.9979314804077148], [0.02708653174340725], [0.9973289966583252], [0.9576942920684814], [0.004729648120701313], [0.7045549750328064], [0.0010923148365691304], [0.6463426947593689], [0.9736014008522034], [0.6678224802017212], [0.9733861684799194], [0.00831175222992897], [0.9986227750778198], [0.001099908142350614], [0.9990523457527161], [0.03955051675438881], [0.9588144421577454], [0.8061308264732361], [0.012976400554180145], [0.9998986721038818], [0.9996740818023682], [0.9923096895217896], [0.009715530090034008], [0.7797040343284607], [0.008901997469365597], [0.9982613921165466], [0.9761649370193481], [0.9856461882591248], [0.9359145760536194], [0.989852249622345], [0.9957279562950134], [0.0010408969828858972], [0.002983197569847107], [0.003133270191028714], [0.0320146419107914], [0.9900590777397156], [0.9994434714317322], [0.9833889603614807], [0.9982115030288696], [0.9996218681335449], [0.053544364869594574], [0.9808356761932373], [0.14937232434749603], [0.0012573039857670665], [0.9996461868286133], [0.0015833164798095822], [0.9961209893226624], [0.009338608011603355], [0.9998195767402649], [0.997320830821991], [0.5165824890136719], [0.05605880916118622], [0.9891658425331116], [0.013151727616786957], [0.9996298551559448], [0.008560799062252045], [0.999913215637207], [0.9847704172134399], [0.9963561296463013]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "image_ids = []\n",
        "result_angle = []\n",
        "result_speed = []\n",
        "for i in img_path_test:\n",
        "    img = process_image(i);\n",
        "    i_id = int(str(i).split('/')[-1].split('.')[0])\n",
        "    img = img.reshape(1,224,224,3)\n",
        "    res_angle = my_inc_angle.predict(img)\n",
        "    res_speed = my_inc_speed.predict(img)\n",
        "    result_angle.append(res_angle)\n",
        "    result_speed.append(res_speed)\n",
        "    image_ids.append(i_id)\n"
      ],
      "metadata": {
        "id": "A9CR3EPNYvd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_result_angle = list(map(lambda x: x.tolist(), result_angle))\n",
        "final_result_angle = [item for sublist in final_result_angle for item in sublist]\n",
        "final_result_angle = [item for sublist in final_result_angle for item in sublist]\n",
        "print(final_result_angle)\n",
        "final_result_speed = list(map(lambda x: x.tolist(), result_speed))\n",
        "final_result_speed = [item for sublist in final_result_speed for item in sublist]\n",
        "final_result_speed = [item for sublist in final_result_speed for item in sublist]\n",
        "print(final_result_speed)\n",
        "print(image_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4I9hU5dY4UQp",
        "outputId": "34ec9fc2-5101-44d8-c671-50ec269cf05e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.3408866226673126, 0.6262978315353394, 0.15645389258861542, 0.5725411176681519, 0.42492151260375977, 0.5881374478340149, 0.7401219010353088, 0.711682915687561, 0.4291408956050873, 0.6880265474319458, 0.6716923713684082, 0.6695213913917542, 0.7004832029342651, 0.5666773915290833, 0.6837525963783264, 0.44735288619995117, 0.6923030614852905, 0.638121485710144, 0.7840225696563721, 0.7023571133613586, 0.2243773192167282, 0.7049775123596191, 0.7131773233413696, 0.48401129245758057, 0.4850512146949768, 0.529827892780304, 0.4897163510322571, 0.7369695901870728, 0.6334044337272644, 0.5008676052093506, 0.7917647361755371, 0.5836697220802307, 0.7544696927070618, 0.462216317653656, 0.8158219456672668, 0.628065824508667, 0.5483233332633972, 0.41509580612182617, 0.4787461757659912, 0.4963187873363495, 0.5884669423103333, 0.7090204954147339, 0.6830455660820007, 0.6109650135040283, 0.4531325399875641, 0.5117426514625549, 0.6047192215919495, 0.4145357012748718, 0.5294594764709473, 0.5062023997306824, 0.3099556267261505, 0.376986563205719, 0.7920366525650024, 0.2861138582229614, 0.7178508043289185, 0.910324215888977, 0.5785440802574158, 0.5137394666671753, 0.6324615478515625, 0.7648818492889404, 0.8266444802284241, 0.2903163433074951, 0.5146399736404419, 0.42009106278419495, 0.37627387046813965, 0.8255980014801025, 0.838879406452179, 0.148062065243721, 0.679146409034729, 0.569416344165802, 0.6847528219223022, 0.2942829728126526, 0.4535292983055115, 0.6108884215354919, 0.531153678894043, 0.6379901766777039, 0.6545823812484741, 0.41968557238578796, 0.5739076137542725, 0.3574150800704956, 0.7060225605964661, 0.5245170593261719, 0.7878690361976624, 0.8001846075057983, 0.522911012172699, 0.12556159496307373, 0.593502402305603, 0.4403168559074402, 0.5634891390800476, 0.6057164669036865, 0.461117148399353, 0.5031660795211792, 0.4292624592781067, 0.6789510250091553, 0.8222852349281311, 0.5830272436141968, 0.41005000472068787, 0.7574463486671448, 0.5603049397468567, 0.4129776954650879, 0.6039104461669922, 0.2546265125274658, 0.5727422833442688, 0.611407995223999, 0.6257794499397278, 0.5855700373649597, 0.436515212059021, 0.7226290106773376, 0.13457730412483215, 0.31795358657836914, 0.5901165008544922, 0.7009931802749634, 0.3448024392127991, 0.6661230325698853, 0.7961302399635315, 0.6934842467308044, 0.5506685972213745, 0.6572931408882141, 0.16442175209522247, 0.7482767701148987, 0.5021581649780273, 0.547914445400238, 0.7813405990600586, 0.4601515531539917, 0.6404411792755127, 0.4682888984680176, 0.657315194606781, 0.6017988920211792, 0.7392262816429138, 0.6953995227813721, 0.612352728843689, 0.5441389083862305, 0.7479299306869507, 0.45431578159332275, 0.6991649866104126, 0.8146026730537415, 0.0630403533577919, 0.6852836608886719, 0.4008089303970337, 0.5390770435333252, 0.25569814443588257, 0.3352213203907013, 0.700641393661499, 0.6899614334106445, 0.6198422908782959, 0.42023196816444397, 0.5258965492248535, 0.6223545670509338, 0.29822397232055664, 0.5106616020202637, 0.7081048488616943, 0.259050577878952, 0.1309828907251358, 0.8346943855285645, 0.7087069749832153, 0.7372740507125854, 0.5539751648902893, 0.4098522663116455, 0.676505982875824, 0.8697718381881714, 0.5193405747413635, 0.7417724132537842, 0.5431324243545532, 0.6015090942382812, 0.5320267677307129, 0.5824006795883179, 0.47541892528533936, 0.4377707839012146, 0.20911380648612976, 0.516588032245636, 0.5390604734420776, 0.48182058334350586, 0.5763664841651917, 0.5912125706672668, 0.4820694923400879, 0.16643355786800385, 0.7254918217658997, 0.42216411232948303, 0.6937204599380493, 0.5993779897689819, 0.5804803371429443, 0.506808876991272, 0.9136718511581421, 0.45905911922454834, 0.8100725412368774, 0.357128769159317, 0.5152937769889832, 0.5354467034339905, 0.7054586410522461, 0.5422089099884033, 0.34711506962776184, 0.7009027600288391, 0.13678117096424103, 0.6642369031906128, 0.6834679841995239, 0.4052268862724304, 0.9096570014953613, 0.5333616733551025, 0.4612964391708374, 0.11695985496044159, 0.7849307656288147, 0.6384545564651489, 0.4623195230960846, 0.4850556254386902, 0.6482245922088623, 0.41411036252975464, 0.631813645362854, 0.7052657008171082, 0.46664538979530334, 0.4583815038204193, 0.7545777559280396, 0.014192657545208931, 0.15707959234714508, 0.26781249046325684, 0.4431241750717163, 0.7176110148429871, 0.6806966662406921, 0.4718211889266968, 0.48869138956069946, 0.5004230737686157, 0.4885577857494354, 0.5983725786209106, 0.15013857185840607, 0.6753432154655457, 0.8918094635009766, 0.3883916437625885, 0.7475528717041016, 0.5724544525146484, 0.7666016221046448, 0.5162854790687561, 0.06200911104679108, 0.68175208568573, 0.04945563152432442, 0.798380434513092, 0.6757313013076782, 0.6552808284759521, 0.6231459379196167, 0.523353099822998, 0.6012188196182251, 0.5161952376365662, 0.7055833339691162, 0.7025656700134277, 0.7710311412811279, 0.7169325351715088, 0.5760352611541748, 0.6114532351493835, 0.4416191577911377, -0.007885010913014412, 0.5846533179283142, 0.5239800214767456, 0.5703760385513306, 0.6872923970222473, -0.04354606196284294, 0.5515031218528748, 0.25327613949775696, 0.3486408591270447, 0.5379108786582947, 0.3460870385169983, 0.6508781909942627, 0.7458567023277283, 0.3011958599090576, 0.19324584305286407, 0.5202242136001587, 0.4601706564426422, 0.6045699119567871, 0.3566334545612335, 0.51282799243927, 0.6911249160766602, 0.8584827184677124, 0.18930049240589142, 0.48472797870635986, 0.49377334117889404, 0.6061760187149048, 0.6477068066596985, 0.4733128547668457, 0.5037941336631775, 0.403315007686615, 0.7367326617240906, 0.5105383396148682, 0.1503777951002121, 0.3469935953617096, 0.6025424003601074, 0.6103824973106384, 0.7073745131492615, 0.1171618402004242, 0.6801393628120422, 0.5264765620231628, 0.6981474757194519, 0.4883333742618561, 0.4713909924030304, 0.6811956167221069, 0.5804033279418945, 0.12083446234464645, 0.4601607322692871, 0.6197105646133423, 0.44498324394226074, 0.802281379699707, 0.5902817249298096, 0.5714532136917114, 0.4225459694862366, 0.45734903216362, 0.25573769211769104, 0.8168094158172607, 0.7005022764205933, 0.3262253701686859, 0.42928606271743774, 0.7250411510467529, 0.8544195294380188, 0.7025066018104553, 0.69046950340271, 0.5902090668678284, 0.19068430364131927, 0.4997328817844391, 0.6020780801773071, 0.4964425563812256, 0.537265956401825, 0.5870282649993896, 0.376912921667099, 0.8590143918991089, 0.7211766839027405, 0.1967208832502365, 0.5459376573562622, 0.46591898798942566, 0.5819228291511536, 0.4339427351951599, 0.30781757831573486, 0.6244100332260132, 0.47464802861213684, 0.4067203998565674, 0.750490665435791, 0.2967602014541626, 0.39444178342819214, 0.43095093965530396, 0.7793586254119873, 0.46672308444976807, 0.7305830121040344, 0.7074306607246399, 0.45456135272979736, 0.20829461514949799, 0.7711162567138672, 0.4830392599105835, 0.5648137927055359, 0.2808789014816284, 0.7886567711830139, 0.7292331457138062, 0.7146357297897339, 0.7495306730270386, 0.07028213143348694, 0.7216405868530273, 0.45749497413635254, 0.4652880132198334, 0.7261769771575928, 0.5423958897590637, 0.5698264241218567, 0.751291036605835, 0.43140774965286255, 0.5008069276809692, 0.7052854299545288, 0.8938241600990295, 0.5332593321800232, 0.6651415228843689, 0.5789996981620789, 0.6615620255470276, 0.9103421568870544, 0.5857571363449097, 0.696229100227356, 0.6123968362808228, 0.5975649356842041, 0.4956417679786682, 0.5259808897972107, 0.6899522542953491, 0.2359679490327835, 0.4987836480140686, 0.569703221321106, 0.49673545360565186, 0.27515870332717896, 0.7499276399612427, 0.7185884714126587, 0.629592776298523, 0.7145355343818665, 0.1858351081609726, 0.6370429992675781, 0.5942407846450806, 0.42011570930480957, 0.21025805175304413, 0.5653656721115112, 0.4439730644226074, 0.5471243858337402, 0.5859233736991882, 0.317793071269989, 0.7778283357620239, 0.5370045304298401, 0.4951012134552002, 0.6896619200706482, 0.8058610558509827, 0.6951786279678345, 0.7356845140457153, 0.44660383462905884, 0.6449549198150635, 0.7806872725486755, 0.4922030568122864, 0.5931905508041382, 0.12755687534809113, 0.6867619156837463, 0.5075019598007202, 0.8900794982910156, 0.6408514976501465, 0.15308122336864471, 0.4943671226501465, 0.4719734787940979, 0.49240580201148987, 0.5295209288597107, 0.39950352907180786, 0.46221157908439636, 0.5220164656639099, 0.47376829385757446, 0.46732980012893677, 0.46469882130622864, 0.5751615166664124, 0.3643377721309662, 0.6487233638763428, 0.6509466171264648, 0.5575471520423889, 0.46032416820526123, 0.10285322368144989, 0.5207420587539673, 0.6394122838973999, 0.26182931661605835, 0.7412449717521667, 0.6882964372634888, 0.7615864872932434, 0.44050830602645874, 0.3808099627494812, 0.659102201461792, 0.45158591866493225, 0.2368144690990448, 0.45274823904037476, 0.5784504413604736, 0.7712026238441467, 0.6238002181053162, 0.5929646492004395, 0.6390559077262878, 0.7297972440719604, 0.7587097883224487, 0.6569409370422363, 0.5588164329528809, 0.7881587147712708, 0.509446382522583, 0.45547163486480713, 0.5231724977493286, 0.09571895003318787, 0.4797791838645935, 0.6205602288246155, 0.5479015707969666, 0.5116293430328369, 0.5188151597976685, 0.5764294862747192, 0.6786662340164185, 0.8192170858383179, 0.4417675733566284, 0.8837252855300903, 0.5114109516143799, 0.401652455329895, 0.5844749808311462, 0.4072509706020355, 0.45417851209640503, 0.7263741493225098, 0.6348748207092285, 0.5490267872810364, 0.4177311062812805, 0.44701552391052246, 0.20834246277809143, 0.6483091115951538, 0.7253618240356445, 0.6238406896591187, 0.5078200697898865, 0.6723335385322571, 0.47953903675079346, 0.7023837566375732, 0.6780523061752319, 0.39562398195266724, 0.7574458122253418, 0.5143754482269287, 0.4898298978805542, 0.7956680655479431, 0.5641687512397766, 0.47983020544052124, 0.6719062328338623, 0.48413407802581787, 0.3506823182106018, 0.2652339041233063, 0.6120758652687073, 0.47696641087532043, 0.2465796321630478, 0.42810142040252686, 0.5745237469673157, 0.47132670879364014, 0.6655051708221436, 0.2761821150779724, 0.6979916095733643, 0.5293374061584473, 0.7340471148490906, 0.5564290285110474, 0.4451262354850769, 0.7367097735404968, 0.8038164973258972, 0.6814723610877991, 0.623184323310852, 0.4837515950202942, 0.6244683265686035, 0.5855531692504883, 0.6134642362594604, 0.5175502300262451, 0.36895567178726196, 0.20237024128437042, 0.7174059748649597, 0.1968783587217331, 0.45806384086608887, 0.7035243511199951, 0.5880316495895386, 0.43170902132987976, 0.6399593949317932, 0.6882027983665466, 0.7481911182403564, 0.5964206457138062, 0.9016332626342773, 0.5131950378417969, 0.59965980052948, 0.6546213030815125, 0.3303227126598358, 0.6745009422302246, 0.8162323236465454, 0.537513017654419, 0.7941646575927734, 0.5151885151863098, 0.54730224609375, 0.307401567697525, 0.7623195052146912, 0.6378120183944702, 0.6108266115188599, 0.7835255265235901, 0.6359466910362244, 0.7392687797546387, 0.5466672778129578, 0.44091153144836426, 0.7749803066253662, 0.6669480800628662, 0.43430468440055847, 0.8130829334259033, 0.744357705116272, 0.8948371410369873, 0.6272817850112915, 0.7080596685409546, 0.5019221305847168, 0.6975439190864563, 0.48726505041122437, 0.7345089912414551, 0.0927453562617302, 0.620997428894043, 0.6193662881851196, 0.9650210738182068, 0.6255189180374146, 0.6400285959243774, 0.6236352920532227, 0.4792410731315613, 0.06524359434843063, 0.6986382603645325, 0.5613736510276794, 0.6840897798538208, 0.7154577970504761, 0.3607872724533081, 0.40238356590270996, 0.46782055497169495, 0.6326638460159302, 0.5532432794570923, 0.6159324049949646, 0.2860035300254822, 0.5675593614578247, 0.864700198173523, 0.2711136043071747, 0.6937720775604248, 0.6295586824417114, 0.31952428817749023, 0.15228432416915894, 0.7790929079055786, 0.4306579828262329, 0.6384800672531128, 0.6126278042793274, 0.5471096038818359, 0.2354445606470108, 0.4661412239074707, 0.526592493057251, 0.7488353848457336, 0.7747323513031006, 0.6524612903594971, 0.6345421671867371, 0.6831484436988831, 0.26633429527282715, 0.6216332912445068, 0.7173852920532227, 0.29698535799980164, 0.6457338929176331, 0.5877426862716675, 0.6918118596076965, 0.45746418833732605, 0.7354418635368347, 0.4545972943305969, 0.7015582323074341, 0.507401168346405, 0.5433871150016785, 0.7841525673866272, 0.4176143407821655, 0.690144956111908, 0.6497296690940857, 0.5114635229110718, 0.17244671285152435, 0.6129543781280518, 0.4738534092903137, 0.4822550117969513, 0.10221732407808304, 0.47692883014678955, 0.19229811429977417, 0.5628970861434937, 0.7662050724029541, 0.5915036201477051, 0.6431021094322205, 0.4370969533920288, 0.8763565421104431, 0.4522295594215393, 0.8760155439376831, 0.1571972817182541, 0.400712251663208, 0.16690488159656525, 0.4317404627799988, 0.557273268699646, 0.7093677520751953, 0.5435003042221069, 0.328720360994339, 0.46596604585647583, 0.4902244210243225, 0.7692922353744507, 0.47501736879348755, 0.7835144996643066, 0.4740641713142395, 0.6060171127319336, 0.7358359098434448, 0.6348194479942322, 0.42032989859580994, 0.4687403738498688, 0.5068694353103638, 0.7042114734649658, 0.6664632558822632, 0.5241722464561462, 0.43432655930519104, 0.5996054410934448, 0.1510310024023056, 0.10876060277223587, 0.7188853025436401, 0.5478558540344238, 0.5629096031188965, 0.43045878410339355, 0.664106011390686, 0.49397099018096924, 0.15997645258903503, 0.7798258066177368, 0.518478512763977, 0.665721595287323, 0.6275052428245544, 0.7344878911972046, 0.5863704681396484, 0.7886961102485657, 0.7717596888542175, 0.7124853730201721, 0.7864289879798889, 0.40868300199508667, 0.7043299674987793, 0.5628762245178223, 0.5120779275894165, 0.5608938932418823, 0.5212823748588562, 0.7664246559143066, 0.5186530351638794, 0.6637217998504639, 0.5945609211921692, 0.8242640495300293, 0.1344524621963501, 0.2232227772474289, 0.48382681608200073, 0.5418030619621277, 0.6247155070304871, 0.7638667821884155, 0.6286537647247314, 0.7533745169639587, 0.17937861382961273, 0.4856853187084198, 0.09076860547065735, 0.7657802104949951, 0.6623051762580872, 0.7173358201980591, 0.7200883030891418, 0.46809083223342896, 0.31792688369750977, 0.32910192012786865, 0.6460676789283752, 0.6093140244483948, 0.43525516986846924, 0.7093769311904907, 0.7481284141540527, 0.6955267190933228, 0.3653823733329773, 0.3699650466442108, 0.6994833946228027, 0.7710069417953491, 0.583537220954895, 0.771695077419281, 0.48539555072784424, 0.4741736054420471, 0.5641234517097473, 0.7143262624740601, 0.43981143832206726, 0.5083279013633728, 0.7537018060684204, 0.6060030460357666, 0.4316498637199402, 0.6619477272033691, 0.38019704818725586, 0.3499661386013031, 0.7320530414581299, 0.6659002304077148, 0.717026948928833, 0.6862805485725403, 0.6290975213050842, 0.3372545838356018, 0.5129638314247131, 0.6383718848228455, 0.39812231063842773, 0.6887972354888916, 0.6278700828552246, 0.5051751732826233, 0.7200660109519958, 0.5280604362487793, 0.3745685815811157, 0.7701021432876587, 0.6694254875183105, 0.6163253784179688, 0.3799518942832947, 0.6396464705467224, 0.3878752291202545, 0.2467726767063141, 0.5914182662963867, 0.4621856212615967, 0.5375189781188965, 0.8123540878295898, 0.03824658319354057, 0.538652777671814, 0.6140120029449463, 0.41476866602897644, 0.6046332716941833, 0.4797995388507843, 0.14783160388469696, 0.5945208668708801, 0.7441403865814209, 0.8597424030303955, 0.49248307943344116, 0.7156293392181396, 0.15329885482788086, 0.6007907390594482, 0.750592827796936, 0.6277677416801453, 0.7316194772720337, 0.6241634488105774, 0.7815532088279724, 0.7041327953338623, 0.5742462277412415, 0.6962152123451233, 0.6382688879966736, 0.668950080871582, 0.35370585322380066, 0.5853089094161987, 0.66168212890625, 0.6783843040466309, 0.43682992458343506, 0.5480583906173706, 0.5017760992050171, 0.5071545243263245, 0.46726030111312866, 0.6370563507080078, 0.6658437252044678, 0.7159327268600464, 0.5747365951538086, 0.8725368976593018, 0.5255379676818848, 0.6838176250457764, 0.05517646670341492, 0.43834710121154785, 0.570533275604248, 0.7695074081420898, 0.48841404914855957, 0.7241913676261902, 0.12458398193120956, 0.2863137125968933, 0.49917107820510864, 0.7597975730895996, 0.8372981548309326, 0.6722933650016785, 0.5073971152305603, 0.6968899965286255, 0.5253753662109375, 0.6142023205757141, 0.627159595489502, 0.6263043284416199, 0.634519100189209, 0.532884955406189, 0.4496989846229553, 0.8134379982948303, 0.3186218738555908, 0.74483323097229, -0.09712695330381393, -0.08066704124212265, 0.3793509900569916, 0.45224618911743164, 0.6253331899642944, 0.4385649561882019, 0.22885531187057495, 0.31300443410873413, 0.4576290249824524, 0.6847667694091797, 0.058087050914764404, 0.9054942727088928, 0.5551227331161499, 0.7873096466064453, 0.6008806228637695, 0.7205498814582825, 0.35555925965309143, 0.5910696983337402, 0.5974166393280029, 0.36273467540740967, 0.6634140610694885, 0.4926183819770813, 0.3250574469566345, 0.5769519209861755, 0.7979313135147095, 0.42634302377700806, 0.508889377117157, -0.10036638379096985, 0.7672723531723022, 0.4254145324230194, 0.7356606721878052, 0.5133775472640991, 0.7367108464241028, 0.2787438631057739, 0.4104803204536438, 0.3919922709465027, 0.7898396253585815, 0.7171907424926758, 0.669135332107544, 0.6419618129730225, 0.742015540599823, 0.6082849502563477, 0.7532312870025635, 0.5053953528404236, 0.4183814525604248, 0.5305947065353394, 0.6432957649230957, 0.6894737482070923, 0.4812707304954529, 0.6885032653808594, 0.11861586570739746, 0.17619363963603973, 0.5068910121917725, 0.6439601182937622, 0.5311112999916077, 0.5739268660545349, 0.7434048652648926, 0.5308456420898438, 0.31789183616638184, 0.14634233713150024, 0.6363320350646973, 0.63314288854599, 0.6726728081703186, 0.005764746572822332, 0.44807344675064087, 0.7524640560150146, 0.6780663132667542, 0.5446398258209229, 0.6173779964447021, 0.4272463321685791, 0.6745595932006836, 0.6566855907440186, 0.5370745658874512, 0.6768814325332642, 0.8356238603591919, 0.3120673596858978, 0.591697096824646, 0.1869644671678543, 0.29831573367118835, 0.7236244678497314, 0.3615000545978546, 0.7259175777435303, 0.48191332817077637, 0.6436769962310791, 0.5170036554336548, 0.7116989493370056, 0.4853651225566864, 0.7195949554443359, 0.6467896699905396, 0.7201074361801147, 0.12665583193302155, 0.4701921343803406, 0.6929706931114197, 0.5996150970458984, 0.8275461196899414, 0.3460209369659424, 0.4907386898994446, 0.4849005341529846, 0.453330934047699, 0.5753206610679626, 0.3650793135166168, 0.3001105487346649, 0.5332244634628296, 0.5367122888565063, 0.436190128326416, 0.7355703711509705, 0.4727335572242737, 0.2741970717906952, 0.6882394552230835, 0.5009358525276184, 0.19345220923423767, 0.5407201051712036, 0.7110985517501831, 0.042683087289333344, 0.4026060700416565, 0.4502865672111511, 0.27694496512413025, 0.6103555560112, 0.46096915006637573, 0.7572889924049377, 0.6949747800827026, 0.8470731973648071, 0.758249044418335, 0.5931105613708496, 0.500294029712677, 0.6891310214996338, 0.6911696791648865, 0.7238562107086182, 0.48709118366241455, 0.800769567489624, 0.7319913506507874, 0.5377359986305237, 0.6347065567970276, 0.392135888338089, 0.7148051857948303, 0.6143154501914978, 0.4690413475036621, 0.42627695202827454, 0.5361357927322388, 0.7930057048797607, 0.317710816860199, 0.6434304714202881, 0.7099401950836182, 0.4029686152935028, 0.7652944922447205, 0.40182799100875854, 0.7095859050750732, 0.16888783872127533, 0.7603845000267029, 0.7265535593032837, 0.6076363325119019, 0.6912712454795837, 0.7494192123413086, 0.47906729578971863, 0.7261050939559937, 0.7282880544662476, 0.0721699446439743, 0.5321570038795471, 0.8210455179214478, 0.598572850227356, 0.21822844445705414, 0.6208312511444092, 0.6488656997680664, 0.40535905957221985, 0.49344247579574585, 0.028365446254611015, 0.7424702048301697, 0.41285669803619385, 0.6910555362701416, 0.40730544924736023, 0.23510301113128662, 0.7735085487365723, 0.46702808141708374, 0.42965251207351685, 0.8391976356506348, 0.6589368581771851, 0.7881582975387573, 0.8045986294746399, 0.5953564047813416, 0.364200234413147, 0.7376068830490112, 0.5666730403900146, 0.7237040996551514, 0.5729869604110718, 0.6596929430961609, 0.4246269464492798, 0.6646981239318848, 0.5262610912322998, 0.6093837022781372, 0.5007381439208984, 0.5985338091850281, 0.5079156756401062, 0.6094239354133606, 0.49287307262420654, 0.7261655330657959, 0.4575069546699524, 0.14967429637908936, 0.6391347646713257, 0.6031558513641357, 0.5610184669494629, 0.7711546421051025, 0.14643023908138275, 0.7455402612686157]\n",
            "[0.98594069480896, 0.9997414946556091, 0.9674745202064514, 0.9967093467712402, 0.9803674221038818, 0.9955924153327942, 0.9966508746147156, 0.13644836843013763, 0.9944467544555664, 0.018165264278650284, 0.1306328922510147, 0.9942193031311035, 0.007605595979839563, 0.010050115175545216, 0.6954772472381592, 0.9875596165657043, 0.588015079498291, 0.26293572783470154, 0.0038517776411026716, 0.007330304477363825, 0.985395610332489, 0.05143047869205475, 0.001273436238989234, 0.9838083386421204, 0.9974083304405212, 0.006104527972638607, 0.9992328882217407, 0.0010231593623757362, 0.0018863551085814834, 0.0256260484457016, 0.9964313507080078, 0.08875754475593567, 0.9865281581878662, 0.004740983713418245, 0.04663243889808655, 0.003145970171317458, 0.027734823524951935, 0.019246086478233337, 0.005637248046696186, 0.8463513255119324, 0.042696453630924225, 0.0053473422303795815, 0.003043178701773286, 0.007729819510132074, 0.9941660165786743, 0.004162504803389311, 0.9944887161254883, 0.09432437270879745, 0.0015834467485547066, 0.0027449633926153183, 0.9300000667572021, 0.9986183643341064, 0.02811349183320999, 0.9364833831787109, 0.32482782006263733, 0.996483325958252, 0.009249375201761723, 0.9846929311752319, 0.009422718547284603, 0.03824148327112198, 0.9986212253570557, 0.9842788577079773, 0.932424783706665, 0.994951605796814, 0.9908279180526733, 0.9995934367179871, 0.9832443594932556, 0.9908984303474426, 0.021362781524658203, 0.0059298016130924225, 0.007253148127347231, 0.9951322674751282, 0.9861204028129578, 0.014233673922717571, 0.021706879138946533, 0.011490993201732635, 0.9815623760223389, 0.16530025005340576, 0.9954214692115784, 0.9091393351554871, 0.0057828365825116634, 0.003400572342798114, 0.9919861555099487, 0.7418922185897827, 0.991219699382782, 0.9072756171226501, 0.01103826891630888, 0.9852247834205627, 0.997651994228363, 0.9993605017662048, 0.9969325065612793, 0.9283172488212585, 0.9661426544189453, 0.006695834454149008, 0.317592054605484, 0.010175107046961784, 0.007023966871201992, 0.007832946255803108, 0.8009883761405945, 0.9897183179855347, 0.1325860619544983, 0.7634220123291016, 0.9833464622497559, 0.9866318106651306, 0.028364896774291992, 0.003014026442542672, 0.007640008348971605, 0.9971449971199036, 0.9945216178894043, 0.9062886834144592, 0.004429812077432871, 0.9986701011657715, 0.8658847808837891, 0.9963918328285217, 0.02333684265613556, 0.019144907593727112, 0.0016388766234740615, 0.06757833808660507, 0.9730927348136902, 0.08249309659004211, 0.0012281190138310194, 0.8799653649330139, 0.9936664700508118, 0.9722254276275635, 0.0076920827850699425, 0.01602241024374962, 0.002984045771881938, 0.9991476535797119, 0.0017421161755919456, 0.004896701313555241, 0.014055847190320492, 0.34072786569595337, 0.7134856581687927, 0.021136390045285225, 0.9979214072227478, 0.0009895882103592157, 0.8767799735069275, 0.9940266609191895, 0.9948616623878479, 0.05620511621236801, 0.9326032996177673, 0.8487700819969177, 0.0036197947338223457, 0.014218348078429699, 0.01249485183507204, 0.9780599474906921, 0.9976396560668945, 0.29585394263267517, 0.8547029495239258, 0.19642077386379242, 0.0050597465597093105, 0.949277400970459, 0.9833618998527527, 0.03129592537879944, 0.024057354778051376, 0.005216123070567846, 0.42776283621788025, 0.9957253932952881, 0.9985754489898682, 0.9869678616523743, 0.016720900312066078, 0.009550405666232109, 0.0029286444187164307, 0.010369121097028255, 0.009397532790899277, 0.16722744703292847, 0.00928935594856739, 0.9861066341400146, 0.9810560345649719, 0.00838413555175066, 0.9963347911834717, 0.9975780844688416, 0.030583344399929047, 0.005177855025976896, 0.006628004834055901, 0.35130399465560913, 0.9998125433921814, 0.9896026253700256, 0.9260690808296204, 0.0114322230219841, 0.026921769604086876, 0.02489556185901165, 0.8924570679664612, 0.9948424696922302, 0.9974443912506104, 0.8750402331352234, 0.05745985358953476, 0.9982357025146484, 0.9918568134307861, 0.37161099910736084, 0.025394296273589134, 0.9995903372764587, 0.9911255240440369, 0.008925443515181541, 0.00668040756136179, 0.027568884193897247, 0.9369611740112305, 0.002279015025123954, 0.993355393409729, 0.9134785532951355, 0.9999110698699951, 0.007027843035757542, 0.9214919805526733, 0.9906593561172485, 0.00165405566804111, 0.9963217973709106, 0.0023180765565484762, 0.007595345843583345, 0.12828026711940765, 0.041879910975694656, 0.03025222197175026, 0.9962565898895264, 0.9848141074180603, 0.9705907106399536, 0.01936626061797142, 0.016437619924545288, 0.9969903230667114, 0.9666727185249329, 0.004395018797367811, 0.012969324365258217, 0.994317352771759, 0.00358721730299294, 0.9525653719902039, 0.0258221123367548, 0.023298341780900955, 0.3288740813732147, 0.0039856224320828915, 0.006900448352098465, 0.006147157400846481, 0.9939088821411133, 0.8790159225463867, 0.9997836947441101, 0.9969102740287781, 0.6302352547645569, 0.998683750629425, 0.007505336776375771, 0.0048326123505830765, 0.9990031123161316, 0.02603621780872345, 0.013150540180504322, 0.9987612962722778, 0.003122643567621708, 0.9840909838676453, 0.9991857409477234, 0.006308554206043482, 0.0038037695921957493, 0.991370677947998, 0.9975076913833618, 0.7671202421188354, 0.9868528246879578, 0.0037062016781419516, 0.005892288871109486, 0.9770962595939636, 0.024362675845623016, 0.9550362825393677, 0.9920046925544739, 0.5156720876693726, 0.07541868835687637, 0.9677661061286926, 0.9957650899887085, 0.969222903251648, 0.9985381364822388, 0.6571505665779114, 0.12959802150726318, 0.00849958136677742, 0.006426477339118719, 0.9954832792282104, 0.0063499026000499725, 0.007994734682142735, 0.9837734699249268, 0.9862035512924194, 0.01114644668996334, 0.01895354501903057, 0.999283492565155, 0.9994319081306458, 0.4978460371494293, 0.9505317211151123, 0.9965657591819763, 0.006756297778338194, 0.9840818643569946, 0.8274325728416443, 0.01019846461713314, 0.009090447798371315, 0.9972182512283325, 0.9678760766983032, 0.006193861830979586, 0.023054737597703934, 0.012105430476367474, 0.903801441192627, 0.9952876567840576, 0.9997127652168274, 0.008940392173826694, 0.8731255531311035, 0.009479694068431854, 0.9836433529853821, 0.009278777055442333, 0.007107760291546583, 0.007638789247721434, 0.003481091232970357, 0.9966745376586914, 0.6242969632148743, 0.9983363747596741, 0.9994282126426697, 0.033764295279979706, 0.020960595458745956, 0.8099405765533447, 0.003452883567661047, 0.9990133047103882, 0.982557475566864, 0.966587483882904, 0.005395854357630014, 0.9916240572929382, 0.028967462480068207, 0.0629952996969223, 0.9755191206932068, 0.0013838078593835235, 0.015632595866918564, 0.0034875499550253153, 0.8861660361289978, 0.9851220846176147, 0.8763083219528198, 0.896579921245575, 0.005080538336187601, 0.9949007034301758, 0.011155159212648869, 0.001332679996266961, 0.9970235228538513, 0.022603755816817284, 0.9501133561134338, 0.996619701385498, 0.3015424609184265, 0.011958708055317402, 0.9979968667030334, 0.9318557381629944, 0.9687080979347229, 0.009990316815674305, 0.939669668674469, 0.9993442893028259, 0.9851477742195129, 0.9912286996841431, 0.5764853358268738, 0.995081901550293, 0.23141570389270782, 0.9992337226867676, 0.9750757813453674, 0.9968433380126953, 0.9993228912353516, 0.7419872879981995, 0.00527481734752655, 0.9954302310943604, 0.9873162508010864, 0.001796538126654923, 0.003575014416128397, 0.007318675518035889, 0.019370611757040024, 0.0323607474565506, 0.9974027276039124, 0.9984594583511353, 0.9955770969390869, 0.9132080674171448, 0.0021835442166775465, 0.9949186444282532, 0.9977657794952393, 0.960461437702179, 0.9741515517234802, 0.22471655905246735, 0.02332436479628086, 0.0070943282917141914, 0.9968775510787964, 0.1862809807062149, 0.9970476031303406, 0.9861109256744385, 0.9944325089454651, 0.0121280113235116, 0.9971150159835815, 0.9634813070297241, 0.9893156290054321, 0.003562855999916792, 0.1270400434732437, 0.002933153649792075, 0.9374480843544006, 0.011482134461402893, 0.011182400397956371, 0.9971064925193787, 0.7857905626296997, 0.03048050031065941, 0.8381759524345398, 0.030718330293893814, 0.9840942025184631, 0.9937771558761597, 0.041359689086675644, 0.05647842586040497, 0.990213930606842, 0.002345748944208026, 0.9976987242698669, 0.1385013461112976, 0.9924436211585999, 0.10377968102693558, 0.0026222600135952234, 0.998381495475769, 0.9861173033714294, 0.0031456726137548685, 0.963229775428772, 0.9944624304771423, 0.9981595873832703, 0.9757037162780762, 0.010327075608074665, 0.8933074474334717, 0.9949807524681091, 0.00657248217612505, 0.15897975862026215, 0.9969366788864136, 0.9839433431625366, 0.9637343883514404, 0.014279644936323166, 0.003519116435199976, 0.603888750076294, 0.007542306557297707, 0.003645552322268486, 0.9900813698768616, 0.9236602783203125, 0.010535530745983124, 0.011159294284880161, 0.9447901844978333, 0.9873933792114258, 0.9962604641914368, 0.007940994575619698, 0.8770537972450256, 0.03435181453824043, 0.9354562163352966, 0.007097593508660793, 0.9969673752784729, 0.7472420334815979, 0.015258999541401863, 0.8958867788314819, 0.9990689158439636, 0.03156058490276337, 0.9984941482543945, 0.9965062737464905, 0.012768218293786049, 0.9949354529380798, 0.03543265163898468, 0.004963213112205267, 0.9989803433418274, 0.9721444249153137, 0.9974607229232788, 0.012028939090669155, 0.014243707060813904, 0.9923738837242126, 0.015299306251108646, 0.9710010290145874, 0.03080061823129654, 0.011414598673582077, 0.004126342479139566, 0.010493788868188858, 0.002703131875023246, 0.03999275714159012, 0.05250396579504013, 0.008254592306911945, 0.997832715511322, 0.9971711039543152, 0.012540207244455814, 0.9961556792259216, 0.002555584069341421, 0.013327810913324356, 0.21853597462177277, 0.9939103126525879, 0.015977350994944572, 0.02515825629234314, 0.9588493704795837, 0.7887520790100098, 0.9941785335540771, 0.9972925782203674, 0.9958997368812561, 0.01864100992679596, 0.9963093400001526, 0.9971699118614197, 0.9932031631469727, 0.9979370832443237, 0.013001698069274426, 0.01907241903245449, 0.995441198348999, 0.9954980611801147, 0.9883441925048828, 0.7072051167488098, 0.005946430843323469, 0.9969183206558228, 0.016941282898187637, 0.9980818033218384, 0.011778685264289379, 0.9920591115951538, 0.9791777729988098, 0.0014105685986578465, 0.9478825330734253, 0.02984483353793621, 0.995868980884552, 0.9992896318435669, 0.9814890027046204, 0.6312876343727112, 0.00665261410176754, 0.010784287005662918, 0.003198883729055524, 0.00820055790245533, 0.9692395329475403, 0.004930898081511259, 0.011236494407057762, 0.018912917003035545, 0.005981835070997477, 0.9943812489509583, 0.1998184323310852, 0.05357276648283005, 0.0103056151419878, 0.038863204419612885, 0.9878101348876953, 0.9469707608222961, 0.01910184882581234, 0.9172760844230652, 0.019545679911971092, 0.019422654062509537, 0.011926945298910141, 0.9939209222793579, 0.01074217725545168, 0.012036523781716824, 0.9365270733833313, 0.02488950826227665, 0.9839774966239929, 0.011781517416238785, 0.006601481698453426, 0.012990637682378292, 0.8652917146682739, 0.9940150380134583, 0.001735666417516768, 0.9922044277191162, 0.8893491625785828, 0.007069466635584831, 0.05194152519106865, 0.9886427521705627, 0.0020761792548000813, 0.011211114935576916, 0.9973536729812622, 0.9899811148643494, 0.013328589498996735, 0.9745522737503052, 0.0300779789686203, 0.848098635673523, 0.993578314781189, 0.9693979024887085, 0.9932644367218018, 0.9953523874282837, 0.9962018132209778, 0.9993684887886047, 0.998261034488678, 0.8430783748626709, 0.8414208292961121, 0.006607553921639919, 0.03740338981151581, 0.9965482354164124, 0.9648792147636414, 0.9943985342979431, 0.9964826107025146, 0.07087533921003342, 0.04798753559589386, 0.7555010914802551, 0.0014371038414537907, 0.9937173128128052, 0.7323368787765503, 0.0024695047177374363, 0.18207474052906036, 0.8328041434288025, 0.004521150141954422, 0.6539575457572937, 0.9870963096618652, 0.004093505442142487, 0.002942693652585149, 0.02821679599583149, 0.030044499784708023, 0.9820268750190735, 0.868554949760437, 0.9999330043792725, 0.9489079117774963, 0.9970452189445496, 0.0146110774949193, 0.9768323302268982, 0.9791507720947266, 0.9971863627433777, 0.006789880339056253, 0.023145340383052826, 0.008465495891869068, 0.0016609977465122938, 0.9959889054298401, 0.03786821663379669, 0.676964521408081, 0.0009687056881375611, 0.12468856573104858, 0.003343496471643448, 0.005411822814494371, 0.9953309893608093, 0.9981057643890381, 0.9965397119522095, 0.0011309911496937275, 0.07866846770048141, 0.99625164270401, 0.9759576916694641, 0.9987950325012207, 0.9957309365272522, 0.0053884838707745075, 0.0037482616025954485, 0.01177122350782156, 0.007095014210790396, 0.5318673253059387, 0.005623939447104931, 0.047128450125455856, 0.008788557723164558, 0.006060772575438023, 0.019450630992650986, 0.8597750067710876, 0.18577417731285095, 0.1438799500465393, 0.9310161471366882, 0.9531360864639282, 0.005170459859073162, 0.9842960834503174, 0.9953224062919617, 0.9994163513183594, 0.9991928935050964, 0.6879302859306335, 0.007109991740435362, 0.9964278340339661, 0.011284594424068928, 0.9948341846466064, 0.9102614521980286, 0.06992834806442261, 0.9465291500091553, 0.7575420141220093, 0.9666867852210999, 0.005633277352899313, 0.995162844657898, 0.08470717072486877, 0.005373238120228052, 0.0048819598741829395, 0.012799318879842758, 0.9954599738121033, 0.006291487719863653, 0.0029394724406301975, 0.9928373694419861, 0.9964119791984558, 0.005079408176243305, 0.9848798513412476, 0.015793994069099426, 0.9634045362472534, 0.022496702149510384, 0.9960046410560608, 0.12340564280748367, 0.09552476555109024, 0.03972209617495537, 0.9798935651779175, 0.9600442051887512, 0.008232892490923405, 0.9903443455696106, 0.0023468683939427137, 0.9819962978363037, 0.9949860572814941, 0.07534652948379517, 0.9784106016159058, 0.041242875158786774, 0.9763466119766235, 0.9970759153366089, 0.0028959447517991066, 0.007116817869246006, 0.9987949132919312, 0.9896076321601868, 0.04631026089191437, 0.9996801614761353, 0.987288236618042, 0.9843863844871521, 0.8042799234390259, 0.9989350438117981, 0.9990226030349731, 0.004266099072992802, 0.03974417224526405, 0.9872488379478455, 0.010229079984128475, 0.0022910763509571552, 0.98408442735672, 0.9645042419433594, 0.971045196056366, 0.27962055802345276, 0.006692436058074236, 0.9904711246490479, 0.9742876887321472, 0.004256237298250198, 0.0302412286400795, 0.007202627137303352, 0.960960328578949, 0.9946174025535583, 0.8158465623855591, 0.005489667411893606, 0.00494155939668417, 0.9988386034965515, 0.007917551323771477, 0.8976524472236633, 0.9992746710777283, 0.9991239905357361, 0.0033367089927196503, 0.06331276148557663, 0.01187411043792963, 0.013733578845858574, 0.016576744616031647, 0.022465743124485016, 0.9918141961097717, 0.9724959135055542, 0.9954506754875183, 0.9946685433387756, 0.9871670603752136, 0.013867815025150776, 0.02993283048272133, 0.9885685443878174, 0.023062212392687798, 0.03065376728773117, 0.8947636485099792, 0.014025588519871235, 0.3217129111289978, 0.9886279702186584, 0.9789809584617615, 0.9961556792259216, 0.9856802225112915, 0.7319115400314331, 0.09750216454267502, 0.9942054152488708, 0.001456617726944387, 0.01703440025448799, 0.9974603652954102, 0.014339624904096127, 0.0342184416949749, 0.9958720803260803, 0.012378601357340813, 0.9970429539680481, 0.011430866084992886, 0.9811261296272278, 0.0028024206403642893, 0.01801428198814392, 0.027188101783394814, 0.0164277832955122, 0.9847202301025391, 0.019393565133213997, 0.9921886920928955, 0.02708945982158184, 0.9783663749694824, 0.9956037998199463, 0.0017377696931362152, 0.01534185465425253, 0.9886381030082703, 0.9314269423484802, 0.9179823398590088, 0.002492734929546714, 0.004052846226841211, 0.9952049255371094, 0.976477324962616, 0.011602090671658516, 0.9994720816612244, 0.006967978086322546, 0.10439127683639526, 0.992767333984375, 0.08667605370283127, 0.024630866944789886, 0.9895877838134766, 0.03543521836400032, 0.11214649677276611, 0.9957503080368042, 0.9936098456382751, 0.0032245665788650513, 0.21326300501823425, 0.9986020922660828, 0.11718130111694336, 0.0015867785550653934, 0.002646259730681777, 0.006433709058910608, 0.00786097813397646, 0.04445486143231392, 0.0038213874213397503, 0.9997912049293518, 0.007925405167043209, 0.7488365173339844, 0.9965223073959351, 0.25850456953048706, 0.005346832796931267, 0.02222876437008381, 0.997150719165802, 0.9385901093482971, 0.047545842826366425, 0.9941052198410034, 0.008696790784597397, 0.9989105463027954, 0.7082003355026245, 0.004188285209238529, 0.02451420947909355, 0.9928110241889954, 0.014850528910756111, 0.00702516408637166, 0.9508568048477173, 0.7171972393989563, 0.03093789890408516, 0.9984943866729736, 0.9612533450126648, 0.010603209026157856, 0.9970042109489441, 0.9973575472831726, 0.9748948216438293, 0.9962847232818604, 0.02176610752940178, 0.01613505184650421, 0.997840404510498, 0.0038919171784073114, 0.004775097593665123, 0.9849461317062378, 0.8865869045257568, 0.00365492538549006, 0.9917423725128174, 0.9259188175201416, 0.9097892642021179, 0.9992544054985046, 0.9982149600982666, 0.9195312857627869, 0.9954670667648315, 0.9639590978622437, 0.014911266043782234, 0.9975242018699646, 0.9792349934577942, 0.9840654730796814, 0.0025172578170895576, 0.09749148786067963, 0.9958578944206238, 0.053432926535606384, 0.9982210993766785, 0.988552451133728, 0.019558364525437355, 0.03489360213279724, 0.9750192165374756, 0.0030231617856770754, 0.004369088914245367, 0.004526242148131132, 0.9856600761413574, 0.02323148585855961, 0.0061713955365121365, 0.9842937588691711, 0.9880861043930054, 0.004172788001596928, 0.9923108816146851, 0.00646243616938591, 0.006808753125369549, 0.014555325731635094, 0.009494144469499588, 0.7271782755851746, 0.9992232322692871, 0.009167954325675964, 0.009840276092290878, 0.014081927016377449, 0.008965861983597279, 0.9958211183547974, 0.011334622278809547, 0.011889606714248657, 0.005852581933140755, 0.014877816662192345, 0.9968781471252441, 0.03228055685758591, 0.9919899106025696, 0.9985911250114441, 0.211642324924469, 0.9808563590049744, 0.017134981229901314, 0.014481397345662117, 0.004592910874634981, 0.9872990250587463, 0.9370245337486267, 0.9994193315505981, 0.3139108717441559, 0.8584795594215393, 0.32223832607269287, 0.8151773810386658, 0.9964889287948608, 0.9771373867988586, 0.9906466007232666, 0.004883261863142252, 0.006672321353107691, 0.016259634867310524, 0.012786359526216984, 0.9778147339820862, 0.004748648498207331, 0.0012482046149671078, 0.005616392008960247, 0.046352747827768326, 0.9214451313018799, 0.042797088623046875, 0.004228480625897646, 0.7791783809661865, 0.9826319217681885, 0.9753290414810181, 0.6213732361793518, 0.003046069061383605, 0.9974095225334167, 0.3345774710178375, 0.00620593735948205, 0.9935134053230286, 0.00376594508998096, 0.06561347097158432, 0.03710869327187538, 0.0054736207239329815, 0.9861207604408264, 0.9935492873191833, 0.9873372912406921, 0.07491835951805115, 0.9946423768997192, 0.9607727527618408, 0.9811217784881592, 0.015465025790035725, 0.29557713866233826, 0.996746301651001, 0.11029596626758575, 0.8979985117912292, 0.9870740175247192, 0.9501045346260071, 0.9795515537261963, 0.9976146221160889, 0.15585976839065552, 0.9879041314125061, 0.34339919686317444, 0.9906147718429565, 0.8918730020523071, 0.0937361791729927, 0.7208185791969299, 0.9932578802108765, 0.9331428408622742, 0.021383026614785194, 0.9824305772781372, 0.015825526788830757, 0.006860489025712013, 0.99924635887146, 0.0118902288377285, 0.022626399993896484, 0.001850458444096148, 0.005667895078659058, 0.9737141728401184, 0.011401481926441193, 0.9865913987159729, 0.008938516490161419, 0.007301139645278454, 0.9916951060295105, 0.9912669062614441, 0.9947899580001831, 0.022292092442512512, 0.11403224617242813, 0.01253629382699728, 0.01659836806356907, 0.9704031348228455, 0.020780164748430252, 0.028738506138324738, 0.9985572695732117, 0.06028987467288971, 0.003206458641216159, 0.0015112472465261817, 0.8911539912223816, 0.9981990456581116, 0.017485808581113815, 0.995521068572998, 0.9132625460624695, 0.0061101545579731464, 0.5985024571418762, 0.0021627633832395077, 0.9118372201919556, 0.9836034774780273, 0.7381176352500916, 0.9590768814086914, 0.017644619569182396, 0.989310622215271, 0.002339473459869623, 0.9944266080856323, 0.07561369985342026, 0.9784723520278931, 0.8330105543136597, 0.02138841524720192, 0.9994531273841858, 0.9986710548400879, 0.9757311940193176, 0.017548924311995506, 0.9245955944061279, 0.014761650003492832, 0.9969767332077026, 0.9436396360397339, 0.9582295417785645, 0.9524187445640564, 0.9940499067306519, 0.9932565689086914, 0.003504184540361166, 0.0077527668327093124, 0.016971096396446228, 0.013536331243813038, 0.9832461476325989, 0.9957621693611145, 0.9600580334663391, 0.9969519376754761, 0.9958957433700562, 0.042479678988456726, 0.9845304489135742, 0.09814676642417908, 0.0025811740197241306, 0.9967677593231201, 0.0032144682481884956, 0.9935770630836487, 0.012184453196823597, 0.999232292175293, 0.9734644293785095, 0.4917997717857361, 0.018847130239009857, 0.9847491383552551, 0.02027350477874279, 0.9967909455299377, 0.03037215955555439, 0.9987128973007202, 0.9687052965164185, 0.9903330206871033]\n",
            "[180, 40, 160, 554, 683, 51, 599, 294, 856, 421, 255, 783, 525, 167, 664, 702, 994, 597, 562, 325, 1001, 744, 53, 280, 700, 614, 108, 809, 55, 155, 63, 93, 26, 165, 152, 274, 107, 553, 28, 874, 362, 739, 737, 839, 229, 666, 939, 730, 130, 208, 858, 958, 12, 302, 485, 135, 332, 589, 243, 687, 573, 142, 293, 658, 432, 591, 927, 305, 534, 334, 645, 759, 879, 530, 747, 1012, 505, 297, 884, 829, 386, 184, 999, 199, 472, 595, 900, 740, 982, 100, 445, 384, 477, 220, 360, 451, 179, 901, 766, 366, 650, 246, 853, 365, 117, 718, 86, 76, 396, 862, 1004, 830, 764, 922, 920, 173, 288, 643, 409, 219, 690, 501, 249, 894, 912, 850, 789, 290, 817, 673, 138, 674, 10, 516, 151, 919, 749, 696, 657, 104, 489, 896, 564, 943, 659, 31, 454, 344, 703, 195, 954, 846, 752, 101, 483, 193, 1017, 312, 788, 822, 200, 110, 841, 968, 427, 625, 665, 91, 757, 760, 782, 486, 753, 351, 898, 694, 429, 781, 256, 150, 857, 415, 997, 231, 574, 515, 844, 245, 502, 713, 341, 60, 557, 786, 775, 191, 576, 824, 509, 224, 908, 482, 529, 972, 413, 769, 859, 335, 490, 701, 295, 388, 941, 815, 649, 845, 814, 487, 669, 265, 447, 304, 952, 996, 511, 638, 266, 639, 277, 87, 959, 314, 498, 813, 733, 387, 965, 37, 89, 848, 984, 120, 538, 494, 287, 651, 145, 804, 990, 52, 805, 728, 960, 291, 634, 902, 441, 147, 691, 317, 667, 276, 873, 322, 917, 808, 185, 95, 689, 318, 987, 448, 72, 907, 424, 378, 132, 32, 778, 45, 442, 106, 346, 976, 15, 855, 499, 724, 478, 319, 439, 157, 463, 298, 528, 679, 780, 899, 57, 194, 105, 440, 36, 113, 69, 754, 587, 34, 967, 446, 114, 893, 44, 457, 391, 122, 39, 371, 933, 363, 993, 315, 316, 866, 560, 910, 680, 539, 596, 211, 401, 239, 71, 102, 357, 827, 604, 397, 772, 877, 369, 1011, 225, 81, 559, 455, 913, 627, 521, 891, 349, 197, 520, 9, 889, 526, 251, 903, 258, 13, 745, 507, 849, 904, 20, 726, 812, 1, 85, 218, 2, 860, 353, 686, 535, 187, 790, 244, 434, 233, 611, 946, 629, 864, 532, 48, 212, 670, 787, 97, 748, 123, 174, 221, 644, 338, 361, 1002, 61, 411, 871, 29, 169, 177, 688, 607, 508, 272, 881, 758, 425, 214, 832, 536, 801, 260, 103, 124, 355, 426, 493, 497, 115, 196, 230, 895, 613, 23, 953, 624, 723, 584, 605, 328, 92, 259, 354, 642, 449, 286, 275, 308, 890, 400, 765, 154, 556, 289, 692, 436, 936, 623, 33, 252, 571, 198, 392, 83, 254, 774, 612, 372, 840, 944, 524, 784, 348, 217, 938, 924, 343, 722, 77, 307, 885, 821, 510, 715, 330, 465, 868, 186, 799, 875, 49, 394, 826, 380, 181, 504, 283, 127, 11, 677, 636, 579, 602, 793, 88, 514, 697, 531, 974, 540, 236, 807, 585, 14, 945, 395, 42, 464, 247, 458, 417, 414, 84, 166, 333, 716, 1018, 746, 461, 311, 374, 119, 347, 989, 301, 385, 925, 861, 582, 543, 500, 791, 711, 273, 685, 210, 38, 136, 109, 548, 1007, 811, 828, 375, 479, 8, 423, 616, 16, 751, 617, 921, 292, 518, 323, 452, 303, 412, 835, 796, 79, 709, 262, 340, 637, 911, 570, 403, 1003, 418, 67, 257, 695, 90, 329, 704, 949, 140, 552, 476, 546, 474, 568, 156, 503, 541, 956, 955, 18, 492, 282, 906, 810, 779, 278, 215, 241, 170, 512, 141, 299, 854, 600, 242, 795, 1020, 660, 356, 471, 721, 309, 209, 957, 192, 632, 376, 234, 59, 1013, 641, 519, 435, 1006, 725, 183, 164, 712, 736, 735, 139, 563, 24, 986, 149, 762, 870, 188, 886, 663, 918, 213, 923, 1010, 327, 558, 484, 408, 1009, 732, 1015, 914, 248, 717, 942, 847, 459, 951, 991, 734, 227, 755, 456, 517, 601, 776, 134, 661, 831, 182, 469, 668, 228, 296, 569, 656, 171, 705, 373, 652, 630, 263, 78, 443, 823, 118, 738, 444, 935, 640, 794, 675, 125, 163, 462, 727, 73, 937, 962, 5, 865, 75, 438, 359, 654, 578, 566, 693, 979, 65, 172, 618, 17, 653, 995, 419, 175, 731, 682, 880, 358, 523, 162, 947, 678, 633, 466, 467, 261, 852, 284, 743, 470, 54, 46, 406, 56, 720, 635, 818, 950, 216, 204, 647, 603, 676, 567, 320, 450, 905, 491, 863, 189, 719, 929, 480, 816, 35, 47, 250, 96, 542, 129, 800, 710, 876, 368, 207, 267, 268, 143, 533, 988, 985, 620, 326, 66, 133, 399, 168, 240, 381, 590, 506, 80, 934, 948, 741, 770, 1005, 237, 843, 370, 178, 238, 594, 205, 777, 148, 226, 771, 206, 232, 271, 437, 756, 379, 888, 7, 153, 619, 475, 609, 398, 128, 729, 116, 671, 706, 878, 606, 420, 561, 621, 496, 577, 750, 146, 615, 324, 698, 592, 300, 137, 610, 973, 714, 593, 21, 572, 909, 1000, 565, 19, 978, 838, 930, 892, 648, 453, 202, 58, 98, 932, 25, 975, 513, 916, 176, 281, 158, 806, 121, 367, 270, 473, 581, 983, 428, 41, 887, 961, 970, 882, 481, 310, 631, 872, 62, 279, 431, 837, 27, 551, 269, 549, 684, 681, 313, 981, 742, 22, 522, 708, 404, 131, 792, 1016, 608, 50, 897, 763, 963, 969, 1019, 598, 342, 364, 377, 144, 544, 699, 430, 405, 159, 555, 495, 655, 626, 773, 99, 94, 798, 966, 883, 468, 527, 583, 201, 707, 785, 253, 352, 30, 767, 672, 819, 622, 836, 74, 842, 422, 112, 971, 537, 867, 264, 550, 803, 1008, 834, 547, 345, 928, 977, 410, 68, 580, 331, 964, 1014, 43, 126, 416, 646, 161, 223, 389, 382, 222, 926, 869, 802, 70, 3, 402, 488, 588, 768, 4, 825, 336, 433, 833, 460, 980, 545, 337, 628, 306, 64, 190, 797, 915, 931, 586, 992, 761, 321, 662, 350, 393, 820, 407, 383, 203, 390, 111, 339, 285, 940, 998, 6, 851, 82, 575, 235]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pandas library\n",
        "import pandas as pd\n",
        " \n",
        "# Create the pandas DataFrame\n",
        "test_pred = pd.DataFrame({'image_id': image_ids, 'angel': final_result_angle, 'speed': final_result_speed})\n",
        "\n",
        "test_pred.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ptc3hyTqjjyU",
        "outputId": "5fbf5d56-0935-40ed-dc8e-6989bba53ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   image_id  angel  speed\n",
              "0       180 0.3409 0.9859\n",
              "1        40 0.6263 0.9997\n",
              "2       160 0.1565 0.9675\n",
              "3       554 0.5725 0.9967\n",
              "4       683 0.4249 0.9804"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5c92d6d6-c6f4-4aa1-a562-55e7713d5420\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>angel</th>\n",
              "      <th>speed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>180</td>\n",
              "      <td>0.3409</td>\n",
              "      <td>0.9859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>40</td>\n",
              "      <td>0.6263</td>\n",
              "      <td>0.9997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>160</td>\n",
              "      <td>0.1565</td>\n",
              "      <td>0.9675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>554</td>\n",
              "      <td>0.5725</td>\n",
              "      <td>0.9967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>683</td>\n",
              "      <td>0.4249</td>\n",
              "      <td>0.9804</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5c92d6d6-c6f4-4aa1-a562-55e7713d5420')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5c92d6d6-c6f4-4aa1-a562-55e7713d5420 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5c92d6d6-c6f4-4aa1-a562-55e7713d5420');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sort_result = test_pred.sort_values(test_pred.columns[0], ascending = True).reset_index(drop=True)\n",
        "sort_result.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_NQ-d9BllDKo",
        "outputId": "672fe063-44f3-4320-a04a-69ee268b605f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   image_id  angel  speed\n",
              "0         1 0.5976 0.0071\n",
              "1         2 0.6900 0.9970\n",
              "2         3 0.2182 0.9785\n",
              "3         4 0.0284 0.9757\n",
              "4         5 0.3291 0.9991"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5c11ec1d-b84e-499f-8c41-e236bb6ca326\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>angel</th>\n",
              "      <th>speed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5976</td>\n",
              "      <td>0.0071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.6900</td>\n",
              "      <td>0.9970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.2182</td>\n",
              "      <td>0.9785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.0284</td>\n",
              "      <td>0.9757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.3291</td>\n",
              "      <td>0.9991</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5c11ec1d-b84e-499f-8c41-e236bb6ca326')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5c11ec1d-b84e-499f-8c41-e236bb6ca326 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5c11ec1d-b84e-499f-8c41-e236bb6ca326');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')\n",
        "\n",
        "sort_result.to_csv('/content/drive/My Drive/tesla_submission_2models.csv', encoding='utf-8', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QREQQymXnRjj",
        "outputId": "947eec82-72df-4945-f19a-18220b23cc1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "toPdcTy1UG26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c machine-learning-in-science-2022 -f submission.csv -m \"Message\""
      ],
      "metadata": {
        "id": "UFhwsiJJ-86E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}